"""
Procesador de mensajes y comandos para la interfaz de chat
VERSIÃ“N SIMPLIFICADA CON SISTEMA DE PLANTILLAS SQL
"""
import random
from datetime import datetime
from typing import Dict, Any
from app.core.config import Config
from app.core.logging import get_logger

class MessageProcessor:
    """Procesador de mensajes simplificado con sistema de plantillas SQL"""

    def __init__(self, gemini_client=None, pdf_panel=None):
        """Inicializa el procesador de mensajes"""
        self.gemini_client = gemini_client
        self.pdf_panel = pdf_panel

        # ğŸ†• LOGGING CENTRALIZADO
        self.logger = get_logger(__name__)

        # ğŸ¯ INICIALIZAR MASTER INTERPRETER AL CARGAR EL SISTEMA
        self.logger.info("ğŸ¯ [MESSAGEPROCESSOR] Inicializando MasterInterpreter al cargar sistema...")
        from app.core.ai.interpretation.master_interpreter import MasterInterpreter
        self.master_interpreter = MasterInterpreter(self.gemini_client)
        self.logger.info("âœ… [MESSAGEPROCESSOR] MasterInterpreter inicializado y listo")

        # CONTEXTO CONVERSACIONAL SIMPLE
        self.conversation_history = []
        self.last_query_results = None

        # ğŸ”§ ATRIBUTOS REQUERIDOS PARA COMPATIBILIDAD
        self.conversation_stack = []
        self.awaiting_continuation = False

        # ğŸ†• FRASES DESDE CONFIGURACIÃ“N CENTRALIZADA
        self.greeting_phrases = Config.RESPONSES['greeting_phrases']
        self.success_phrases = Config.RESPONSES['success_phrases']

        # ğŸ¯ CONFIGURACIÃ“N INTELIGENTE: ACCIONES â†’ VISUALIZACIÃ“N
        self.action_display_config = {
            # SIEMPRE mostrar datos (distribuciones, estadÃ­sticas)
            "CALCULAR_ESTADISTICA": "always",

            # CONDICIONAL segÃºn cantidad (bÃºsquedas, listas)
            "BUSCAR_UNIVERSAL": "conditional",
            "consulta_sql_exitosa": "conditional",

            # NUNCA mostrar datos tÃ©cnicos (solo respuesta humana)
            "OBTENER_ALUMNO_EXACTO": "never",
            "GENERAR_CONSTANCIA_COMPLETA": "never",
            "ayuda_proporcionada": "never",
            "ayuda_funcionalidades": "never",
            "conversacion_general": "never",

            # ESPECIALES (manejan su propia visualizaciÃ³n)
            "constancia_preview": "special",
            "transformation_preview": "special",
            "solicitar_aclaracion": "special"
        }

        # ğŸ¯ UMBRALES CONFIGURABLES
        self.display_thresholds = {
            "max_auto_display": 50,      # MÃ¡ximo para mostrar automÃ¡ticamente
            "large_list_limit": 25,      # LÃ­mite para listas grandes
            "summary_threshold": 100     # Umbral para mostrar solo resumen
        }

        self.logger.info("âœ… MessageProcessor inicializado con MasterInterpreter cargado")

    def should_display_data(self, action: str, data_count: int) -> bool:
        """
        ğŸ¯ DECISIÃ“N INTELIGENTE: Â¿Mostrar datos estructurados?

        Args:
            action: AcciÃ³n ejecutada (ej: CALCULAR_ESTADISTICA, BUSCAR_UNIVERSAL)
            data_count: Cantidad de registros en los datos

        Returns:
            bool: True si debe mostrar datos estructurados
        """
        # Obtener configuraciÃ³n para esta acciÃ³n
        display_mode = self.action_display_config.get(action, "conditional")

        if display_mode == "always":
            self.logger.info(f"ğŸ¯ [DISPLAY_DECISION] {action} â†’ ALWAYS â†’ Mostrar datos ({data_count} registros)")
            return True
        elif display_mode == "never":
            self.logger.info(f"ğŸ¯ [DISPLAY_DECISION] {action} â†’ NEVER â†’ Solo respuesta humana")
            return False
        elif display_mode == "special":
            self.logger.info(f"ğŸ¯ [DISPLAY_DECISION] {action} â†’ SPECIAL â†’ Manejo especÃ­fico")
            return False  # Las acciones especiales manejan su propia visualizaciÃ³n
        elif display_mode == "conditional":
            # DecisiÃ³n basada en cantidad de datos
            should_show = data_count <= self.display_thresholds["max_auto_display"]
            decision = "Mostrar" if should_show else "Solo resumen"
            self.logger.info(f"ğŸ¯ [DISPLAY_DECISION] {action} â†’ CONDITIONAL â†’ {decision} ({data_count} registros)")
            return should_show
        else:
            # Fallback: comportamiento por defecto
            self.logger.info(f"ğŸ¯ [DISPLAY_DECISION] {action} â†’ DEFAULT â†’ Condicional ({data_count} registros)")
            return data_count <= self.display_thresholds["max_auto_display"]





    def process_command(self, command_data, current_pdf=None, original_query=None, conversation_context=None):
        """Procesa un comando y devuelve el resultado"""
        if not command_data:
            return False, "No se pudo entender tu solicitud. Â¿PodrÃ­as reformularla?", {}

        # AÃ±adir consulta original para el bypass SQL
        if original_query and isinstance(command_data, dict):
            command_data["original_query"] = original_query

        # NUEVO: AÃ±adir contexto conversacional
        if conversation_context and isinstance(command_data, dict):
            command_data["conversation_context"] = conversation_context

        # Procesar el comando
        accion = command_data.get("accion", "desconocida")
        parametros = command_data.get("parametros", {})

        # Si es una transformaciÃ³n y hay un PDF cargado, manejarla especialmente
        if accion == "transformar_constancia" and current_pdf:
            # Verificar si el usuario quiere guardar los datos
            if "guardar" in parametros and parametros["guardar"]:
                parametros["guardar_alumno"] = True
            elif "guardar_alumno" in parametros and parametros["guardar_alumno"]:
                # Asegurarse de que el valor sea booleano
                parametros["guardar_alumno"] = True

            # Si hay un PDF cargado, usar esa ruta
            if not parametros.get("ruta_archivo"):
                parametros["ruta_archivo"] = current_pdf

            # Para transformaciones, devolver directamente el comando sin ejecutar
            # El ChatWindow se encargarÃ¡ de manejar la transformaciÃ³n
            return True, "Iniciando transformaciÃ³n de constancia...", {
                "accion": "transformar_constancia",
                "parametros": parametros
            }

        # Si es guardar alumno desde PDF y hay un PDF cargado, usar esa ruta
        if accion == "guardar_alumno_pdf" and current_pdf:
            if not parametros.get("ruta_archivo"):
                parametros["ruta_archivo"] = current_pdf

            command_data["parametros"] = parametros

        # Ejecutar con MasterInterpreter
        return self._execute_with_master_interpreter(command_data, current_pdf=current_pdf)

    def _execute_with_master_interpreter(self, command_data, current_pdf=None):
        """Ejecuta comando con sistema de plantillas SQL (simplificado)"""
        try:
            accion = command_data.get("accion", "")
            parametros = command_data.get("parametros", {})
            original_query = command_data.get("original_query", "")
            conversation_context = command_data.get("conversation_context", {})

            self.logger.debug(f"Procesando: {accion}")
            self.logger.debug(f"Query original: {original_query}")

            # Preparar consulta para procesar
            consulta_para_procesar = original_query or parametros.get("consulta_original", "")

            if not consulta_para_procesar:
                return False, "No se pudo determinar la consulta a procesar", {}

            # ğŸ¯ FLUJO PRINCIPAL: MASTERINTERPRETER
            self.logger.info("ğŸ¯ [MESSAGEPROCESSOR] Procesando con MasterInterpreter (ya inicializado)")

            self.logger.debug(f"Enviando a MasterInterpreter: '{consulta_para_procesar}'")

            # Crear contexto de interpretaciÃ³n
            from app.core.ai.interpretation.base_interpreter import InterpretationContext

            context = InterpretationContext(
                user_message=consulta_para_procesar,
                conversation_state=conversation_context.get("conversation_state", {}),
                user_preferences={}
            )

            # ğŸ†• AGREGAR PDF_PANEL AL CONTEXTO PARA TRANSFORMACIONES
            if hasattr(self, 'pdf_panel') and self.pdf_panel:
                context.pdf_panel = self.pdf_panel

            # CORREGIDO: Agregar historial conversacional desde self.conversation_history
            if self.conversation_history:
                context.conversation_history = self.conversation_history
                self.logger.debug(f"Pasando {len(self.conversation_history)} mensajes de contexto conversacional")

            # TambiÃ©n agregar resultados de Ãºltima consulta para referencias
            if self.last_query_results:
                context.last_query_results = self.last_query_results
                self.logger.debug(f"Pasando resultados de Ãºltima consulta: '{self.last_query_results['user_query']}'")

            # Mantener contexto externo si existe
            if conversation_context.get("conversation_history"):
                context.external_conversation_history = conversation_context["conversation_history"]
            if conversation_context.get("recent_messages"):
                context.recent_messages = conversation_context["recent_messages"]

            # ğŸ¯ PROCESAMIENTO CON CONTEXTO CONVERSACIONAL ACTIVADO
            conversation_stack = self.conversation_stack  # â† USAR PILA REAL

            # ğŸ›‘ PAUSA ESTRATÃ‰GICA #6: CONVERSATION_STACK ANTES DE PROCESAR
            import os
            if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                print(f"\nğŸ›‘ [CONVERSATION_STACK] ESTADO ANTES DE PROCESAR:")
                print(f"    â”œâ”€â”€ ğŸ“ Consulta: '{consulta_para_procesar}'")
                print(f"    â”œâ”€â”€ ğŸ“Š Niveles en stack: {len(conversation_stack)}")
                print(f"    â”œâ”€â”€ ğŸ§  Nueva Arquitectura: MASTER decide TODO sobre contexto")

                if conversation_stack:
                    print(f"    â”œâ”€â”€ ğŸ“‹ CONTEXTO DISPONIBLE PARA MASTER:")
                    for i, nivel in enumerate(conversation_stack, 1):
                        query = nivel.get('query', 'N/A')
                        data_count = nivel.get('row_count', 0)
                        awaiting = nivel.get('awaiting', 'N/A')
                        action_type = nivel.get('action_type', 'N/A')
                        query_type = nivel.get('query_type', 'N/A')

                        print(f"    â”‚   NIVEL {i}: '{query}'")
                        print(f"    â”‚   â”œâ”€â”€ Elementos: {data_count}")
                        print(f"    â”‚   â”œâ”€â”€ Esperando: {awaiting}")
                        print(f"    â”‚   â”œâ”€â”€ Tipo acciÃ³n: {action_type}")
                        print(f"    â”‚   â”œâ”€â”€ Tipo query: {query_type}")

                        if nivel.get('data') and len(nivel['data']) <= 3:
                            print(f"    â”‚   â””â”€â”€ Datos disponibles:")
                            for j, item in enumerate(nivel['data'][:3], 1):
                                nombre = item.get('nombre', 'N/A')
                                id_alumno = item.get('id', item.get('alumno_id', 'N/A'))
                                print(f"    â”‚       {j}. {nombre} (ID: {id_alumno})")
                        elif nivel.get('data') and len(nivel['data']) > 3:
                            print(f"    â”‚   â””â”€â”€ Lista grande: {data_count} elementos (regenerable)")
                        print(f"    â”‚")
                else:
                    print(f"    â””â”€â”€ Stack VACÃO - Primera consulta")

                print(f"    â””â”€â”€ Presiona ENTER para que Master analice contexto...")
                input()

            # ğŸ”§ CRÃTICO: Agregar conversation_stack al context para que llegue al Student
            context.conversation_stack = conversation_stack

            if conversation_stack:
                self.logger.info(f"ğŸ¯ CONTEXTO ACTIVO - {len(conversation_stack)} niveles disponibles")
            else:
                self.logger.info("ğŸ¯ CONTEXTO VACÃO - Procesando consulta individual")



            result = self.master_interpreter.interpret(context, conversation_stack, current_pdf=current_pdf)

            if result:
                # ğŸ” [DEBUG] RASTREO COMPLETO DEL FLUJO
                self.logger.info(f"ğŸ” [DEBUG] Resultado recibido: action='{result.action}'")

                # ğŸ¯ DECISIÃ“N INTELIGENTE BASADA EN CONFIGURACIÃ“N DE ACCIONES
                data = result.parameters.get("data", [])
                data_count = len(data) if isinstance(data, list) else 0
                self.logger.info(f"ğŸ” [DEBUG] Datos extraÃ­dos: {data_count} elementos")

                if self.should_display_data(result.action, data_count):
                    self.logger.info(f"ğŸ” [DEBUG] Entrando en should_display_data=True para '{result.action}'")
                    message = result.parameters.get("human_response",
                                                   result.parameters.get("message", "Consulta procesada"))

                    # ğŸ“ [STUDENT] Reporte procesado - Master decide contexto



                    # ğŸ§  NUEVA ARQUITECTURA: MASTER COMO CEREBRO CENTRAL DEL CONTEXTO
                    # ================================================================
                    # - MASTER decide TODO sobre el conversation_stack
                    # - Student solo reporta lo que hizo
                    # - Master usa contexto completo para anÃ¡lisis inicial y respuesta final
                    # - SincronizaciÃ³n perfecta entre anÃ¡lisis y respuesta
                    # - SIEMPRE agregar resultados relevantes al contexto
                    # ================================================================

                    # Preparar datos para el stack
                    stack_data = {
                        "data": data,
                        "row_count": len(data),
                        "sql_executed": result.parameters.get("sql_executed", ""),
                        "action_type": result.action,
                        "query_type": "search_results"
                    }

                    # Determinar tipo de continuaciÃ³n esperada basado en los datos
                    if len(data) == 1:
                        awaiting_type = "action"  # Un elemento especÃ­fico - probable acciÃ³n
                    elif len(data) > 1 and len(data) <= 50:
                        awaiting_type = "filter"  # Lista manejable - probable filtro
                    else:
                        awaiting_type = "analysis"  # Lista grande - probable anÃ¡lisis

                    self.logger.info(f"ğŸ§  [MASTER DECIDE] Agregando {len(data)} resultados al contexto (esperando: {awaiting_type})")

                    # SIEMPRE agregar al conversation_stack
                    self.add_to_conversation_stack(
                        consulta_para_procesar,
                        stack_data,
                        awaiting_type
                    )

                    # ğŸ¯ CONFIGURAR DATOS ESTRUCTURADOS PARA DATADISPLAYMANAGER
                    # (data ya extraÃ­do arriba)
                    row_count = result.parameters.get("row_count", data_count)

                    # Si hay datos, configurar para mostrar con DataDisplayManager
                    if data and data_count > 0:
                        self.logger.info(f"ğŸ¯ Datos estructurados detectados ({row_count} registros) - Configurando para DataDisplayManager")

                        # Configurar parÃ¡metros para DataDisplayManager
                        formatted_parameters = result.parameters.copy()
                        formatted_parameters["action"] = "show_data"

                        # Mantener datos en formato original para que DataDisplayManager los detecte
                        # No necesitamos agregar clave "alumnos" - DataDisplayManager lo detecta automÃ¡ticamente

                        # NUEVO: Guardar conversaciÃ³n con resultados formateados
                        self.logger.debug(f"Guardando conversaciÃ³n con datos estructurados: '{consulta_para_procesar}' -> '{message[:50]}...'")
                        self.add_to_conversation(consulta_para_procesar, message, formatted_parameters)
                        self.logger.debug(f"Total mensajes en historial: {len(self.conversation_history)}")



                        return True, message, formatted_parameters

                    else:
                        # Sin datos estructurados, procesar normalmente
                        # NUEVO: Guardar conversaciÃ³n con resultados
                        self.logger.debug(f"Guardando conversaciÃ³n: '{consulta_para_procesar}' -> '{message[:50]}...'")
                        self.add_to_conversation(consulta_para_procesar, message, result.parameters)
                        self.logger.debug(f"Total mensajes en historial: {len(self.conversation_history)}")

                        return True, message, result.parameters

                elif result.action in ["ayuda_proporcionada", "ayuda_funcionalidades", "ayuda_solucion", "ayuda_ejemplo",
                                     "conversacion_general", "estadisticas_generadas",
                                     "constancia_generada", "constancia_requiere_aclaracion"]:
                    self.logger.info(f"ğŸ” [DEBUG] Entrando en condiciÃ³n ayuda/conversaciÃ³n para '{result.action}'")
                    # ğŸ› ï¸ USAR EL MENSAJE REAL GENERADO POR EL HELPINTERPRETER
                    message = result.parameters.get("message",
                                                   result.parameters.get("mensaje", "Respuesta procesada"))

                    # NUEVO: Guardar conversaciÃ³n
                    self.add_to_conversation(consulta_para_procesar, message, result.parameters)

                    return True, message, result.parameters

                # ğŸ†• NUEVAS ACCIONES DE CONSTANCIA
                elif result.action == "constancia_preview":
                    self.logger.info("ğŸ¯ [DEBUG] CONSTANCIA_PREVIEW DETECTADA - Procesando...")
                    message = result.parameters.get("message", "Vista previa de constancia generada")

                    # ğŸ§  MASTER DECIDE - SIEMPRE AGREGAR CONSTANCIAS AL CONVERSATION_STACK
                    # Extraer informaciÃ³n del alumno para el contexto
                    raw_data = result.parameters.get("data", {})

                    # ğŸ”§ MANEJAR TANTO DICT COMO LIST
                    alumno_data = None
                    if isinstance(raw_data, list) and len(raw_data) > 0:
                        # Si es lista, tomar el primer elemento
                        first_item = raw_data[0]
                        if isinstance(first_item, dict) and "data" in first_item:
                            # Estructura: [{"data": {"alumno": {...}, "ruta_archivo": "..."}}]
                            alumno_data = first_item["data"].get("alumno", {})
                            ruta_archivo = first_item["data"].get("ruta_archivo", "")
                        elif isinstance(first_item, dict) and "alumno" in first_item:
                            # Estructura: [{"alumno": {...}, "ruta_archivo": "..."}]
                            alumno_data = first_item.get("alumno", {})
                            ruta_archivo = first_item.get("ruta_archivo", "")
                    elif isinstance(raw_data, dict):
                        # Si es dict directo
                        if "alumno" in raw_data:
                            alumno_data = raw_data.get("alumno", {})
                            ruta_archivo = raw_data.get("ruta_archivo", "")
                        else:
                            alumno_data = raw_data
                            ruta_archivo = raw_data.get("ruta_archivo", "")

                    self.logger.info(f"ğŸ”§ [DEBUG] Estructura detectada - raw_data type: {type(raw_data)}, alumno_data: {bool(alumno_data)}")

                    if alumno_data and isinstance(alumno_data, dict) and alumno_data.get("nombre"):
                        # Crear datos para el conversation_stack
                        stack_data = {
                            "data": [alumno_data],  # Lista con el alumno especÃ­fico
                            "row_count": 1,
                            "sql_executed": f"Constancia generada para {alumno_data.get('nombre')}",
                            "action_type": "constancia_preview",
                            "query_type": "constancia_generated"
                        }

                        self.logger.info(f"ğŸ§  [MASTER DECIDE] Agregando constancia al contexto: {alumno_data.get('nombre')} (ID: {alumno_data.get('id')})")
                        self.add_to_conversation_stack(
                            consulta_para_procesar,
                            stack_data,
                            "action"  # Espera acciÃ³n sobre este alumno especÃ­fico
                        )
                        # Nota: La pausa #9 se ejecuta automÃ¡ticamente en add_to_conversation_stack()

                    # ğŸ¯ AGREGAR ACCIÃ“N Y ARCHIVO A LOS PARÃMETROS PARA QUE LLEGUE AL CHATENGINE
                    parameters_with_action = result.parameters.copy()
                    parameters_with_action["action"] = result.action  # â† PRESERVAR ACCIÃ“N

                    # ğŸ”§ CRÃTICO: Preservar ruta_archivo y alumno para que ChatEngine pueda mostrar PDF
                    if alumno_data and isinstance(alumno_data, dict):
                        # Preservar datos del alumno para el contexto del panel
                        parameters_with_action["alumno"] = alumno_data
                        self.logger.info(f"ğŸ”§ [DEBUG] Datos del alumno agregados para contexto del panel: {alumno_data.get('nombre')}")

                        # Buscar ruta_archivo en la estructura original
                        if isinstance(raw_data, list) and len(raw_data) > 0:
                            first_item = raw_data[0]
                            if isinstance(first_item, dict) and "data" in first_item and "ruta_archivo" in first_item["data"]:
                                parameters_with_action["ruta_archivo"] = first_item["data"]["ruta_archivo"]
                                self.logger.info(f"ğŸ”§ [DEBUG] Archivo PDF agregado a parÃ¡metros: {first_item['data']['ruta_archivo']}")
                            elif isinstance(first_item, dict) and "ruta_archivo" in first_item:
                                parameters_with_action["ruta_archivo"] = first_item["ruta_archivo"]
                                self.logger.info(f"ğŸ”§ [DEBUG] Archivo PDF agregado a parÃ¡metros: {first_item['ruta_archivo']}")
                    else:
                        self.logger.warning(f"ğŸš¨ [DEBUG] NO se encontrÃ³ alumno_data vÃ¡lido: {raw_data}")

                    # ğŸ›‘ PAUSA ESTRATÃ‰GICA #7: CONVERSATION_STACK DESPUÃ‰S DE PROCESAR
                    import os
                    if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                        print(f"\nğŸ›‘ [CONVERSATION_STACK] ESTADO DESPUÃ‰S DE PROCESAR:")
                        print(f"    â”œâ”€â”€ ğŸ“ Consulta procesada: '{consulta_para_procesar}'")
                        print(f"    â”œâ”€â”€ ğŸ¯ AcciÃ³n ejecutada: {result.action}")
                        print(f"    â”œâ”€â”€ ğŸ“Š Niveles en stack: {len(self.conversation_stack)}")
                        print(f"    â”œâ”€â”€ ğŸ§  Master decidiÃ³ agregar resultado al contexto")

                        if self.conversation_stack:
                            print(f"    â”œâ”€â”€ ğŸ“‹ CONTEXTO ACTUALIZADO:")
                            for i, nivel in enumerate(self.conversation_stack, 1):
                                query = nivel.get('query', 'N/A')
                                data_count = nivel.get('row_count', 0)
                                awaiting = nivel.get('awaiting', 'N/A')
                                action_type = nivel.get('action_type', 'N/A')
                                query_type = nivel.get('query_type', 'N/A')

                                print(f"    â”‚   NIVEL {i}: '{query}'")
                                print(f"    â”‚   â”œâ”€â”€ Elementos: {data_count}")
                                print(f"    â”‚   â”œâ”€â”€ Esperando: {awaiting}")
                                print(f"    â”‚   â”œâ”€â”€ Tipo acciÃ³n: {action_type}")
                                print(f"    â”‚   â”œâ”€â”€ Tipo query: {query_type}")

                                if nivel.get('data') and len(nivel['data']) <= 3:
                                    print(f"    â”‚   â””â”€â”€ Datos disponibles:")
                                    for j, item in enumerate(nivel['data'][:3], 1):
                                        nombre = item.get('nombre', 'N/A')
                                        id_alumno = item.get('id', item.get('alumno_id', 'N/A'))
                                        print(f"    â”‚       {j}. {nombre} (ID: {id_alumno})")
                                elif nivel.get('data') and len(nivel['data']) > 3:
                                    print(f"    â”‚   â””â”€â”€ Lista grande: {data_count} elementos")
                                print(f"    â”‚")
                        else:
                            print(f"    â””â”€â”€ Stack VACÃO")

                        print(f"    â””â”€â”€ ğŸ¯ PRÃ“XIMA CONSULTA podrÃ¡ usar ESTE contexto")
                        print(f"    â””â”€â”€ Presiona ENTER para continuar...")
                        input()

                    self.add_to_conversation(consulta_para_procesar, message, parameters_with_action)
                    return True, message, parameters_with_action

                elif result.action in ["constancia_confirmada", "constancia_abierta", "constancia_cancelada"]:
                    message = result.parameters.get("message", "AcciÃ³n de constancia completada")

                    # Limpiar pila conversacional despuÃ©s de acciÃ³n de constancia
                    self.clear_conversation_stack()

                    self.add_to_conversation(consulta_para_procesar, message, result.parameters)
                    return True, message, result.parameters

                elif result.action == "transformar_constancia":
                    # Manejar transformaciÃ³n de constancia
                    data_with_action = result.parameters.copy()
                    data_with_action["accion"] = "transformar_constancia"
                    return True, result.parameters.get("message", "TransformaciÃ³n iniciada"), data_with_action

                elif result.action == "constancia_error":
                    # ğŸ†• USAR MENSAJE AMIGABLE EN LUGAR DE ERROR TÃ‰CNICO
                    error_message = result.parameters.get("message", "Error al procesar constancia")
                    return False, error_message, result.parameters

                elif result.action == "transformation_preview":
                    # ğŸ†• MANEJAR VISTA PREVIA DE TRANSFORMACIÃ“N
                    message = result.parameters.get("message", "Vista previa de transformaciÃ³n generada")

                    # ğŸ¯ AGREGAR ACCIÃ“N A LOS PARÃMETROS PARA QUE LLEGUE AL CHATENGINE
                    parameters_with_action = result.parameters.copy()
                    parameters_with_action["action"] = result.action  # â† PRESERVAR ACCIÃ“N



                    self._handle_transformation_preview(parameters_with_action)
                    return True, message, parameters_with_action

                elif result.action == "transformation_error":
                    # ğŸ†• MANEJAR ERRORES DE TRANSFORMACIÃ“N
                    error_message = result.parameters.get("message", "Error al transformar PDF")
                    return False, error_message, result.parameters

                # NUEVO: Manejar resultados del sistema conversacional
                elif result.action == "seleccion_realizada":
                    message = result.parameters.get("message", "SelecciÃ³n realizada")

                    # Agregar a la pila conversacional si es necesario
                    elemento_seleccionado = result.parameters.get("elemento_seleccionado", {})
                    if elemento_seleccionado:
                        self.add_to_conversation_stack(
                            consulta_para_procesar,
                            {"data": [elemento_seleccionado], "row_count": 1, "message": message},
                            "action"  # Espera acciÃ³n sobre el elemento seleccionado
                        )

                    self.add_to_conversation(consulta_para_procesar, message, result.parameters)
                    return True, message, result.parameters

                elif result.action in ["accion_realizada", "confirmacion_realizada", "especificacion_realizada"]:
                    message = result.parameters.get("message", "AcciÃ³n procesada")
                    self.add_to_conversation(consulta_para_procesar, message, result.parameters)
                    return True, message, result.parameters

                elif result.action == "continuacion_error":
                    # Error en continuaciÃ³n - limpiar pila y proceder normalmente
                    self.clear_conversation_stack()
                    error_message = result.parameters.get("message", "Error en continuaciÃ³n conversacional")
                    return False, error_message, result.parameters

                elif result.action == "continuacion_inteligente":
                    # NUEVO: Manejo de continuaciÃ³n inteligente
                    message = result.parameters.get("message", "ContinuaciÃ³n procesada")

                    # ğŸ§  MASTER DECIDE - Agregar continuaciones inteligentes al contexto
                    # Las continuaciones inteligentes siempre se agregan para mantener contexto
                    stack_data = {
                        "data": result.parameters.get("data", []),
                        "row_count": result.parameters.get("row_count", 0),
                        "sql_executed": result.parameters.get("sql_executed", "ContinuaciÃ³n inteligente"),
                        "action_type": result.action,
                        "query_type": "intelligent_continuation"
                    }

                    self.logger.info(f"ğŸ§  [MASTER DECIDE] Agregando continuaciÃ³n inteligente al contexto")
                    self.add_to_conversation_stack(consulta_para_procesar, stack_data, "action")

                    # ğŸ”§ IMPORTANTE: Agregar return que faltaba
                    self.add_to_conversation(consulta_para_procesar, message, result.parameters)
                    return True, message, result.parameters

                elif result.action == "solicitar_aclaracion":
                    # ğŸ”„ COMUNICACIÃ“N BIDIRECCIONAL: Master solicita aclaraciÃ³n al usuario
                    message = result.parameters.get("message", "Necesito mÃ¡s informaciÃ³n")

                    # ğŸ¯ AGREGAR ACCIÃ“N A LOS PARÃMETROS PARA QUE LLEGUE AL CHATENGINE
                    parameters_with_action = result.parameters.copy()
                    parameters_with_action["action"] = result.action  # â† PRESERVAR ACCIÃ“N

                    # ğŸ”„ GUARDAR ESTADO DE ESPERA DE CLARIFICACIÃ“N
                    # Crear conversation_context si no existe
                    if not hasattr(self, 'conversation_context'):
                        self.conversation_context = {}
                    self.conversation_context["waiting_for"] = "clarification"
                    self.conversation_context["clarification_info"] = result.parameters

                    self.logger.info(f"ğŸ”„ [MESSAGEPROCESSOR] Solicitando aclaraciÃ³n al usuario")
                    return True, message, parameters_with_action

                else:
                    self.logger.info(f"ğŸ” [DEBUG] Entrando en condiciÃ³n ELSE (otros resultados) para '{result.action}'")
                    # Otros tipos de resultado
                    message = result.parameters.get("mensaje",
                                                   result.parameters.get("message", "Consulta procesada"))

                    # ğŸ§  MASTER DECIDE - Evaluar si otros resultados necesitan contexto
                    # Solo agregar al stack si hay datos relevantes
                    data = result.parameters.get("data", [])
                    if data and len(data) > 0:
                        stack_data = {
                            "data": data,
                            "row_count": len(data),
                            "sql_executed": result.parameters.get("sql_executed", f"Resultado: {result.action}"),
                            "action_type": result.action,
                            "query_type": "other_result"
                        }

                        self.logger.info(f"ğŸ§  [MASTER DECIDE] Agregando resultado '{result.action}' al contexto ({len(data)} elementos)")
                        self.add_to_conversation_stack(consulta_para_procesar, stack_data, "action")

                    self.add_to_conversation(consulta_para_procesar, message, result.parameters)
                    return True, message, result.parameters
            else:
                return False, "No se pudo procesar la consulta", {}

        except Exception as e:
            self.logger.error(f"Error en _execute_with_master_interpreter: {str(e)}")
            return False, f"Error procesando consulta: {str(e)}", {}





    def get_current_time(self):
        """Obtiene la hora actual en formato HH:MM"""
        now = datetime.now()
        return now.strftime("%H:%M")

    def get_random_greeting(self):
        """Devuelve un saludo aleatorio"""
        return random.choice(self.greeting_phrases)

    def get_random_success_phrase(self):
        """Devuelve una frase de Ã©xito aleatoria"""
        return random.choice(self.success_phrases)

    def add_to_conversation_stack(self, query, result_data, awaiting_type):
        """
        Agrega nivel a la pila conversacional segÃºn PROTOCOLO_COMUNICACION_BIDIRECCIONAL.md

        Args:
            query (str): Consulta del usuario que generÃ³ estos datos
            result_data (dict): Datos del resultado (data, row_count, sql_executed)
            awaiting_type (str): Tipo de continuaciÃ³n esperada (analysis, action, confirmation, selection)
        """
        from datetime import datetime



        # ğŸ”§ NORMALIZAR DATOS: SIEMPRE LISTA
        raw_data = result_data.get("data", [])
        if isinstance(raw_data, dict):
            # Si es diccionario, convertir a lista con un elemento
            normalized_data = [raw_data]
            self.logger.info(f"ğŸ”§ [STACK] Normalizando diccionario a lista: {list(raw_data.keys())}")
        elif isinstance(raw_data, list):
            # Si ya es lista, usar tal como estÃ¡
            normalized_data = raw_data
        else:
            # Si es otro tipo, crear lista vacÃ­a
            normalized_data = []
            self.logger.warning(f"ğŸ”§ [STACK] Tipo de datos no reconocido: {type(raw_data)}")

        # Estructura segÃºn PROTOCOLO_COMUNICACION_BIDIRECCIONAL.md
        level = {
            "id": len(self.conversation_stack) + 1,
            "query": query,
            "data": normalized_data,  # âœ… SIEMPRE LISTA
            "row_count": result_data.get("row_count", len(normalized_data)),
            "sql_executed": result_data.get("sql_executed", ""),
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "awaiting": awaiting_type,
            "active": True,
            "context_available": {
                "positions": len(normalized_data) > 0,  # "el segundo", "el tercero"
                "names": len(normalized_data) > 0,      # "FRANCO", "VALERIA"
                "actions": awaiting_type in ["confirmation", "action"],  # "constancia para"
                "filters": awaiting_type in ["analysis", "selection"]    # "que tengan"
            },
            "priority": 0.9  # MÃ¡s reciente = mayor prioridad
        }

        # Actualizar prioridades de niveles anteriores
        for existing_level in self.conversation_stack:
            existing_level["priority"] *= 0.7  # Reducir prioridad

        self.conversation_stack.append(level)
        self.awaiting_continuation = True

        self.logger.info(f"ğŸ“‹ [CONVERSATION_STACK] Nivel agregado:")
        self.logger.info(f"    â”œâ”€â”€ Query: '{query}'")
        self.logger.info(f"    â”œâ”€â”€ Datos: {len(result_data.get('data', []))} elementos")
        self.logger.info(f"    â”œâ”€â”€ Esperando: {awaiting_type}")
        self.logger.info(f"    â””â”€â”€ Total niveles: {len(self.conversation_stack)}")

        # ğŸ”” VERIFICAR SALUD DE LA SESIÃ“N
        session_warning = self._check_session_health()
        if session_warning:
            self.logger.info(f"ğŸ”” [SESSION_HEALTH] {session_warning['type'].upper()}: {session_warning['message']}")
            self._show_session_warning(session_warning)

            # ğŸ§¹ LIMPIEZA AUTOMÃTICA EN CASO CRÃTICO
            if session_warning['type'] == 'critical':
                self._auto_cleanup_context()

        # ğŸ›‘ PAUSA ESPECÃFICA PARA AGREGAR AL CONVERSATION_STACK
        import os
        if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
            print(f"\nğŸ›‘ [CONVERSATION_STACK] NUEVO NIVEL AGREGADO:")
            print(f"    â”œâ”€â”€ ğŸ“ Query: '{query}'")
            print(f"    â”œâ”€â”€ ğŸ“Š Datos: {len(result_data.get('data', []))} elementos")
            print(f"    â”œâ”€â”€ ğŸ¯ Esperando: {awaiting_type}")
            print(f"    â”œâ”€â”€ ğŸ“‹ Total niveles: {len(self.conversation_stack)}")
            print(f"    â”œâ”€â”€ ğŸ§  NUEVA ARQUITECTURA: Master decidiÃ³ agregar este resultado")

            # Mostrar el nivel que se acaba de agregar
            if result_data.get('data'):
                action_type = result_data.get('action_type', 'N/A')
                query_type = result_data.get('query_type', 'N/A')
                print(f"    â”œâ”€â”€ ğŸ”§ Tipo acciÃ³n: {action_type}")
                print(f"    â”œâ”€â”€ ğŸ”§ Tipo query: {query_type}")
                print(f"    â”œâ”€â”€ ğŸ“‹ Datos agregados:")

                for i, item in enumerate(result_data['data'][:3], 1):
                    nombre = item.get('nombre', 'N/A')
                    id_alumno = item.get('id', item.get('alumno_id', 'N/A'))
                    print(f"    â”‚   {i}. {nombre} (ID: {id_alumno})")

                if len(result_data['data']) > 3:
                    print(f"    â”‚   ... y {len(result_data['data']) - 3} mÃ¡s")

            print(f"    â”œâ”€â”€ ğŸ¯ PRÃ“XIMAS CONSULTAS podrÃ¡n usar este contexto")
            print(f"    â””â”€â”€ Presiona ENTER para continuar...")
            input()



    def _check_session_health(self):
        """
        ğŸ”” VERIFICAR SALUD DE LA SESIÃ“N
        Analiza el estado del conversation_stack y genera avisos apropiados
        """
        try:
            levels = len(self.conversation_stack)

            # Calcular tamaÃ±o aproximado del contexto
            context_size_kb = self._estimate_context_size()

            if levels >= 20:
                return {
                    "type": "critical",
                    "message": f"ğŸš¨ Esta sesiÃ³n estÃ¡ muy cargada ({levels} niveles, {context_size_kb:.1f}KB). Te recomiendo crear una nueva sesiÃ³n para mejor rendimiento.",
                    "action": "suggest_new_session",
                    "levels": levels,
                    "size_kb": context_size_kb
                }
            elif levels >= 15:
                return {
                    "type": "warning",
                    "message": f"âš ï¸ Esta sesiÃ³n tiene mucho contexto ({levels} niveles, {context_size_kb:.1f}KB). Â¿Quieres continuar o prefieres una nueva sesiÃ³n?",
                    "action": "offer_choice",
                    "levels": levels,
                    "size_kb": context_size_kb
                }
            elif levels >= 12:
                return {
                    "type": "info",
                    "message": f"ğŸ’¡ Esta sesiÃ³n estÃ¡ acumulando contexto ({levels} niveles). Considera crear nueva sesiÃ³n para consultas independientes.",
                    "action": "gentle_suggestion",
                    "levels": levels,
                    "size_kb": context_size_kb
                }

            return None

        except Exception as e:
            self.logger.error(f"Error verificando salud de sesiÃ³n: {e}")
            return None

    def _estimate_context_size(self):
        """Estima el tamaÃ±o del contexto en KB"""
        try:
            import sys
            total_size = 0

            # TamaÃ±o del conversation_stack
            for level in self.conversation_stack:
                total_size += sys.getsizeof(str(level))

            # TamaÃ±o del conversation_history
            for message in self.conversation_history:
                total_size += sys.getsizeof(str(message))

            return total_size / 1024  # Convertir a KB

        except Exception as e:
            self.logger.error(f"Error estimando tamaÃ±o de contexto: {e}")
            return 0.0

    def _show_session_warning(self, warning_data):
        """
        ğŸ”” MOSTRAR AVISO DE SESIÃ“N EN EL CHAT
        Agrega un mensaje del sistema informando sobre el estado de la sesiÃ³n
        """
        try:
            # Crear mensaje del sistema con el aviso
            warning_message = {
                "role": "system",
                "content": warning_data["message"],
                "timestamp": datetime.now().isoformat(),
                "type": "session_warning",
                "warning_type": warning_data["type"],
                "metadata": {
                    "levels": warning_data.get("levels", 0),
                    "size_kb": warning_data.get("size_kb", 0.0),
                    "action": warning_data.get("action", "none")
                }
            }

            # Agregar al historial conversacional
            self.conversation_history.append(warning_message)

            self.logger.info(f"ğŸ”” [SESSION_WARNING] Aviso mostrado en chat: {warning_data['type']}")

        except Exception as e:
            self.logger.error(f"Error mostrando aviso de sesiÃ³n: {e}")

    def clear_conversation_stack(self):
        """Limpiar pila conversacional"""
        niveles_eliminados = len(self.conversation_stack)



        self.conversation_stack = []
        self.awaiting_continuation = False

        self.logger.info(f"ğŸ—‘ï¸ [CONVERSATION_STACK] Limpiado - {niveles_eliminados} niveles eliminados")

    def _auto_cleanup_context(self):
        """
        ğŸ§¹ LIMPIEZA AUTOMÃTICA DEL CONTEXTO
        Mantiene solo los niveles mÃ¡s recientes cuando se alcanza el lÃ­mite
        """
        try:
            max_levels = 15
            if len(self.conversation_stack) > max_levels:
                # Mantener solo los Ãºltimos 10 niveles
                keep_levels = 10
                removed_levels = len(self.conversation_stack) - keep_levels

                self.conversation_stack = self.conversation_stack[-keep_levels:]

                self.logger.info(f"ğŸ§¹ [AUTO_CLEANUP] Contexto optimizado: {removed_levels} niveles antiguos eliminados, {keep_levels} mantenidos")

                # Mostrar aviso de limpieza automÃ¡tica
                cleanup_message = {
                    "role": "system",
                    "content": f"ğŸ§¹ Contexto optimizado automÃ¡ticamente. Mantuve los Ãºltimos {keep_levels} niveles de conversaciÃ³n para mejor rendimiento.",
                    "timestamp": datetime.now().isoformat(),
                    "type": "auto_cleanup",
                    "metadata": {
                        "removed_levels": removed_levels,
                        "kept_levels": keep_levels
                    }
                }

                self.conversation_history.append(cleanup_message)

        except Exception as e:
            self.logger.error(f"Error en limpieza automÃ¡tica: {e}")



    def add_to_conversation(self, user_message, system_response, query_results=None):
        """Agregar intercambio al historial conversacional"""
        from datetime import datetime

        # Agregar mensaje del usuario
        self.conversation_history.append({
            'role': 'user',
            'content': user_message,
            'timestamp': datetime.now().strftime("%H:%M:%S")
        })

        # Agregar respuesta del sistema
        self.conversation_history.append({
            'role': 'system',
            'content': system_response,
            'timestamp': datetime.now().strftime("%H:%M:%S")
        })

        # Guardar resultados de la Ãºltima consulta para referencias
        if query_results:
            self.last_query_results = {
                'user_query': user_message,
                'results': query_results,
                'timestamp': datetime.now().strftime("%H:%M:%S")
            }

        # Mantener solo Ãºltimos 10 intercambios (20 mensajes)
        if len(self.conversation_history) > 20:
            self.conversation_history = self.conversation_history[-20:]

    def _get_conversation_context(self):
        """Genera contexto conversacional inteligente"""
        if not self.conversation_history:
            return ""

        # Ãšltimos 3 intercambios (6 mensajes)
        recent_messages = self.conversation_history[-6:]

        if not recent_messages:
            return ""

        context = """

=== CONTEXTO CONVERSACIONAL RECIENTE ===
ğŸ“ HISTORIAL DE LA SESIÃ“N ACTUAL:
"""

        for msg in recent_messages:
            role_emoji = "ğŸ‘¤" if msg['role'] == 'user' else "ğŸ¤–"
            # Truncar respuestas largas del sistema
            content = msg['content']
            if msg['role'] == 'system' and len(content) > 150:
                content = content[:150] + "..."

            context += f"{role_emoji} {msg['role'].title()}: {content}\n"

        # Agregar informaciÃ³n de Ãºltimos resultados si existen
        if self.last_query_results:
            context += f"""
ğŸ“Š ÃšLTIMA CONSULTA REALIZADA:
- Consulta: "{self.last_query_results['user_query']}"
- Resultados disponibles para referencias
"""

        context += """
ğŸ§  INSTRUCCIONES PARA USO INTELIGENTE DEL CONTEXTO:
- USA el contexto SOLO si la consulta actual hace referencia a informaciÃ³n anterior
- Si el usuario dice "el primero", "ese", "Ã©l", "ella" â†’ busca en el contexto
- Si dice "y cuÃ¡ntos...", "tambiÃ©n...", "ademÃ¡s..." â†’ relaciona con consulta anterior
- Si es una consulta completamente nueva â†’ IGNORA el contexto
- Si hay dudas sobre referencias â†’ pregunta al usuario para aclarar

EJEMPLOS DE CUÃNDO USAR CONTEXTO:
âœ… "CURP del tercero" (despuÃ©s de mostrar lista)
âœ… "constancia para Ã©l" (despuÃ©s de mostrar alumno especÃ­fico)
âœ… "y del turno vespertino" (despuÃ©s de consulta de grado)
âŒ "cuÃ¡ntos alumnos hay" (consulta independiente)
âŒ "buscar GarcÃ­a" (consulta nueva)
"""

        return context

    def _handle_transformation_preview(self, parameters: Dict[str, Any]):
        """Maneja vista previa de transformaciÃ³n de PDF"""
        try:
            files = parameters.get("files", [])
            if files and self.pdf_panel:
                pdf_path = files[0]
                self.logger.info(f"Cargando vista previa de transformaciÃ³n: {pdf_path}")

                # Cargar PDF transformado en el panel
                self.pdf_panel.show_pdf(pdf_path)

                # Establecer contexto de transformaciÃ³n completada
                alumno_data = parameters.get("alumno", {})
                transformation_info = parameters.get("transformation_info", {})

                if hasattr(self.pdf_panel, 'set_transformation_completed_context'):
                    self.pdf_panel.set_transformation_completed_context(
                        original_data=self.pdf_panel.pdf_data,
                        transformed_data=parameters.get("data", {}),
                        alumno_data=alumno_data
                    )

                self.logger.debug(f"Vista previa de transformaciÃ³n cargada: {transformation_info.get('tipo_transformacion', 'N/A')}")

        except Exception as e:
            self.logger.error(f"Error manejando vista previa de transformaciÃ³n: {e}")

    # ==========================================
    # MÃ‰TODOS DE GESTIÃ“N DE PILA CONVERSACIONAL
    # ==========================================





    # ğŸ—‘ï¸ MÃ‰TODOS DEL SISTEMA DE MEMORIA PROBLEMÃTICO ELIMINADOS
    # El sistema de plantillas SQL los reemplaza completamente
