"""
IntÃ©rprete maestro - Coordina todos los mÃ³dulos de interpretaciÃ³n
"""
from typing import Optional
from app.core.ai.interpretation.base_interpreter import InterpretationContext, InterpretationResult
from app.core.ai.interpretation.intention_detector import IntentionDetector
from app.core.ai.interpretation.student_query_interpreter import StudentQueryInterpreter
from app.core.ai.interpretation.master_knowledge import MasterKnowledge
from app.core.logging import get_logger
from app.core.config import Config

class MasterInterpreter:
    """
    ğŸ¯ INTÃ‰RPRETE MAESTRO - LÃDER INTELIGENTE DEL SISTEMA

    RESPONSABILIDADES:
    - Detectar intenciones con contexto estratÃ©gico completo
    - Dirigir al especialista correcto con informaciÃ³n precisa
    - Mantener memoria de interacciones para retroalimentaciÃ³n
    - ComunicaciÃ³n bidireccional con especialistas
    """

    def __init__(self, gemini_client):
        self.gemini_client = gemini_client
        self.logger = get_logger(__name__)

        # ğŸ§  INICIALIZAR CEREBRO DEL MASTER (CONOCIMIENTO PROFUNDO)
        self.knowledge = MasterKnowledge()
        self.logger.info("ğŸ§  [MASTER] Cerebro inicializado con conocimiento profundo del sistema")

        # ğŸ¯ CONTEXTO ESTRATÃ‰GICO DEL SISTEMA (SEGÃšN INTENCIONES_ACCIONES_DEFINITIVAS.md)
        self.system_map = {
            "StudentQueryInterpreter": {
                "handles": ["consulta_alumnos", "transformacion_pdf"],
                "sub_intentions": ["busqueda_simple", "busqueda_compleja", "estadisticas", "generar_constancia", "transformacion_pdf"],
                "capabilities": "Consultas de BD, documentos, anÃ¡lisis de 211 alumnos",
                "description": "Especialista en datos de alumnos y generaciÃ³n de documentos"
            },
            "HelpInterpreter": {
                "handles": ["ayuda_sistema"],
                "sub_intentions": ["pregunta_capacidades", "pregunta_tecnica"],
                "capabilities": "Ayuda y soporte tÃ©cnico del sistema",
                "description": "Especialista en ayuda y explicaciones del sistema"
            },
            "MasterInterpreter": {
                "handles": ["aclaracion_requerida"],
                "sub_intentions": ["multiple_interpretations", "incomplete_query", "ambiguous_reference"],
                "capabilities": "DetecciÃ³n de ambigÃ¼edades y comunicaciÃ³n directa con usuario",
                "description": "Master se delega a sÃ­ mismo para consultas ambiguas"
            }
        }

        # ğŸ’­ MEMORIA DE INTERACCIONES (RETROALIMENTACIÃ“N)
        self.interaction_memory = {
            "last_specialist": None,
            "last_result_summary": None,
            "conversation_flow": None,
            "specialist_feedback": None,
            "awaiting_continuation": False,
            "continuation_type": None
        }

        # ğŸ”§ INICIALIZAR COMPONENTES
        self.intention_detector = IntentionDetector(gemini_client)

        # ğŸ¯ LOGS DE DEPURACIÃ“N FORZADOS - CONTEXTO ESTRATÃ‰GICO COMPLETO
        self.logger.info("ğŸ¯ [MASTER] INICIALIZADO CON CONTEXTO ESTRATÃ‰GICO")
        self.logger.info(f"   â”œâ”€â”€ Especialistas disponibles: {len(self.system_map)}")
        self.logger.info(f"   â”œâ”€â”€ StudentQueryInterpreter: {self.system_map['StudentQueryInterpreter']['capabilities']}")
        self.logger.info(f"   â””â”€â”€ HelpInterpreter: {self.system_map['HelpInterpreter']['capabilities']}")

        # ğŸ§  [MASTER] Contexto estratÃ©gico inicializado
        self._log_strategic_context()

        # ğŸ¯ INICIALIZAR ESPECIALISTAS (DESPUÃ‰S DE MOSTRAR CONTEXTO MASTER)
        self.logger.info("ğŸ¯ [MASTER] Inicializando especialistas...")
        from app.core.config import Config
        db_path = Config.DB_PATH
        self.student_interpreter = StudentQueryInterpreter(db_path, gemini_client)

        from app.core.ai.interpretation.help_interpreter import HelpInterpreter
        self.help_interpreter = HelpInterpreter(gemini_client)
        self.logger.info("âœ… [MASTER] Especialistas inicializados correctamente")

    def interpret(self, context: InterpretationContext, conversation_stack=None, current_pdf=None) -> Optional[InterpretationResult]:
        """
        ğŸ¯ INTERPRETACIÃ“N MAESTRO CON CONTEXTO ESTRATÃ‰GICO COMPLETO

        FLUJO MEJORADO:
        1. AnÃ¡lisis con contexto estratÃ©gico completo
        2. DetecciÃ³n de intenciÃ³n con memoria de interacciones
        3. DelegaciÃ³n inteligente al especialista correcto
        4. ComunicaciÃ³n bidireccional y retroalimentaciÃ³n
        """
        try:
            # ğŸ¯ LOGS DE DEPURACIÃ“N FORZADOS
            self.logger.info("ğŸ¯ [MASTER] INICIANDO INTERPRETACIÃ“N CON CONTEXTO ESTRATÃ‰GICO")
            self.logger.info(f"   â”œâ”€â”€ Consulta: '{context.user_message}'")
            self.logger.info(f"   â”œâ”€â”€ Conversation_stack: {len(conversation_stack) if conversation_stack else 0} niveles")
            self.logger.info(f"   â””â”€â”€ Memoria anterior: {self.interaction_memory}")

            # ğŸ¯ ALMACENAR CONVERSATION_STACK PARA USO EN RESPUESTA FINAL
            self.current_conversation_stack = conversation_stack or []

            # ğŸ¯ PROCESAMIENTO CON CONTEXTO CONVERSACIONAL ACTIVADO
            context.conversation_stack = conversation_stack or []
            if context.conversation_stack:
                self.logger.info(f"ğŸ¯ [MASTER] Procesando con contexto - {len(context.conversation_stack)} niveles disponibles")
            else:
                self.logger.info("ğŸ¯ [MASTER] Procesando consulta individual")

            # PASO 1: DETECTAR INTENCIÃ“N CON CONTEXTO
            intention = self._detect_intention_with_context(context.user_message, context.conversation_stack)

            # PASO 2: RESOLVER REFERENCIAS CONTEXTUALES SI ES NECESARIO
            if intention.requiere_contexto and context.conversation_stack:
                # ğŸ§  RESOLUCIÃ“N INTELIGENTE CON LLM (DINÃMICO Y FLEXIBLE)
                result_or_intention = self._resolve_contextual_references(intention, context.conversation_stack, context.user_message)

                # Si devuelve un InterpretationResult, es una aclaraciÃ³n por ambigÃ¼edad
                if hasattr(result_or_intention, 'action') and result_or_intention.action == "aclaracion_requerida":
                    return result_or_intention

                # Si no, es una intention actualizada
                intention = result_or_intention

            # ğŸ§  [MASTER] IntenciÃ³n detectada y categorizada
            self.logger.info(f"ğŸ§  [MASTER] Analizando: \"{context.user_message}\" â†’ {intention.intention_type} ({intention.confidence})")

            # ğŸ”§ DEBUG: InformaciÃ³n detallada solo en modo debug
            from app.core.logging import debug_detailed
            debug_detailed(self.logger, f"ğŸ”§ [MASTER] Detalles: {intention.intention_type}/{intention.sub_intention}")
            debug_detailed(self.logger, f"ğŸ”§ [MASTER] CategorÃ­a: {intention.categoria}, Sub-tipo: {intention.sub_tipo}")
            debug_detailed(self.logger, f"ğŸ”§ [MASTER] Complejidad: {intention.complejidad}, Flujo: {intention.flujo_optimo}")

            # PASO 3: VALIDAR INTENCIÃ“N CON SISTEMA MAP
            validated_intention = self._validate_intention_with_system_map(intention)
            if validated_intention != intention:
                self.logger.info(f"ğŸ”§ [MASTER] IntenciÃ³n corregida por system_map")

            # ğŸ›‘ PAUSA ESTRATÃ‰GICA #1: ANÃLISIS SEMÃNTICO (Â¿QUÃ‰ QUIERE?)
            import os
            if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                print(f"\nğŸ›‘ [MASTER-BRAIN] PASO 1: ANÃLISIS SEMÃNTICO")
                print(f"    â”œâ”€â”€ ğŸ§  PREGUNTA: Â¿QuÃ© quiere el usuario?")
                print(f"    â”œâ”€â”€ ğŸ“ Consulta: '{context.user_message}'")
                print(f"    â”œâ”€â”€ ğŸ¯ IntenciÃ³n detectada: {intention.intention_type}/{intention.sub_intention}")
                print(f"    â”œâ”€â”€ ğŸ“Š Confianza: {intention.confidence}")
                print(f"    â”œâ”€â”€ ğŸ¯ Entidades extraÃ­das: {list(intention.detected_entities.keys())}")
                for key, value in intention.detected_entities.items():
                    if isinstance(value, list) and len(value) > 2:
                        print(f"    â”‚   â”œâ”€â”€ {key}: {value[:2]}... (+{len(value)-2} mÃ¡s)")
                    else:
                        print(f"    â”‚   â”œâ”€â”€ {key}: {value}")
                print(f"    â”œâ”€â”€ ğŸ’­ Razonamiento: {intention.reasoning[:100]}...")
                print(f"    â””â”€â”€ Presiona ENTER para PASO 2: AnÃ¡lisis de Conocimiento...")
                input()



            # ğŸ§  PASO 2: ANÃLISIS DE CONOCIMIENTO (Â¿PUEDO HACERLO?)
            feasibility = self._validate_feasibility_with_knowledge(validated_intention, context.user_message)

            # ğŸ›‘ PAUSA ESTRATÃ‰GICA #2: ANÃLISIS DE CONOCIMIENTO
            if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                print(f"\nğŸ›‘ [MASTER-BRAIN] PASO 2: ANÃLISIS DE CONOCIMIENTO")
                print(f"    â”œâ”€â”€ ğŸ§  PREGUNTA: Â¿Puedo hacer esto con mi sistema?")
                print(f"    â”œâ”€â”€ ğŸ¯ IntenciÃ³n a evaluar: {validated_intention.intention_type}/{validated_intention.sub_intention}")
                print(f"    â”œâ”€â”€ âœ… Puede manejar: {feasibility['can_handle']}")
                print(f"    â”œâ”€â”€ ğŸ“Š Confianza del conocimiento: {feasibility['confidence']}")
                print(f"    â”œâ”€â”€ ğŸ’¡ ExplicaciÃ³n: {feasibility['explanation']}")
                if feasibility['limitations']:
                    print(f"    â”œâ”€â”€ âš ï¸ Limitaciones conocidas:")
                    for limitation in feasibility['limitations']:
                        print(f"    â”‚   â€¢ {limitation}")
                if feasibility['alternatives']:
                    print(f"    â”œâ”€â”€ ğŸ”„ Alternativas disponibles:")
                    for alternative in feasibility['alternatives']:
                        print(f"    â”‚   â€¢ {alternative}")
                print(f"    â”œâ”€â”€ ğŸ¯ Mejor interpreter: {feasibility.get('best_interpreter', 'N/A')}")
                if not feasibility["can_handle"]:
                    print(f"    â”œâ”€â”€ âŒ DECISIÃ“N: No factible - ExplicarÃ© limitaciÃ³n al usuario")
                    print(f"    â””â”€â”€ Presiona ENTER para generar respuesta de limitaciÃ³n...")
                else:
                    print(f"    â”œâ”€â”€ âœ… DECISIÃ“N: Factible - ContinuarÃ© con anÃ¡lisis de contexto")
                    print(f"    â””â”€â”€ Presiona ENTER para PASO 3: AnÃ¡lisis de Contexto...")
                input()

            # Si no es factible, crear respuesta de limitaciÃ³n inmediatamente
            if not feasibility["can_handle"]:
                return self._create_limitation_response(feasibility, context.user_message)

            # ğŸ§  PASO 3: ANÃLISIS DE CONTEXTO (Â¿HAY INFORMACIÃ“N PREVIA RELEVANTE?)
            context_analysis = self._analyze_context_relevance(validated_intention, context.conversation_stack, context.user_message)

            # ğŸ›‘ PAUSA ESTRATÃ‰GICA #3: ANÃLISIS DE CONTEXTO
            if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                print(f"\nğŸ›‘ [MASTER-BRAIN] PASO 3: ANÃLISIS DE CONTEXTO")
                print(f"    â”œâ”€â”€ ğŸ§  PREGUNTA: Â¿Hay informaciÃ³n previa relevante?")
                print(f"    â”œâ”€â”€ ğŸ“Š Niveles de contexto disponibles: {len(context.conversation_stack)}")
                if context.conversation_stack:
                    print(f"    â”œâ”€â”€ ğŸ“‹ CONTEXTO DISPONIBLE:")
                    for i, nivel in enumerate(context.conversation_stack, 1):
                        query = nivel.get('query', 'N/A')
                        data_count = nivel.get('row_count', 0)
                        awaiting = nivel.get('awaiting', 'N/A')
                        print(f"    â”‚   NIVEL {i}: '{query}' ({data_count} elementos, esperando: {awaiting})")
                else:
                    print(f"    â”œâ”€â”€ ğŸ“‹ Sin contexto conversacional previo")
                print(f"    â”œâ”€â”€ ğŸ” Necesita contexto: {context_analysis.get('needs_context', False)}")
                print(f"    â”œâ”€â”€ ğŸ’¡ AnÃ¡lisis: {context_analysis.get('analysis', 'N/A')}")
                if context_analysis.get('resolved_reference'):
                    print(f"    â”œâ”€â”€ âœ… Referencia resuelta: {context_analysis['resolved_reference']}")
                print(f"    â””â”€â”€ Presiona ENTER para PASO 4: DecisiÃ³n y DelegaciÃ³n...")
                input()

            # PASO 4: VERIFICAR SI NECESITA ACLARACIÃ“N
            # ğŸ”§ ARREGLO: Verificar si es InterpretationResult con action aclaracion_requerida
            if hasattr(validated_intention, 'action') and validated_intention.action == "aclaracion_requerida":
                return validated_intention
            elif hasattr(validated_intention, 'intention_type') and validated_intention.intention_type == "aclaracion_requerida":
                return self._handle_ambiguous_query(context, validated_intention)

            # ğŸ§  PASO 4: DECISIÃ“N Y DELEGACIÃ“N (Â¿CÃ“MO PROCEDO?)
            # ğŸ›‘ PAUSA ESTRATÃ‰GICA #4: DECISIÃ“N FINAL
            if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                print(f"\nğŸ›‘ [MASTER-BRAIN] PASO 4: DECISIÃ“N Y DELEGACIÃ“N")
                print(f"    â”œâ”€â”€ ğŸ§  PREGUNTA: Â¿CÃ³mo procedo?")
                print(f"    â”œâ”€â”€ âœ… Factibilidad: Confirmada")
                print(f"    â”œâ”€â”€ ğŸ“Š Contexto: Analizado")
                # ğŸ”§ ARREGLO: Verificar si validated_intention tiene intention_type
                if hasattr(validated_intention, 'intention_type'):
                    print(f"    â”œâ”€â”€ ğŸ¯ IntenciÃ³n final: {validated_intention.intention_type}/{validated_intention.sub_intention}")
                else:
                    print(f"    â”œâ”€â”€ ğŸ¯ IntenciÃ³n final: {type(validated_intention).__name__}")
                print(f"    â”œâ”€â”€ ğŸ¯ Especialista seleccionado: StudentQueryInterpreter")
                print(f"    â”œâ”€â”€ âš¡ DECISIÃ“N: Delegar al Student para ejecuciÃ³n")
                print(f"    â””â”€â”€ Presiona ENTER para ejecutar delegaciÃ³n...")
                input()

            # PASO 5: DIRIGIR AL ESPECIALISTA DIRECTAMENTE
            result = self._delegate_to_specialist_direct(context, validated_intention, current_pdf)

            # PASO 5: ANALIZAR RESULTADOS Y DECIDIR SI NECESITA COMUNICACIÃ“N BIDIRECCIONAL
            # ğŸ”§ ARREGLO: Solo si validated_intention no es InterpretationResult
            if (result and hasattr(validated_intention, 'intention_type') and
                self._should_ask_user_about_results(result, context.user_message)):
                return self._handle_results_analysis(context, validated_intention, result)

            # PASO 6: PROCESAR RETROALIMENTACIÃ“N DEL ESPECIALISTA
            # ğŸ”§ ARREGLO: Solo si validated_intention no es InterpretationResult
            if hasattr(validated_intention, 'intention_type'):
                self._process_specialist_feedback(validated_intention, result)

            return result

        except Exception as e:
            self.logger.error(f"âŒ [MASTER] Error en interpretaciÃ³n: {e}")
            return None

    def _detect_intention_with_context(self, user_message: str, conversation_stack: list = None):
        """ğŸ¯ DETECTAR INTENCIÃ“N CON CONTEXTO CONVERSACIONAL"""
        try:
            return self.intention_detector.detect_intention(user_message, conversation_stack)
        except Exception as e:
            self.logger.error(f"âŒ Error detectando intenciÃ³n: {e}")
            # Fallback bÃ¡sico
            from app.core.ai.interpretation.intention_detector import IntentionResult
            return IntentionResult(
                intention_type="consulta_alumnos",
                sub_intention="busqueda_simple",
                confidence=0.5,
                reasoning="Fallback por error en detecciÃ³n",
                detected_entities={}
            )

    def _resolve_contextual_references(self, intention, conversation_stack: list, user_query: str):
        """
        ğŸ§  ANÃLISIS INTELIGENTE UNIFICADO
        Reemplaza anÃ¡lisis semÃ¡ntico + resoluciÃ³n de referencias con razonamiento humano
        """
        try:
            self.logger.info("ğŸ¯ [MASTER] INICIANDO ANÃLISIS INTELIGENTE UNIFICADO...")

            # ğŸ›‘ PAUSA ESTRATÃ‰GICA #8: MASTER ANÃLISIS UNIFICADO
            import os
            if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                print(f"\nğŸ›‘ [MASTER] ANÃLISIS INTELIGENTE UNIFICADO:")
                print(f"    â”œâ”€â”€ ğŸ“ Consulta: '{user_query}'")
                print(f"    â”œâ”€â”€ ğŸ“Š Contexto disponible: {len(conversation_stack)} niveles")
                print(f"    â”œâ”€â”€ ğŸ§  Master analizarÃ¡ ESTRATÃ‰GICAMENTE como humano")

                if conversation_stack:
                    print(f"    â”œâ”€â”€ ğŸ“‹ CONTEXTO COMPLETO DISPONIBLE:")
                    for i, nivel in enumerate(conversation_stack, 1):
                        query = nivel.get('query', 'N/A')
                        data_count = nivel.get('row_count', 0)
                        awaiting = nivel.get('awaiting', 'N/A')
                        action_type = nivel.get('action_type', 'N/A')

                        print(f"    â”‚   NIVEL {i}: '{query}'")
                        print(f"    â”‚   â”œâ”€â”€ {data_count} elementos (esperando: {awaiting})")
                        print(f"    â”‚   â”œâ”€â”€ Tipo: {action_type}")

                        # Mostrar datos especÃ­ficos para anÃ¡lisis
                        if nivel.get('data'):
                            if data_count == 1:
                                alumno = nivel['data'][0]
                                nombre = alumno.get('nombre', 'N/A')
                                id_alumno = alumno.get('id', 'N/A')
                                print(f"    â”‚   â””â”€â”€ ğŸ‘¤ {nombre} (ID: {id_alumno}) - ESPECÃFICO")
                            elif data_count <= 5:
                                print(f"    â”‚   â””â”€â”€ ğŸ‘¥ Lista pequeÃ±a - seleccionable")
                                for j, item in enumerate(nivel['data'][:3], 1):
                                    nombre = item.get('nombre', 'N/A')
                                    print(f"    â”‚       {j}. {nombre}")
                            else:
                                print(f"    â”‚   â””â”€â”€ ğŸ“‹ Lista grande - filtrable")
                        print(f"    â”‚")
                else:
                    print(f"    â”‚   â””â”€â”€ Sin contexto previo")

                print(f"    â”œâ”€â”€ ğŸ¯ ANÃLISIS ESTRATÃ‰GICO:")
                print(f"    â”‚   Â¿QuÃ© quiere hacer el usuario?")
                print(f"    â”‚   Â¿CÃ³mo se conecta con el contexto?")
                print(f"    â”‚   Â¿QuÃ© especialista necesita?")
                print(f"    â””â”€â”€ Presiona ENTER para anÃ¡lisis unificado...")
                input()

            # ğŸ§  ANÃLISIS UNIFICADO CON LLM
            analysis_result = self._analyze_and_delegate_intelligently(user_query, conversation_stack)

            if analysis_result:
                # Procesar resultado del anÃ¡lisis unificado
                processed_intention = self._process_unified_analysis(analysis_result, intention)

                # ğŸ§  VALIDAR FACTIBILIDAD CON CONOCIMIENTO PROFUNDO (INTEGRADO)
                feasibility = self._validate_feasibility_with_knowledge(processed_intention, user_query)

                # ğŸ›‘ PAUSA ESTRATÃ‰GICA: VALIDACIÃ“N INTEGRADA DE FACTIBILIDAD
                import os
                if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                    print(f"\nğŸ›‘ [MASTER-KNOWLEDGE] VALIDACIÃ“N INTEGRADA DE FACTIBILIDAD:")
                    # ğŸ”§ ARREGLO: Verificar si processed_intention tiene intention_type
                    if hasattr(processed_intention, 'intention_type'):
                        print(f"    â”œâ”€â”€ ğŸ§  IntenciÃ³n procesada: {processed_intention.intention_type}/{processed_intention.sub_intention}")
                    else:
                        print(f"    â”œâ”€â”€ ğŸ§  IntenciÃ³n procesada: {type(processed_intention).__name__}")
                    print(f"    â”œâ”€â”€ ğŸ“Š Contexto analizado: {analysis_result.get('contexto_analizado', 'N/A')}")
                    print(f"    â”œâ”€â”€ ğŸ” Referencia encontrada: {analysis_result.get('referencia_encontrada', 'N/A')}")
                    print(f"    â”œâ”€â”€ âœ… Puede manejar: {feasibility['can_handle']}")
                    print(f"    â”œâ”€â”€ ğŸ“Š Confianza: {feasibility['confidence']}")
                    print(f"    â”œâ”€â”€ ğŸ’¡ ExplicaciÃ³n: {feasibility['explanation']}")
                    if feasibility['limitations']:
                        print(f"    â”œâ”€â”€ âš ï¸ Limitaciones:")
                        for limitation in feasibility['limitations']:
                            print(f"    â”‚   â€¢ {limitation}")
                    if feasibility['alternatives']:
                        print(f"    â”œâ”€â”€ ğŸ”„ Alternativas:")
                        for alternative in feasibility['alternatives']:
                            print(f"    â”‚   â€¢ {alternative}")
                    print(f"    â”œâ”€â”€ ğŸ¯ Mejor interpreter: {feasibility.get('best_interpreter', 'N/A')}")
                    if not feasibility["can_handle"]:
                        print(f"    â”œâ”€â”€ âŒ CONSULTA NO FACTIBLE - Se generarÃ¡ respuesta de limitaciÃ³n")
                        print(f"    â””â”€â”€ Presiona ENTER para ver respuesta de limitaciÃ³n...")
                    else:
                        print(f"    â”œâ”€â”€ âœ… CONSULTA FACTIBLE - Continuando con delegaciÃ³n")
                        print(f"    â””â”€â”€ Presiona ENTER para continuar...")
                    input()

                # Si no es factible, crear respuesta de limitaciÃ³n
                if not feasibility["can_handle"]:
                    return self._create_limitation_response(feasibility, user_query)

                return processed_intention
            else:
                # Fallback: mantener intenciÃ³n original
                self.logger.warning("âš ï¸ [MASTER] AnÃ¡lisis unificado fallÃ³, usando intenciÃ³n original")
                return intention

        except Exception as e:
            self.logger.error(f"âŒ Error en anÃ¡lisis unificado: {e}")
            return intention

    def _analyze_and_delegate_intelligently(self, user_query: str, conversation_stack: list):
        """
        ğŸ§  ANÃLISIS INTELIGENTE UNIFICADO CON RAZONAMIENTO HUMANO
        Reemplaza anÃ¡lisis semÃ¡ntico + resoluciÃ³n de referencias
        """
        try:
            # Crear contexto para el LLM
            context_summary = self._create_context_summary(conversation_stack)

            prompt = f"""
ğŸ§  MASTER INTELIGENTE - CONOCIMIENTO COMPLETO DEL SISTEMA

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ« IDENTIDAD Y CONTEXTO COMPLETO:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- Sistema escolar: "PROF. MAXIMO GAMIZ FERNANDEZ"
- Base de datos: 211 alumnos activos (1Â° a 6Â° grado, turnos matutino/vespertino)
- Arquitectura: Master-Student (YO analizo y delego, Student ejecuta)
- Especialistas disponibles: StudentQueryInterpreter, HelpInterpreter

CONSULTA ACTUAL: "{user_query}"

CONTEXTO CONVERSACIONAL DISPONIBLE:
{context_summary}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ CONOCIMIENTO PROFUNDO DE STUDENT (CRÃTICO):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” QUÃ‰ NECESITA STUDENT PARA FUNCIONAR (INFORMACIÃ“N MÃNIMA):
- NOMBRE COMPLETO â†’ âœ… SUFICIENTE (ej: "MANUEL GARCIA FLORES")
- ID NUMÃ‰RICO â†’ âœ… SUFICIENTE (ej: 123)
- MATRÃCULA â†’ âœ… SUFICIENTE (ej: "MAT2024001")
- CURP â†’ âœ… SUFICIENTE (ej: "GAFM123456...")
- POSICIÃ“N + CONTEXTO â†’ âœ… SUFICIENTE (ej: "el segundo" + lista disponible)
- CRITERIOS ESPECÃFICOS â†’ âœ… SUFICIENTE (ej: "grado 2, grupo B")

âš ï¸ IMPORTANTE: Student NO necesita ID obligatorio si tiene nombre completo vÃ¡lido.

ğŸ¯ CAPACIDADES COMPLETAS DE STUDENT:
**BÃšSQUEDAS:**
- Buscar por cualquier campo: nombre, matrÃ­cula, CURP, grado, grupo, turno
- Filtrar resultados existentes con nuevos criterios
- InformaciÃ³n completa de alumno especÃ­fico
- BÃºsquedas combinadas con mÃºltiples criterios

**DOCUMENTOS:**
- Generar constancias: estudio, calificaciones, traslado
- Transformar PDFs entre formatos
- Extraer datos de PDFs existentes

**ANÃLISIS:**
- Conteos y estadÃ­sticas por cualquier criterio
- Distribuciones por grado, grupo, turno
- AnÃ¡lisis de calificaciones y promedios

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§  PRIORIZACIÃ“N DE CONTEXTO (COMPORTAMIENTO HUMANO):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- NIVEL 1 = MÃS RECIENTE (mÃ¡xima prioridad para referencias)
- Para referencias ambiguas â†’ usar NIVEL 1 primero
- Niveles anteriores como contexto adicional
- Comportamiento humano: usar la informaciÃ³n mÃ¡s reciente y relevante

ğŸ¯ RESOLUCIÃ“N DINÃMICA DE REFERENCIAS (CASOS REALES):
- "el segundo alumno" â†’ Buscar posiciÃ³n 2 en lista mÃ¡s reciente
- "informaciÃ³n de juan" + mÃºltiples Juanes â†’ AMBIGUO â†’ Pedir aclaraciÃ³n
- "marÃ­a del grupo A" â†’ Buscar MarÃ­a que estÃ© en grupo A
- "el de la matrÃ­cula 12345" â†’ Buscar por matrÃ­cula especÃ­fica
- "antonio" + solo 1 Antonio â†’ CLARO â†’ Seleccionar ese Antonio
- "dame info de luis" + 3 Luis â†’ AMBIGUO â†’ Listar opciones
- "manuel" + contexto con "MANUEL GARCIA FLORES" â†’ CLARO â†’ Usar ese Manuel

ğŸ” EVALUACIÃ“N DE ESPECIFICIDAD:
- Â¿La referencia es ÃšNICA? â†’ Resolver directamente
- Â¿La referencia es AMBIGUA? â†’ Pedir aclaraciÃ³n al usuario
- Â¿Hay criterios adicionales? â†’ Aplicar filtros para desambiguar
- Â¿Tengo informaciÃ³n suficiente para Student? â†’ Delegar inmediatamente

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¤” RAZONAMIENTO ESTRATÃ‰GICO COMO HUMANO EXPERTO:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. ANÃLISIS CRÃTICO DEL CONTEXTO:
   - Â¿QuÃ© datos ESPECÃFICOS tengo disponibles en el contexto?
   - Â¿El usuario se refiere a algo del contexto actual?
   - Â¿Puedo encontrar lo que busca en los datos que ya tengo?
   - Â¿La informaciÃ³n disponible es SUFICIENTE para que Student resuelva?

2. RESOLUCIÃ“N INTELIGENTE DE REFERENCIAS:
   - Si menciona un NOMBRE: Â¿EstÃ¡ ese nombre en el contexto actual?
   - Si menciona una POSICIÃ“N: Â¿Puedo identificar exactamente cuÃ¡l alumno?
   - Si es AMBIGUO: Â¿Hay mÃºltiples opciones que requieren aclaraciÃ³n?
   - Â¿Tengo informaciÃ³n mÃ­nima suficiente? (nombre completo, ID, matrÃ­cula, etc.)

3. VALIDACIÃ“N DE SUFICIENCIA PARA STUDENT:
   - Â¿Tengo NOMBRE COMPLETO vÃ¡lido? â†’ âœ… SUFICIENTE para delegar
   - Â¿Tengo ID numÃ©rico? â†’ âœ… SUFICIENTE para delegar
   - Â¿Tengo matrÃ­cula/CURP? â†’ âœ… SUFICIENTE para delegar
   - Â¿Tengo posiciÃ³n + contexto? â†’ âœ… SUFICIENTE para delegar
   - Â¿Solo tengo nombre parcial? â†’ âŒ INSUFICIENTE, buscar mÃ¡s

4. DECISIÃ“N ESTRATÃ‰GICA INTELIGENTE:
   - Â¿PUEDO RESOLVER COMPLETAMENTE con el contexto? â†’ Resolver directamente
   - Â¿Es AMBIGUO pero tengo opciones? â†’ Pedir aclaraciÃ³n al usuario
   - Â¿NECESITO NUEVA BÃšSQUEDA? â†’ Delegar con criterios especÃ­ficos
   - Â¿Tengo informaciÃ³n suficiente para Student? â†’ Delegar inmediatamente

5. RESOLUCIÃ“N OBLIGATORIA:
   - Si encuentro UNA coincidencia exacta â†’ RESOLVER con informaciÃ³n disponible
   - Si encuentro MÃšLTIPLES â†’ LISTAR opciones para aclaraciÃ³n
   - Si NO encuentro NADA â†’ Buscar en toda la base de datos

PRINCIPIO CLAVE: Si tengo informaciÃ³n mÃ­nima suficiente para Student, DEBO resolverlo.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ INTENCIONES Y CAPACIDADES COMPLETAS DEL SISTEMA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**INTENCIONES PRINCIPALES:**
- consulta_alumnos: TODO sobre alumnos (bÃºsquedas, estadÃ­sticas, conteos, constancias)
- transformacion: Convertir PDFs a diferentes formatos
- ayuda_sistema: Soporte tÃ©cnico y ayuda del sistema

**SUB-INTENCIONES ESPECÃFICAS (TODAS BAJO consulta_alumnos):**
- busqueda_simple: Un alumno especÃ­fico o criterio Ãºnico
- busqueda_filtrada: Filtros sobre resultados existentes
- informacion_completa: Datos detallados de alumno especÃ­fico
- generar_constancia: Generar constancia para un alumno
- estadisticas: Conteos, distribuciones, anÃ¡lisis numÃ©rico

**MAPEO INTELIGENTE DE CONSULTAS:**
- "informaciÃ³n de X" â†’ consulta_alumnos/busqueda_simple
- "filtrar por Y" â†’ consulta_alumnos/busqueda_filtrada
- "constancia de Z" â†’ consulta_alumnos/generar_constancia
- "cuÃ¡ntos alumnos" â†’ consulta_alumnos/estadisticas
- "distribuciÃ³n por grado" â†’ consulta_alumnos/estadisticas

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ CONOCIMIENTO COMPLETO DEL USUARIO Y CONTEXTO:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**PERFIL DEL USUARIO:**
- Personal escolar que maneja informaciÃ³n de 211 alumnos
- Necesita consultas rÃ¡pidas, constancias oficiales, estadÃ­sticas
- Usa lenguaje natural, referencias contextuales, nombres parciales
- Espera que el sistema "entienda" como un humano inteligente

**COMPORTAMIENTO ESPERADO:**
- Entender referencias: "el segundo", "manuel", "los del turno vespertino"
- Usar contexto conversacional inteligentemente
- No requerir informaciÃ³n tÃ©cnica (IDs, sintaxis especÃ­fica)
- Proporcionar respuestas completas y Ãºtiles

**LIMITACIONES DEL SISTEMA:**
- Solo datos de alumnos activos (no histÃ³ricos)
- Constancias limitadas a formatos predefinidos
- EstadÃ­sticas basadas en datos disponibles
- No puede modificar datos, solo consultar y generar documentos

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ RESPUESTA OBLIGATORIA CON RAZONAMIENTO COMPLETO:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{{
    "razonamiento_completo": "Paso 1: AnalicÃ© el contexto conversacional disponible. El usuario pregunta por... Paso 2: En el contexto encontrÃ©... Paso 3: ValidÃ© que tengo informaciÃ³n suficiente para Student... Paso 4: Mi decisiÃ³n es delegar/resolver porque...",
    "contexto_analizado": "descripciÃ³n especÃ­fica de quÃ© datos revisÃ© del contexto y quÃ© encontrÃ©",
    "referencia_encontrada": "descripciÃ³n de si encontrÃ© lo que busca el usuario en el contexto y quÃ© informaciÃ³n especÃ­fica tengo",
    "usar_contexto": true/false,
    "intencion": "consulta_alumnos|transformacion|ayuda_sistema",
    "sub_intencion": "busqueda_simple|busqueda_filtrada|informacion_completa|generar_constancia|estadisticas",
    "especialista": "StudentQueryInterpreter|HelpInterpreter",
    "alumno_resuelto": {{"id": X, "nombre": "NOMBRE_EXACTO"}} // OBLIGATORIO si encontraste coincidencia Ãºnica con informaciÃ³n suficiente
}}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš¨ REGLAS OBLIGATORIAS DE RESOLUCIÃ“N:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. **RESOLUCIÃ“N DIRECTA:** Si el contexto contiene exactamente lo que busca el usuario Y tengo informaciÃ³n suficiente para Student â†’ RESOLVER con alumno_resuelto

2. **INFORMACIÃ“N SUFICIENTE:** Cualquiera de estos es SUFICIENTE para resolver:
   - Nombre completo vÃ¡lido (ej: "MANUEL GARCIA FLORES")
   - ID numÃ©rico (ej: 123)
   - MatrÃ­cula (ej: "MAT2024001")
   - PosiciÃ³n especÃ­fica + contexto (ej: "el segundo" + lista disponible)

3. **CASOS AMBIGUOS:** Si hay mÃºltiples opciones â†’ LISTAR en razonamiento_completo para aclaraciÃ³n

4. **BÃšSQUEDA NUEVA:** Si no estÃ¡ en contexto â†’ usar_contexto=false y buscar en BD completa

5. **MAPEO INTELIGENTE:** Mapear consultas del usuario a intenciones correctas:
   - "informaciÃ³n de X" â†’ consulta_alumnos/busqueda_simple
   - "filtrar por Y" â†’ consulta_alumnos/busqueda_filtrada
   - "constancia de Z" â†’ consulta_alumnos/generar_constancia
   - "cuÃ¡ntos alumnos" â†’ consulta_alumnos/estadisticas

PRINCIPIO FUNDAMENTAL: ActÃºa como un humano inteligente que entiende el contexto y sabe exactamente quÃ© necesita Student para funcionar. Si tienes informaciÃ³n suficiente, RESUÃ‰LVELO.
"""

            if self.gemini_client:
                response = self.gemini_client.send_prompt_sync(prompt)
                if response:
                    import json
                    try:
                        # Limpiar respuesta JSON
                        clean_response = response.strip()
                        if clean_response.startswith('```json'):
                            clean_response = clean_response[7:]
                        if clean_response.startswith('```'):
                            clean_response = clean_response[3:]
                        if clean_response.endswith('```'):
                            clean_response = clean_response[:-3]
                        clean_response = clean_response.strip()

                        result = json.loads(clean_response)
                        self.logger.info(f"ğŸ§  [MASTER] AnÃ¡lisis unificado exitoso: {result.get('razonamiento_completo', '')[:100]}...")
                        return result

                    except json.JSONDecodeError as e:
                        self.logger.warning(f"ğŸ§  [MASTER] Error parsing JSON: {e}")
                        self.logger.warning(f"ğŸ§  [MASTER] Respuesta: {response}")

            return None

        except Exception as e:
            self.logger.error(f"Error en anÃ¡lisis inteligente unificado: {e}")
            return None

    def _process_unified_analysis(self, analysis_result: dict, original_intention):
        """
        ğŸ”§ PROCESAR RESULTADO DEL ANÃLISIS UNIFICADO
        Convierte el anÃ¡lisis del LLM en intenciÃ³n actualizada
        """
        try:
            # Actualizar intenciÃ³n basada en el anÃ¡lisis
            original_intention.intention_type = analysis_result.get('intencion', original_intention.intention_type)
            original_intention.sub_intention = analysis_result.get('sub_intencion', original_intention.sub_intention)
            original_intention.requiere_contexto = analysis_result.get('usar_contexto', False)

            # ğŸ¯ VALIDAR RESOLUCIÃ“N DE ALUMNO
            alumno_resuelto = analysis_result.get('alumno_resuelto')
            referencia_encontrada = analysis_result.get('referencia_encontrada', '')

            if alumno_resuelto and isinstance(alumno_resuelto, dict):
                alumno_id = alumno_resuelto.get('id')
                alumno_nombre = alumno_resuelto.get('nombre', '')

                # ğŸ§  CONFIAR EN EL ANÃLISIS INTELIGENTE DEL LLM
                # El LLM ya validÃ³ que tiene informaciÃ³n suficiente para resolver
                if alumno_nombre and not alumno_nombre.startswith('identificar'):
                    # âœ… EL LLM ENCONTRÃ“ INFORMACIÃ“N VÃLIDA - CONFIAR EN Ã‰L
                    # No importa si es nombre, matrÃ­cula, posiciÃ³n, etc.

                    # ğŸ” Intentar obtener ID como optimizaciÃ³n (opcional, no obligatorio)
                    if not alumno_id or not str(alumno_id).isdigit():
                        self.logger.info(f"ğŸ” [MASTER] Optimizando: buscando ID para '{alumno_nombre}'...")
                        alumno_id = self._get_student_id_by_name(alumno_nombre)
                        if alumno_id:
                            alumno_resuelto['id'] = alumno_id
                            self.logger.info(f"âœ… [MASTER] ID obtenido como optimizaciÃ³n: {alumno_nombre} â†’ ID: {alumno_id}")

                    # âœ… ACEPTAR LA RESOLUCIÃ“N DEL LLM (ID es bonus, no obligatorio)
                    original_intention.detected_entities['alumno_resuelto'] = alumno_resuelto
                    original_intention.requiere_contexto = False  # Ya no necesita contexto, estÃ¡ resuelto
                    self.logger.info(f"âœ… [MASTER] Confiando en anÃ¡lisis LLM: {alumno_nombre} (ID: {alumno_id or 'se resolverÃ¡ por nombre'})")

                    # âœ… MAPEAR SUB-INTENCIONES CORRECTAS PARA STUDENT
                    if analysis_result.get('sub_intencion') in ['informacion_completa', 'busqueda_filtrada']:
                        original_intention.sub_intention = 'busqueda_simple'
                        self.logger.info(f"ğŸ”§ [MASTER] Mapeando '{analysis_result.get('sub_intencion')}' â†’ 'busqueda_simple' para alumno resuelto")
                    elif analysis_result.get('sub_intencion') == 'generar_constancia':
                        # ğŸ¯ CONSTANCIAS: Mantener como consulta_alumnos + generar_constancia
                        original_intention.intention_type = 'consulta_alumnos'
                        original_intention.sub_intention = 'generar_constancia'
                        self.logger.info(f"ğŸ”§ [MASTER] Mapeando 'generar_constancia' â†’ 'consulta_alumnos/generar_constancia' para alumno resuelto")

                else:
                    # âŒ Solo rechazar si el nombre es claramente invÃ¡lido o descriptivo
                    self.logger.warning(f"âš ï¸ [MASTER] ResoluciÃ³n invÃ¡lida: Nombre invÃ¡lido o descriptivo: '{alumno_nombre}'")
                    original_intention.requiere_contexto = True

            # ğŸ“Š LOG DEL ANÃLISIS COMPLETO
            razonamiento = analysis_result.get('razonamiento_completo', 'No especificado')
            contexto_analizado = analysis_result.get('contexto_analizado', 'No especificado')

            self.logger.info(f"ğŸ§  [MASTER] Razonamiento: {razonamiento[:100]}...")
            self.logger.info(f"ğŸ“Š [MASTER] Contexto analizado: {contexto_analizado}")
            self.logger.info(f"ğŸ” [MASTER] Referencia encontrada: {referencia_encontrada}")
            self.logger.info(f"ğŸ¯ [MASTER] AnÃ¡lisis unificado completado: {analysis_result.get('intencion')}/{original_intention.sub_intention}")

            return original_intention

        except Exception as e:
            self.logger.error(f"Error procesando anÃ¡lisis unificado: {e}")
            return original_intention

    def _create_context_summary(self, conversation_stack: list) -> str:
        """
        ğŸ“‹ CREAR RESUMEN DEL CONTEXTO CONVERSACIONAL
        Genera un resumen inteligente del conversation_stack para el LLM
        """
        try:
            if not conversation_stack:
                return "Sin contexto conversacional previo."

            context_lines = []
            context_lines.append("CONTEXTO CONVERSACIONAL (niveles por prioridad, mÃ¡s reciente primero):")
            context_lines.append("")

            for i, nivel in enumerate(reversed(conversation_stack), 1):
                query = nivel.get('query', 'N/A')
                row_count = nivel.get('row_count', 0)
                awaiting = nivel.get('awaiting', 'N/A')
                data = nivel.get('data', [])

                context_lines.append(f"NIVEL {i}: '{query}'")

                if row_count == 1 and data:
                    # InformaciÃ³n especÃ­fica del alumno
                    alumno = data[0]
                    nombre = alumno.get('nombre', 'N/A')
                    id_alumno = alumno.get('id', 'N/A')
                    grado = alumno.get('grado', 'N/A')
                    grupo = alumno.get('grupo', 'N/A')
                    context_lines.append(f"â†’ Alumno especÃ­fico: {nombre} (ID: {id_alumno}) - {grado}Â° {grupo}")

                elif row_count <= 10 and data:
                    # Lista pequeÃ±a con nombres
                    if isinstance(data, list):
                        nombres = [d.get('nombre', 'N/A') for d in data[:5]]
                        context_lines.append(f"â†’ Lista pequeÃ±a ({row_count} alumnos): {', '.join(nombres)}")
                        if row_count > 5:
                            context_lines.append(f"â†’ (y {row_count - 5} mÃ¡s...)")
                    else:
                        context_lines.append(f"â†’ Datos estructurados ({row_count} elementos)")

                elif row_count > 10:
                    # Lista grande - CONTEXTO COMPLETO PARA RESOLUCIÃ“N INTELIGENTE
                    context_lines.append(f"â†’ Lista grande: {row_count} alumnos disponibles")
                    if data:
                        # âœ… MOSTRAR SUFICIENTES ELEMENTOS PARA RESOLUCIÃ“N DINÃMICA
                        elementos_mostrar = min(10, len(data))  # Mostrar hasta 10 para referencias
                        context_lines.append(f"â†’ Primeros {elementos_mostrar} alumnos (para referencias posicionales, nombres, etc.):")
                        for j, alumno in enumerate(data[:elementos_mostrar], 1):
                            nombre = alumno.get('nombre', 'N/A')
                            id_alumno = alumno.get('id', 'N/A')
                            matricula = alumno.get('matricula', 'N/A')
                            grado = alumno.get('grado', 'N/A')
                            grupo = alumno.get('grupo', 'N/A')
                            context_lines.append(f"   {j}. {nombre} (ID: {id_alumno}, Mat: {matricula}, {grado}Â°{grupo})")

                        if row_count > elementos_mostrar:
                            context_lines.append(f"   ... y {row_count - elementos_mostrar} mÃ¡s disponibles")

                        # Detectar criterios comunes
                        primer_alumno = data[0]
                        if 'grado' in primer_alumno:
                            grado = primer_alumno.get('grado')
                            context_lines.append(f"â†’ Criterio detectado: {grado}Â° grado")
                        if 'turno' in primer_alumno:
                            turno = primer_alumno.get('turno')
                            context_lines.append(f"â†’ Turno: {turno}")

                        # âœ… INFORMACIÃ“N PARA RESOLUCIÃ“N DINÃMICA
                        context_lines.append(f"â†’ LISTA COMPLETA DISPONIBLE: Puedes referenciar por posiciÃ³n, nombre, matrÃ­cula, etc.")
                else:
                    context_lines.append(f"â†’ {row_count} resultados")

                context_lines.append(f"â†’ Esperando: {awaiting}")
                context_lines.append("")

            return "\n".join(context_lines)

        except Exception as e:
            self.logger.error(f"Error creando resumen de contexto: {e}")
            return "Error procesando contexto conversacional."

    def _handle_ambiguous_reference(self, user_query: str, intention, conversation_stack: list):
        """
        ğŸš¨ MANEJA REFERENCIAS AMBIGUAS DETECTADAS POR EL LLM
        Genera una respuesta de aclaraciÃ³n inteligente basada en el contexto
        """
        try:
            from app.core.ai.interpretation.base_interpreter import InterpretationResult

            # Obtener informaciÃ³n del contexto para generar aclaraciÃ³n especÃ­fica
            ultimo_nivel = conversation_stack[-1] if conversation_stack else None
            if not ultimo_nivel:
                return intention

            row_count = ultimo_nivel.get('row_count', 0)
            query_anterior = ultimo_nivel.get('query', 'consulta anterior')

            # Generar mensaje de aclaraciÃ³n especÃ­fico
            if row_count > 1:
                human_response = f"ğŸ¤” Tu consulta '{user_query}' es ambigua. EncontrÃ© {row_count} alumnos en '{query_anterior}'. Â¿PodrÃ­as especificar a cuÃ¡l te refieres? Por ejemplo: 'el segundo', 'el tercero', o menciona el nombre especÃ­fico."
            else:
                human_response = f"ğŸ¤” Tu consulta '{user_query}' no es lo suficientemente clara. Â¿PodrÃ­as ser mÃ¡s especÃ­fico sobre quÃ© informaciÃ³n necesitas?"

            # Crear resultado de aclaraciÃ³n
            result = InterpretationResult(
                action="aclaracion_requerida",
                parameters={
                    "message": human_response,
                    "original_query": user_query,
                    "human_response": human_response,
                    "context_info": {
                        "row_count": row_count,
                        "query_anterior": query_anterior
                    }
                },
                confidence=0.9
            )

            self.logger.info(f"ğŸš¨ [MASTER] Generada aclaraciÃ³n para referencia ambigua: {row_count} candidatos")
            return result

        except Exception as e:
            self.logger.error(f"âŒ Error manejando referencia ambigua: {e}")
            return intention

    def _analyze_context_relevance(self, intention, conversation_stack: list, user_query: str) -> dict:
        """
        ğŸ§  PASO 3: ANÃLISIS DE CONTEXTO COMO HUMANO EXPERTO
        Determina si hay informaciÃ³n previa relevante para la consulta actual
        """
        try:
            # Si no hay contexto, es independiente
            if not conversation_stack:
                return {
                    "needs_context": False,
                    "analysis": "Sin contexto conversacional previo - consulta independiente",
                    "resolved_reference": None
                }

            # AnÃ¡lisis bÃ¡sico de referencias contextuales
            contextual_keywords = [
                "Ã©l", "ella", "ese", "esa", "este", "esta", "aquel", "aquella",
                "el anterior", "la anterior", "el primero", "el segundo", "el Ãºltimo",
                "sus datos", "su informaciÃ³n", "de Ã©l", "para ella",
                "tambiÃ©n", "ademÃ¡s", "igualmente"
            ]

            needs_context = any(keyword in user_query.lower() for keyword in contextual_keywords)

            if needs_context:
                # Hay referencias contextuales - necesita anÃ¡lisis profundo
                return {
                    "needs_context": True,
                    "analysis": f"Detectadas referencias contextuales en: '{user_query}'",
                    "resolved_reference": "Requiere anÃ¡lisis LLM para resoluciÃ³n"
                }
            else:
                # Consulta independiente
                return {
                    "needs_context": False,
                    "analysis": "Consulta semÃ¡nticamente independiente - no requiere contexto",
                    "resolved_reference": None
                }

        except Exception as e:
            self.logger.error(f"Error analizando relevancia de contexto: {e}")
            return {
                "needs_context": False,
                "analysis": "Error en anÃ¡lisis - procesando como independiente",
                "resolved_reference": None
            }

    def _analyze_query_semantic_independence(self, user_query: str, conversation_stack: list) -> bool:
        """
        ğŸ§  ANALIZA SI LA CONSULTA ES SEMÃNTICAMENTE INDEPENDIENTE
        Usa razonamiento LLM para determinar si necesita contexto
        """
        try:
            # Si no hay contexto, obviamente es independiente
            if not conversation_stack:
                return True

            # Crear prompt para anÃ¡lisis semÃ¡ntico
            context_summary = self._create_context_summary(conversation_stack)

            prompt = f"""
ğŸ§  ANÃLISIS CONTEXTUAL INTELIGENTE - MASTER DEL SISTEMA ESCOLAR

CONTEXTO CONVERSACIONAL COMPLETO:
{context_summary}

CONSULTA A ANALIZAR: "{user_query}"

ğŸ¯ MI IDENTIDAD Y CONOCIMIENTO COMPLETO:
- Sistema escolar "PROF. MAXIMO GAMIZ FERNANDEZ"
- Base de datos: 211 alumnos en grados 1Â° a 6Â°
- Especialistas: StudentQueryInterpreter, HelpInterpreter

ğŸ¯ ESPECIALISTAS QUE DIRIJO:
**StudentQueryInterpreter**:
- BUSCAR_UNIVERSAL: BÃºsquedas flexibles
- CONTAR_UNIVERSAL: Conteos y estadÃ­sticas
- GENERAR_CONSTANCIA_COMPLETA: Documentos PDF
- BUSCAR_Y_FILTRAR: Filtros sobre resultados

ğŸ“‹ NIVELES DE CONTEXTO:
- Nivel 1 = MÃS RECIENTE (mÃ¡s relevante)
- Listas grandes = "regenerables" (SQL + metadatos)
- Puedo usar CUALQUIER nivel para resolver referencias

ğŸ§  RAZONAMIENTO INTELIGENTE:
1. Â¿QuÃ© solicita el usuario?
2. Â¿QuÃ© informaciÃ³n tengo disponible?
3. Â¿Puedo resolver con contexto?
4. Â¿Hay referencias que resolver?

ELEMENTOS QUE INDICAN NECESIDAD DE CONTEXTO:
â€¢ Pronombres referenciales: "Ã©l", "ella", "ese", "esa", "este", "esta", "aquel", "aquella"
â€¢ Frases pronominales: "ese alumno", "esa estudiante", "ese chico", "esa persona"
â€¢ Referencias posicionales: "el primero", "el segundo", "el Ãºltimo", "la primera"
â€¢ Adjetivos demostrativos sin sustantivo: "ese", "esa", "este", "esta"
â€¢ Referencias implÃ­citas: "sus datos", "su informaciÃ³n", "de Ã©l", "para ella"
â€¢ Continuaciones: "tambiÃ©n", "ademÃ¡s", "igualmente", "del mismo modo"
â€¢ Filtros sobre resultados previos: "de esos", "entre ellos", "de los anteriores"

EJEMPLOS DE ANÃLISIS INTELIGENTE:
INDEPENDIENTES:
- "buscar GarcÃ­a" â†’ Nueva bÃºsqueda completa
- "buscar JUAN PÃ‰REZ LÃ“PEZ" â†’ Nombre completo especÃ­fico
- "constancia para MARÃA GONZÃLEZ" â†’ Nombre especÃ­fico
- "estadÃ­sticas de grupos" â†’ Consulta general
- "cuÃ¡ntos alumnos hay en total" â†’ Consulta global

NECESITAN CONTEXTO:
- "constancia para el segundo" â†’ Referencia posicional
- "de esos cuÃ¡ntos son del turno matutino" â†’ Filtro sobre anteriores
- "dame la curp de gabriela" â†’ Nombre parcial, buscar en contexto
- "constancia para ella" â†’ Pronombre, resolver referencia
- "tambiÃ©n necesito su constancia" â†’ ContinuaciÃ³n con referencia

RESPUESTA: "INDEPENDIENTE" o "NECESITA_CONTEXTO"
"""

            response = self.gemini_client.send_prompt_sync(prompt)
            result = response.strip().upper() if response else ""

            is_independent = "INDEPENDIENTE" in result

            self.logger.info(f"ğŸ§  [MASTER] AnÃ¡lisis semÃ¡ntico: '{user_query}' â†’ {'INDEPENDIENTE' if is_independent else 'NECESITA_CONTEXTO'}")

            return is_independent

        except Exception as e:
            self.logger.error(f"Error en anÃ¡lisis semÃ¡ntico: {e}")
            # ğŸ§  FALLBACK SIMPLE: Si hay error, asumir que necesita contexto si hay contexto disponible
            if conversation_stack:
                self.logger.warning(f"ğŸ§  [MASTER] Error en LLM, pero hay contexto disponible - asumiendo NECESITA_CONTEXTO")
                return False  # NECESITA CONTEXTO
            else:
                self.logger.warning(f"ğŸ§  [MASTER] Error en LLM, sin contexto disponible - asumiendo INDEPENDIENTE")
                return True  # INDEPENDIENTE

    def _resolve_reference_with_llm(self, user_query: str, conversation_stack: list) -> dict:
        """
        ğŸ§  RESOLUCIÃ“N INTELIGENTE DE REFERENCIAS CON LLM
        El LLM entiende CUALQUIER tipo de referencia sin listas hardcodeadas
        """
        try:
            if not conversation_stack:
                return None

            # Crear contexto para el LLM
            context_summary = self._create_detailed_context_for_reference(conversation_stack)

            prompt = f"""
ğŸ§  RESOLUCIÃ“N INTELIGENTE DE REFERENCIAS - SISTEMA ESCOLAR

CONSULTA DEL USUARIO: "{user_query}"

CONTEXTO CONVERSACIONAL DISPONIBLE:
{context_summary}

ğŸ¯ TU TAREA:
Analiza si la consulta del usuario hace referencia a algÃºn alumno especÃ­fico del contexto.

ğŸ§  REGLAS CRÃTICAS DE RAZONAMIENTO:
1. Si hay UNA SOLA persona en el contexto â†’ Referencia clara
2. Si hay MÃšLTIPLES personas CON OPERACIÃ“N DE FILTRO â†’ Referencia clara a la lista completa
3. Si hay MÃšLTIPLES personas SIN especificaciÃ³n ni operaciÃ³n â†’ AMBIGUO, no asumir
4. Si hay posiciÃ³n especÃ­fica ("el segundo") â†’ Referencia clara
5. Si hay pronombre vago ("su") con lista mÃºltiple SIN OPERACIÃ“N â†’ AMBIGUO

TIPOS DE REFERENCIAS VÃLIDAS:
- Pronominales CLARAS: "su informaciÃ³n" (cuando hay 1 alumno especÃ­fico)
- Posicionales ESPECÃFICAS: "el segundo", "el tercero", "el Ãºltimo"
- ImplÃ­citas CLARAS: "tambiÃ©n necesito" (cuando hay 1 alumno especÃ­fico)
- OPERACIONES DE FILTRO: "de ellos los que...", "de esos cuÃ¡ntos...", "los GarcÃ­a del turno..."

ğŸ¯ OPERACIONES DE FILTRO (SIEMPRE CLARAS):
- "de ellos dame los que esten en el turno matutino" â†’ FILTRO sobre lista completa
- "de esos cuÃ¡ntos son de primer grado" â†’ CONTEO sobre lista completa
- "los GarcÃ­a del turno vespertino" â†’ FILTRO sobre lista completa
- "cuÃ¡ntos de ellos tienen calificaciones" â†’ ANÃLISIS sobre lista completa

âŒ NO ASUMIR REFERENCIAS EN CASOS AMBIGUOS:
- "su informaciÃ³n" con lista de 20+ alumnos SIN OPERACIÃ“N â†’ AMBIGUO
- "Ã©l" con mÃºltiples candidatos SIN ESPECIFICACIÃ“N â†’ AMBIGUO
- Pronombres vagos sin contexto especÃ­fico â†’ AMBIGUO

FORMATO DE RESPUESTA JSON:
{{
    "tiene_referencia": true/false,
    "es_ambiguo": true/false,
    "alumno_referenciado": {{
        "id": nÃºmero_id,
        "nombre": "NOMBRE COMPLETO",
        "razonamiento": "explicaciÃ³n especÃ­fica"
    }},
    "motivo_ambiguedad": "explicaciÃ³n si es ambiguo"
}}

EJEMPLOS CORRECTOS:
- "su informaciÃ³n" + 1 alumno especÃ­fico â†’ tiene_referencia: true
- "su informaciÃ³n" + lista de 21 â†’ es_ambiguo: true
- "el segundo" + lista â†’ tiene_referencia: true (posiciÃ³n especÃ­fica)
- "de ellos los del turno matutino" + lista de 49 â†’ tiene_referencia: false (operaciÃ³n de filtro, NO referencia individual)
- "de esos cuÃ¡ntos son de primer grado" + lista â†’ tiene_referencia: false (operaciÃ³n de conteo, NO referencia individual)

RESPONDE SOLO CON EL JSON:
"""

            if self.gemini_client:
                response = self.gemini_client.send_prompt_sync(prompt)
                if response:
                    import json
                    try:
                        # ğŸ”§ LIMPIAR RESPUESTA: Remover bloques de cÃ³digo markdown
                        clean_response = response.strip()
                        if clean_response.startswith('```json'):
                            clean_response = clean_response[7:]  # Remover ```json
                        if clean_response.startswith('```'):
                            clean_response = clean_response[3:]   # Remover ```
                        if clean_response.endswith('```'):
                            clean_response = clean_response[:-3]  # Remover ``` final
                        clean_response = clean_response.strip()

                        result = json.loads(clean_response)

                        # Verificar si es ambiguo
                        if result.get("es_ambiguo"):
                            motivo = result.get("motivo_ambiguedad", "Referencia ambigua")
                            self.logger.info(f"ğŸ§  [LLM] Referencia AMBIGUA detectada: {motivo}")
                            return None  # No resolver, que pida aclaraciÃ³n

                        # Si tiene referencia clara
                        if result.get("tiene_referencia") and result.get("alumno_referenciado"):
                            alumno = result["alumno_referenciado"]
                            self.logger.info(f"ğŸ§  [LLM] Referencia CLARA detectada: {alumno.get('razonamiento')}")
                            return {
                                'id': alumno.get('id'),
                                'nombre': alumno.get('nombre'),
                                'posicion': 'resuelto por LLM'
                            }
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"ğŸ§  [LLM] Error parsing JSON: {e}")
                        self.logger.warning(f"ğŸ§  [LLM] Respuesta original: {response}")
                        self.logger.warning(f"ğŸ§  [LLM] Respuesta limpia: {clean_response}")

            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia con LLM: {e}")
            return None

    def _create_detailed_context_for_reference(self, conversation_stack: list) -> str:
        """
        ğŸ§  CONTEXTO DETALLADO PARA RESOLUCIÃ“N DE REFERENCIAS
        """
        try:
            if not conversation_stack:
                return "Sin contexto previo"

            context_parts = []

            for i, nivel in enumerate(reversed(conversation_stack), 1):
                query = nivel.get('query', 'N/A')
                data = nivel.get('data', [])
                row_count = nivel.get('row_count', 0)

                if data:
                    if row_count == 1:
                        alumno = data[0]
                        context_parts.append(f"""
NIVEL {i} (mÃ¡s reciente): "{query}"
â†’ Alumno especÃ­fico: {alumno.get('nombre')} (ID: {alumno.get('id')})
â†’ Grado: {alumno.get('grado')}Â° {alumno.get('grupo')}, Turno: {alumno.get('turno')}
â†’ CURP: {alumno.get('curp')}""")
                    elif row_count <= 10:
                        if isinstance(data, list):
                            nombres = [f"{j+1}. {d.get('nombre')} (ID: {d.get('id')})" for j, d in enumerate(data[:5])]
                            context_parts.append(f"""
NIVEL {i}: "{query}"
â†’ Lista de {row_count} alumnos:
{chr(10).join(nombres)}""")
                        else:
                            context_parts.append(f"""
NIVEL {i}: "{query}"
â†’ Datos estructurados: {row_count} elementos""")
                    else:
                        context_parts.append(f"""
NIVEL {i}: "{query}"
â†’ Lista grande de {row_count} alumnos (primeros 3):
1. {data[0].get('nombre')} (ID: {data[0].get('id')})
2. {data[1].get('nombre')} (ID: {data[1].get('id')})
3. {data[2].get('nombre')} (ID: {data[2].get('id')})
... y {row_count-3} mÃ¡s""")

            return "\n".join(context_parts) if context_parts else "Sin contexto Ãºtil"

        except Exception as e:
            self.logger.error(f"Error creando contexto detallado: {e}")
            return "Error en contexto"



    def _resolve_positional_reference(self, user_query: str, conversation_stack: list) -> dict:
        """Resuelve referencias posicionales como 'segundo', 'tercero'"""
        try:
            if not conversation_stack:
                return None

            # Obtener datos del Ãºltimo nivel
            ultimo_nivel = conversation_stack[-1]
            data = ultimo_nivel.get('data', [])

            if not data:
                return None

            # Determinar posiciÃ³n
            position_index = None
            query_lower = user_query.lower()

            if 'primer' in query_lower or 'primero' in query_lower:
                position_index = 0
            elif 'segundo' in query_lower or 'segunda' in query_lower:
                position_index = 1
            elif 'tercer' in query_lower or 'tercero' in query_lower:
                position_index = 2
            elif 'cuarto' in query_lower or 'cuarta' in query_lower:
                position_index = 3
            elif 'quinto' in query_lower or 'quinta' in query_lower:
                position_index = 4
            elif 'Ãºltimo' in query_lower or 'Ãºltima' in query_lower:
                position_index = len(data) - 1

            if position_index is not None and position_index < len(data):
                alumno = data[position_index]
                return {
                    'id': alumno.get('id') or alumno.get('alumno_id'),
                    'nombre': alumno.get('nombre'),
                    'posicion': f"posiciÃ³n {position_index + 1}"
                }

            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia posicional: {e}")
            return None

    def _resolve_pronominal_reference(self, conversation_stack: list) -> dict:
        """
        ğŸ§  RESOLUCIÃ“N INTELIGENTE DE REFERENCIAS PRONOMINALES
        MEJORA: Busca en TODOS los niveles con lÃ³gica inteligente
        """
        try:
            if not conversation_stack:
                return None

            # ğŸ§  BUSCAR EN TODOS LOS NIVELES CON LÃ“GICA INTELIGENTE
            for nivel in reversed(conversation_stack):
                data = nivel.get('data', [])
                query = nivel.get('query', '').lower()

                if not data:
                    continue

                # CASO 1: Un solo alumno (ideal)
                if len(data) == 1:
                    alumno = data[0]
                    # Verificar que no sea consulta de campo especÃ­fico
                    if not any(word in query for word in ['curp', 'matrÃ­cula', 'informaciÃ³n']):
                        return {
                            'id': alumno.get('id') or alumno.get('alumno_id'),
                            'nombre': alumno.get('nombre'),
                            'posicion': 'Ãºltimo mencionado especÃ­ficamente'
                        }

                # CASO 2: MÃºltiples alumnos - buscar referencia especÃ­fica
                elif len(data) > 1:
                    # Si hay nombre especÃ­fico en la query
                    for alumno in data:
                        nombre_completo = alumno.get('nombre', '').lower()
                        if any(part in query for part in nombre_completo.split()):
                            return {
                                'id': alumno.get('id') or alumno.get('alumno_id'),
                                'nombre': alumno.get('nombre'),
                                'posicion': 'referencia especÃ­fica por nombre'
                            }

                    # Si no hay referencia especÃ­fica, tomar el primero
                    alumno = data[0]
                    return {
                        'id': alumno.get('id') or alumno.get('alumno_id'),
                        'nombre': alumno.get('nombre'),
                        'posicion': 'primero de la lista anterior'
                    }

            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia pronominal: {e}")
            return None

    def _resolve_name_reference(self, nombre_detectado: str, conversation_stack: list) -> dict:
        """Resuelve referencias por nombre parcial como 'mario' â†’ 'MARIO LOPEZ GONZALEZ'"""
        try:
            if not conversation_stack or not nombre_detectado:
                return None

            nombre_lower = nombre_detectado.lower()
            self.logger.info(f"ğŸ” [MASTER] Buscando referencia por nombre: '{nombre_detectado}'")

            # Buscar en todos los niveles del contexto
            for nivel in reversed(conversation_stack):
                data = nivel.get('data', [])
                if not data:
                    continue

                # Buscar coincidencia por nombre parcial
                for alumno in data:
                    nombre_completo = alumno.get('nombre', '').lower()

                    # Verificar si el nombre detectado estÃ¡ contenido en el nombre completo
                    if nombre_lower in nombre_completo:
                        alumno_id = alumno.get('id') or alumno.get('alumno_id')
                        self.logger.info(f"âœ… [MASTER] COINCIDENCIA ENCONTRADA: '{nombre_detectado}' â†’ '{alumno.get('nombre')}' (ID: {alumno_id})")
                        return {
                            'id': alumno_id,
                            'nombre': alumno.get('nombre'),
                            'posicion': 'referencia por nombre'
                        }

            self.logger.warning(f"âŒ [MASTER] No se encontrÃ³ referencia para: '{nombre_detectado}'")
            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia por nombre: {e}")
            return None

    def _get_student_id_by_name(self, nombre_completo: str) -> Optional[int]:
        """
        ğŸ” BUSCAR ID DE ALUMNO POR NOMBRE COMPLETO EN BASE DE DATOS
        Comportamiento humano: Si tengo el nombre, puedo buscar el ID
        """
        try:
            if not nombre_completo:
                return None

            self.logger.info(f"ğŸ” [MASTER] Buscando ID para: '{nombre_completo}'")

            # Usar el servicio de alumnos para buscar
            from app.core.service_provider import ServiceProvider
            service_provider = ServiceProvider.get_instance()
            alumno_service = service_provider.alumno_service

            # Buscar por nombre exacto
            alumnos_encontrados = alumno_service.buscar_alumnos(nombre_completo)

            if not alumnos_encontrados:
                self.logger.warning(f"âŒ [MASTER] No se encontrÃ³ alumno con nombre: '{nombre_completo}'")
                return None

            # Buscar coincidencia exacta
            for alumno in alumnos_encontrados:
                alumno_dict = alumno.to_dict() if hasattr(alumno, 'to_dict') else alumno
                if alumno_dict.get('nombre', '').upper() == nombre_completo.upper():
                    alumno_id = alumno_dict.get('id')
                    self.logger.info(f"âœ… [MASTER] ID encontrado: '{nombre_completo}' â†’ ID: {alumno_id}")
                    return alumno_id

            # Si no hay coincidencia exacta, tomar el primero si es muy similar
            primer_alumno = alumnos_encontrados[0]
            alumno_dict = primer_alumno.to_dict() if hasattr(primer_alumno, 'to_dict') else primer_alumno
            alumno_id = alumno_dict.get('id')

            self.logger.info(f"âœ… [MASTER] Usando primer resultado similar: '{alumno_dict.get('nombre')}' â†’ ID: {alumno_id}")
            return alumno_id

        except Exception as e:
            self.logger.error(f"Error buscando ID por nombre: {e}")
            return None

    def _validate_intention_with_system_map(self, intention):
        """ğŸ›¡ï¸ VALIDAR INTENCIÃ“N CON SYSTEM MAP Y CORREGIR AUTOMÃTICAMENTE"""
        try:
            intention_type = intention.intention_type

            # ğŸ¯ LISTA DE INTENCIONES VÃLIDAS (SEGÃšN SYSTEM_MAP)
            valid_intentions = []
            for specialist, config in self.system_map.items():
                valid_intentions.extend(config["handles"])

            # âœ… VERIFICAR SI LA INTENCIÃ“N ES VÃLIDA
            if intention_type in valid_intentions:
                for specialist, config in self.system_map.items():
                    if intention_type in config["handles"]:
                        self.logger.info(f"âœ… [MASTER] IntenciÃ³n '{intention_type}' validada para {specialist}")
                        return intention

            # ğŸ”§ CORRECCIÃ“N AUTOMÃTICA DE INTENCIONES INCORRECTAS
            self.logger.warning(f"âš ï¸ [MASTER] IntenciÃ³n '{intention_type}' no encontrada en system_map")

            # Mapeo automÃ¡tico para intenciones comunes mal detectadas
            incorrect_mappings = {
                "estadistica": "consulta_alumnos",
                "busqueda": "consulta_alumnos",
                "constancia": "consulta_alumnos",
                "transformacion": "transformacion_pdf",  # Corregir transformacion â†’ transformacion_pdf
                "ayuda": "ayuda_sistema",
                "help": "ayuda_sistema"
            }

            if intention_type in incorrect_mappings:
                old_intention = intention_type
                intention.intention_type = incorrect_mappings[old_intention]
                self.logger.info(f"ğŸ”§ [MASTER] Auto-correcciÃ³n: '{old_intention}' â†’ '{intention.intention_type}'")
                return intention

            # âŒ ERROR SI NO SE PUEDE MAPEAR
            self.logger.error(f"âŒ [MASTER] IntenciÃ³n no reconocida: {intention_type}")
            self.logger.error(f"âŒ Intenciones vÃ¡lidas: {valid_intentions}")

            # Fallback a consulta_alumnos para mantener funcionalidad
            self.logger.info(f"ğŸ”§ [MASTER] Fallback: '{intention_type}' â†’ 'consulta_alumnos'")
            intention.intention_type = "consulta_alumnos"
            return intention

        except Exception as e:
            self.logger.error(f"âŒ Error validando intenciÃ³n: {e}")
            return intention

    def _delegate_to_specialist_direct(self, context: InterpretationContext, intention, current_pdf=None):
        """ğŸ¯ DELEGAR AL ESPECIALISTA CON CONTEXTO COMPLETO"""
        try:


            # Agregar informaciÃ³n de intenciÃ³n consolidada al contexto
            context.intention_info = {
                'intention_type': intention.intention_type,
                'sub_intention': intention.sub_intention,
                'confidence': intention.confidence,
                'reasoning': intention.reasoning,
                'detected_entities': intention.detected_entities,
                # ğŸ†• CATEGORIZACIÃ“N ESPECÃFICA CONSOLIDADA
                'categoria': intention.categoria,
                'sub_tipo': intention.sub_tipo,
                'complejidad': intention.complejidad,
                'requiere_contexto': intention.requiere_contexto,
                'flujo_optimo': intention.flujo_optimo
            }

            # ğŸ§  [MASTER] Delegando a Student con instrucciones claras

            # ğŸ¯ DELEGACIÃ“N CONSOLIDADA - Elimina duplicaciÃ³n masiva
            return self._execute_delegation_unified(intention, context, current_pdf=current_pdf)

        except Exception as e:
            self.logger.error(f"âŒ Error delegando al especialista: {e}")
            # ğŸ§¹ SIN FALLBACKS - Que falle claramente para debugging
            raise

    def _execute_delegation_unified(self, intention, context: InterpretationContext, current_pdf=None):
        """
        ğŸ¯ DELEGACIÃ“N UNIFICADA - Elimina duplicaciÃ³n masiva de cÃ³digo

        Consolida la lÃ³gica de delegaciÃ³n que estaba duplicada 4 veces.
        Mantiene 100% la funcionalidad original pero sin repeticiÃ³n.
        """
        try:
            intention_type = intention.intention_type

            # ğŸ¯ MAPEO DE INTENCIONES A ESPECIALISTAS
            specialist_map = {
                "consulta_alumnos": {
                    "interpreter": self.student_interpreter,
                    "name": "StudentQueryInterpreter",
                    "description": ""
                },
                "transformacion_pdf": {
                    "interpreter": self.student_interpreter,
                    "name": "StudentQueryInterpreter",
                    "description": " (transformaciÃ³n PDF)"
                },
                "ayuda_sistema": {
                    "interpreter": self.help_interpreter,
                    "name": "HelpInterpreter",
                    "description": ""
                }
            }

            # ğŸ¯ OBTENER ESPECIALISTA PARA LA INTENCIÃ“N
            specialist_config = specialist_map.get(intention_type)
            if not specialist_config:
                self.logger.error(f"âŒ [MASTER] IntenciÃ³n no reconocida: {intention_type}")
                raise ValueError(f"IntenciÃ³n no reconocida: {intention_type}")

            # ğŸ¯ LOGS UNIFICADOS (MISMA ESTRUCTURA QUE ANTES)
            specialist_name = specialist_config["name"]
            description = specialist_config["description"]
            self.logger.info(f"ğŸ¯ [MASTER] Dirigiendo a {specialist_name}{description}")
            self.logger.info(f"   â”œâ”€â”€ Sub-intenciÃ³n: {intention.sub_intention}")
            self.logger.info(f"   â””â”€â”€ Entidades: {len(intention.detected_entities)} detectadas")

            # ğŸ¯ EJECUTAR DELEGACIÃ“N
            specialist = specialist_config["interpreter"]
            # Verificar si el specialist acepta current_pdf
            if hasattr(specialist, 'interpret'):
                import inspect
                sig = inspect.signature(specialist.interpret)
                if 'current_pdf' in sig.parameters:
                    result = specialist.interpret(context, current_pdf=current_pdf)
                else:
                    result = specialist.interpret(context)
            else:
                result = specialist.interpret(context)
            self.logger.info(f"ğŸ“Š [MASTER] Resultado: {result.action if result else 'None'}")

            # ğŸ¯ MASTER COMO VOCERO: Generar respuesta final (IGUAL QUE ANTES)
            if result:
                final_result = self._generate_master_response(result, context.user_message)
                self.logger.info(f"ğŸ—£ï¸ [MASTER] Respuesta final generada como vocero")
                return final_result

            return result

        except Exception as e:
            self.logger.error(f"âŒ Error en delegaciÃ³n unificada: {e}")
            raise

    def _process_specialist_feedback(self, intention, result):
        """ğŸ¯ PROCESAR RETROALIMENTACIÃ“N DEL ESPECIALISTA"""
        try:
            if result:
                # Actualizar memoria de interacciones
                self.interaction_memory.update({
                    "last_specialist": self._get_specialist_for_intention(intention.intention_type),
                    "last_result_summary": f"AcciÃ³n: {result.action}",
                    "conversation_flow": f"{intention.intention_type} â†’ {result.action}",
                    "specialist_feedback": "Completado exitosamente",
                    "awaiting_continuation": getattr(result, 'awaiting_continuation', False),
                    "continuation_type": getattr(result, 'continuation_type', None)
                })

                self.logger.info(f"ğŸ”„ [MASTER] Memoria actualizada:")
                self.logger.info(f"   â”œâ”€â”€ Especialista: {self.interaction_memory['last_specialist']}")
                self.logger.info(f"   â”œâ”€â”€ Resultado: {self.interaction_memory['last_result_summary']}")
                self.logger.info(f"   â””â”€â”€ Flujo: {self.interaction_memory['conversation_flow']}")
            else:
                self.logger.warning(f"âš ï¸ [MASTER] No se recibiÃ³ resultado del especialista")

        except Exception as e:
            self.logger.error(f"âŒ Error procesando retroalimentaciÃ³n: {e}")

    def _handle_ambiguous_query(self, context: InterpretationContext, intention) -> Optional[InterpretationResult]:
        """
        ğŸ¤” MANEJA CONSULTAS AMBIGUAS - PIDE ACLARACIÃ“N AL USUARIO DE FORMA SIMPLE
        """
        try:
            self.logger.info("ğŸ¤” [MASTER] Consulta ambigua detectada - pidiendo aclaraciÃ³n simple")

            # Crear respuesta de aclaraciÃ³n simple
            from app.core.ai.interpretation.base_interpreter import InterpretationResult

            human_response = f"ğŸ¤” Tu consulta '{context.user_message}' no es lo suficientemente clara para mÃ­. Â¿PodrÃ­as ser mÃ¡s especÃ­fico sobre quÃ© informaciÃ³n necesitas?"

            return InterpretationResult(
                action="aclaracion_requerida",
                parameters={
                    "message": human_response,
                    "original_query": context.user_message,
                    "human_response": human_response
                },
                confidence=intention.confidence
            )

        except Exception as e:
            self.logger.error(f"âŒ Error manejando consulta ambigua: {e}")
            return None



    def _get_specialist_for_intention(self, intention_type: str) -> str:
        """ğŸ¯ OBTENER ESPECIALISTA PARA INTENCIÃ“N"""
        for specialist, config in self.system_map.items():
            if intention_type in config["handles"]:
                return specialist
        return "Unknown"

    def _should_ask_user_about_results(self, result: 'InterpretationResult', user_query: str) -> bool:
        """
        ğŸ§  MASTER ANALIZA RESULTADOS: Â¿DeberÃ­a preguntar al usuario?
        Decide si los resultados del Student requieren aclaraciÃ³n del usuario
        """
        try:
            if not result or not result.parameters:
                return False

            row_count = result.parameters.get('row_count', 0)
            action = result.action

            # ğŸš¨ CASOS DONDE EL MASTER DEBERÃA PREGUNTAR:

            # 1. Constancias con mÃºltiples candidatos
            if 'constancia' in user_query.lower() and row_count > 1:
                self.logger.info(f"ğŸ”„ [MASTER] Constancia con {row_count} candidatos - necesita selecciÃ³n")
                return True

            # 2. BÃºsquedas muy amplias (mÃ¡s de 50 resultados)
            if 'buscar' in user_query.lower() and row_count > 50:
                self.logger.info(f"ğŸ”„ [MASTER] BÃºsqueda muy amplia ({row_count} resultados) - ofrecer filtros")
                return True

            # 3. Sin resultados - ofrecer ayuda
            if row_count == 0:
                self.logger.info(f"ğŸ”„ [MASTER] Sin resultados - ofrecer alternativas")
                return True

            # Para bÃºsquedas normales como "buscar garcia" con 21 resultados: NO preguntar
            self.logger.info(f"ğŸ”„ [MASTER] Resultados normales ({row_count}) - mostrar directamente")
            return False

        except Exception as e:
            self.logger.error(f"Error analizando si preguntar al usuario: {e}")
            return False

    def _handle_results_analysis(self, context, intention, result: 'InterpretationResult') -> 'InterpretationResult':
        """
        ğŸ§  MASTER MANEJA ANÃLISIS DE RESULTADOS
        Procesa los resultados del Student y decide quÃ© preguntar al usuario
        """
        try:
            self.logger.info("ğŸ”„ [MASTER] Analizando resultados para comunicaciÃ³n")

            row_count = result.parameters.get('row_count', 0)
            user_query = context.user_message

            # Determinar tipo de pregunta basado en los resultados
            if 'constancia' in user_query.lower() and row_count > 1:
                return self._create_candidate_selection_question(result, context)
            elif 'buscar' in user_query.lower() and row_count > 50:
                return self._create_filter_suggestion_question(result, context)
            elif row_count == 0:
                return self._create_no_results_help_question(result, context)
            else:
                # No deberÃ­a llegar aquÃ­, pero por seguridad
                return result

        except Exception as e:
            self.logger.error(f"Error analizando resultados: {e}")
            return result

    def _create_candidate_selection_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta para seleccionar candidato para constancia"""
        try:
            data = result.parameters.get('data', [])
            candidates = []

            if isinstance(data, list):
                for item in data[:5]:  # MÃ¡ximo 5 candidatos
                    candidates.append({
                        'nombre': item.get('nombre', 'N/A'),
                        'grado': f"{item.get('grado', 'N/A')}Â°{item.get('grupo', '')}"
                    })
            elif isinstance(data, dict):
                # Si data es un diccionario, tratarlo como un solo candidato
                candidates.append({
                    'nombre': data.get('nombre', 'N/A'),
                    'grado': f"{data.get('grado', 'N/A')}Â°{data.get('grupo', '')}"
                })

            message = f"ğŸ” EncontrÃ© {len(data)} candidatos para la constancia. Â¿CuÃ¡l necesitas?\n\n"
            for i, candidate in enumerate(candidates, 1):
                message += f"**{i}.** {candidate['nombre']} ({candidate['grado']})\n"

            message += f"\nğŸ’¡ Responde con el nÃºmero de la opciÃ³n que necesitas."

            return InterpretationResult(
                action="solicitar_seleccion_constancia",
                parameters={
                    "message": message,
                    "candidates": candidates,
                    "original_query": context.user_message,
                    "waiting_for": "selection",
                    "original_data": data
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de selecciÃ³n: {e}")
            return result

    def _create_filter_suggestion_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta para sugerir filtros en bÃºsquedas amplias"""
        try:
            row_count = result.parameters.get('row_count', 0)

            message = f"ğŸ” EncontrÃ© {row_count} resultados. Â¿Buscabas a todos o necesitas filtrar por algo especÃ­fico como grado, grupo o turno?"

            return InterpretationResult(
                action="solicitar_filtros",
                parameters={
                    "message": message,
                    "original_query": context.user_message,
                    "result_count": row_count,
                    "waiting_for": "filter_specification",
                    "original_data": result.parameters.get('data', [])
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de filtros: {e}")
            return result

    def _create_no_results_help_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta de ayuda cuando no hay resultados"""
        try:
            message = f"ğŸ¤” No encontrÃ© resultados para '{context.user_message}'. Â¿Quieres que busque con otros criterios o necesitas ayuda?"

            return InterpretationResult(
                action="solicitar_ayuda_busqueda",
                parameters={
                    "message": message,
                    "original_query": context.user_message,
                    "waiting_for": "search_help"
                },
                confidence=0.8
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de ayuda: {e}")
            return result

    def _log_strategic_context(self):
        """ğŸ§  [MASTER] Contexto estratÃ©gico del sistema"""
        try:
            self.logger.info("ğŸ§  [MASTER] Sistema Master-Student inicializado")
            self.logger.info(f"ğŸ§  [MASTER] Especialistas disponibles: {list(self.system_map.keys())}")
            self.logger.info("ğŸ§  [MASTER] Listo para procesar consultas")



        except Exception as e:
            self.logger.error(f"âŒ Error mostrando contexto detallado: {e}")

    def _handle_expected_response(self, context: InterpretationContext, conversation_state: dict) -> Optional[InterpretationResult]:
        """
        Maneja respuestas esperadas basadas en el estado conversacional
        VersiÃ³n simplificada que usa el Context Manager
        """
        waiting_for = conversation_state.get('waiting_for')
        user_message = context.user_message.lower().strip()

        # ğŸ†• DETECTAR CONFIRMACIONES DESDE CONFIGURACIÃ“N CENTRALIZADA
        confirmations = Config.RESPONSES['confirmation_words']

        if waiting_for == "confirmacion_constancia_estudios" and user_message in confirmations:
            return self._handle_confirmation(context, conversation_state)

        # AquÃ­ se pueden agregar otros tipos de respuestas esperadas
        # elif waiting_for == "seleccion_alumno":
        #     return self._handle_selection(context, conversation_state)

        return None

    def _generate_master_response(self, student_result: 'InterpretationResult', user_query: str) -> 'InterpretationResult':
        """
        ğŸ¯ MASTER COMO VOCERO: Genera respuesta final basada en reporte del Student

        Args:
            student_result: Resultado tÃ©cnico del Student
            user_query: Consulta original del usuario

        Returns:
            InterpretationResult con respuesta final del Master
        """
        try:
            self.logger.info("ğŸ—£ï¸ [MASTER] Generando respuesta final como vocero...")

            # Extraer datos tÃ©cnicos del Student
            student_data = student_result.parameters
            action_used = student_result.action

            # ğŸ”§ DEBUG: Mostrar reporte recibido del Student
            self._debug_pause("ğŸ“¥ [MASTER] RECIBIENDO REPORTE DEL STUDENT", {
                "action_recibida": action_used,
                "datos_tecnicos": list(student_data.keys()),
                "row_count": student_data.get('row_count', 0),
                "requiere_respuesta_master": student_data.get('requires_master_response', False),
                "sql_ejecutado": student_data.get('sql_executed', '')[:50] + "..." if student_data.get('sql_executed') else "N/A"
            })

            # ğŸ¯ EXTRAER CRITERIOS DE BÃšSQUEDA DINÃMICAMENTE DESPUÃ‰S DE LA EJECUCIÃ“N
            search_criteria = self._extract_search_criteria_for_display(student_data)

            # ğŸ¯ AGREGAR CRITERIOS A STUDENT_DATA PARA LAS FUNCIONES DE RESPUESTA
            student_data["search_criteria"] = search_criteria

            # ğŸ¯ MASTER GENERA RESPUESTA FINAL USANDO PROMPT ESPECIALIZADO
            self._debug_pause("ğŸ§  [MASTER] INTERPRETANDO REPORTE Y GENERANDO RESPUESTA", {
                "tipo_consulta": self._detect_query_type(action_used, student_data, user_query),
                "criterios_busqueda": len(search_criteria),
                "datos_disponibles": student_data.get('row_count', 0),
                "prompt_especializado": "Generando respuesta contextual con LLM"
            })

            master_response = self._generate_master_response_with_llm(student_data, user_query, action_used)

            # ğŸ”§ CASOS ESPECIALES QUE REQUIEREN PROCESAMIENTO ADICIONAL
            if action_used == "seleccion_realizada":
                # Respuesta de selecciÃ³n - mostrar datos del elemento seleccionado
                elemento_seleccionado = student_data.get("elemento_seleccionado")
                posicion = student_data.get("posicion", "N/A")

                if elemento_seleccionado:
                    # Preparar datos para mostrar en la interfaz
                    nombre = elemento_seleccionado.get('nombre', 'N/A')
                    master_response = f"ğŸ‘¤ InformaciÃ³n del alumno en posiciÃ³n {posicion}: **{nombre}**"

                    # Agregar los datos del elemento seleccionado para que se muestren en la interfaz
                    student_data["data"] = [elemento_seleccionado]
                    student_data["row_count"] = 1
                    student_data["human_response"] = master_response
                else:
                    master_response = student_data.get("message", "SelecciÃ³n procesada exitosamente")

            elif action_used == "transformation_preview":
                # ğŸ”„ RESPUESTA ESPECÃFICA PARA TRANSFORMACIONES (mantener por ahora)
                transformation_info = student_data.get("transformation_info", {})
                if transformation_info:
                    tipo_constancia = (transformation_info.get("tipo_constancia") or
                                     transformation_info.get("tipo_transformacion") or
                                     student_data.get("tipo_constancia", "constancia"))
                    alumno_info = (transformation_info.get("alumno") or
                                 student_data.get("alumno", {}))
                    alumno_nombre = alumno_info.get("nombre", "el alumno")

                    master_response = (f"âœ… **TransformaciÃ³n completada exitosamente**\n\n"
                                     f"He convertido tu PDF a una constancia de **{tipo_constancia}** para **{alumno_nombre}**.\n\n"
                                     f"ğŸ“„ **En el panel derecho puedes:**\n\n"
                                     f"Ver la vista previa, comparar con el original, revisar datos extraÃ­dos y abrir en navegador para imprimir.\n\n"
                                     f"ğŸ’¡ Â¿Necesitas hacer algÃºn ajuste o tienes otra consulta?")

            # ğŸ¯ CREAR RESULTADO FINAL CON RESPUESTA DEL MASTER
            final_result = InterpretationResult(
                action=student_result.action,
                parameters={
                    **student_data,  # Mantener datos tÃ©cnicos del Student
                    "human_response": master_response,  # Respuesta final del Master
                    "master_generated": True,  # Flag para indicar que Master generÃ³ la respuesta
                    "student_action": action_used,  # AcciÃ³n original del Student
                    "search_criteria": search_criteria,  # ğŸ†• Criterios para mostrar en listado
                },
                confidence=student_result.confidence
            )

            self.logger.info(f"âœ… [MASTER] Respuesta final: '{master_response[:50]}...'")
            return final_result

        except Exception as e:
            self.logger.error(f"âŒ [MASTER] Error generando respuesta final: {e}")
            # Fallback: retornar resultado original del Student
            return student_result

    def _extract_search_criteria_for_display(self, student_data: dict) -> dict:
        """ğŸ¯ EXTRAE CRITERIOS DE BÃšSQUEDA DINÃMICAMENTE DEL SQL EJECUTADO"""
        try:
            # ğŸ§  [MASTER] Analizando SQL ejecutado para extraer criterios
            sql_query = student_data.get("sql_executed", "") or student_data.get("sql_query", "")
            search_description = ""
            relevant_fields = []

            self.logger.info(f"ğŸ§  [MASTER] SQL encontrado: '{sql_query[:50]}...'" if sql_query else "ğŸ§  [MASTER] No hay SQL disponible")

            if sql_query:
                # Extraer campos de WHERE clause dinÃ¡micamente
                import re

                # ğŸ¯ PATRONES COMPLETOS PARA TODOS LOS CRITERIOS POSIBLES
                where_patterns = [
                    # ğŸ“… FECHAS
                    (r'fecha_nacimiento\s+LIKE\s+[\'"]%(\d{4})%[\'"]', 'fecha_nacimiento', 'nacidos en {}'),
                    (r'fecha_nacimiento\s+BETWEEN\s+[\'"](\d{4}-\d{2}-\d{2})[\'"].*[\'"](\d{4}-\d{2}-\d{2})[\'"]', 'fecha_nacimiento', 'nacidos entre {} y {}'),
                    (r'fecha_nacimiento\s*=\s*[\'"]([^\'\"]+)[\'"]', 'fecha_nacimiento', 'nacidos el {}'),

                    # ğŸ“ DATOS ESCOLARES
                    (r'grado\s*=\s*[\'"](\w+)[\'"]', 'grado', '{}Â° grado'),
                    (r'grupo\s*=\s*[\'"](\w+)[\'"]', 'grupo', 'grupo {}'),
                    (r'turno\s*=\s*[\'"](\w+)[\'"]', 'turno', 'turno {}'),

                    # ğŸ‘¤ IDENTIFICADORES
                    (r'matricula\s*=\s*[\'"]([^\'\"]+)[\'"]', 'matricula', 'matrÃ­cula {}'),
                    (r'curp\s*=\s*[\'"]([^\'\"]+)[\'"]', 'curp', 'CURP {}'),
                    (r'nombre\s+LIKE\s+[\'"]%([^%\'\"]+)%[\'"]', 'nombre', 'con nombre que contiene "{}"'),
                    (r'nombre\s*=\s*[\'"]([^\'\"]+)[\'"]', 'nombre', 'llamado {}'),

                    # ğŸ“Š CALIFICACIONES
                    (r'calificaciones\s+IS\s+NOT\s+NULL', 'calificaciones_status', 'con calificaciones'),
                    (r'calificaciones\s+IS\s+NULL', 'calificaciones_status', 'sin calificaciones'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*>\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio mayor a {}'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*<\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio menor a {}'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*=\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio de {}'),

                    # ğŸ  DATOS PERSONALES
                    (r'telefono\s*=\s*[\'"]([^\'\"]+)[\'"]', 'telefono', 'con telÃ©fono {}'),
                    (r'direccion\s+LIKE\s+[\'"]%([^%\'\"]+)%[\'"]', 'direccion', 'que viven en {}'),
                    (r'email\s*=\s*[\'"]([^\'\"]+)[\'"]', 'email', 'con email {}'),

                    # ğŸ”¢ RANGOS NUMÃ‰RICOS
                    (r'edad\s*>\s*(\d+)', 'edad', 'mayores de {} aÃ±os'),
                    (r'edad\s*<\s*(\d+)', 'edad', 'menores de {} aÃ±os'),
                    (r'edad\s*=\s*(\d+)', 'edad', 'de {} aÃ±os'),
                ]

                for pattern, field, description_template in where_patterns:
                    matches = re.findall(pattern, sql_query, re.IGNORECASE)
                    if matches:
                        relevant_fields.append(field)
                        if isinstance(matches[0], tuple):
                            # MÃºltiples grupos (ej: BETWEEN)
                            search_description += description_template.format(*matches[0]) + " "
                        else:
                            # Un solo grupo
                            search_description += description_template.format(matches[0]) + " "

                self.logger.info(f"ğŸ§  [MASTER] Criterios extraÃ­dos: {len(relevant_fields)} campos")
                if search_description.strip():
                    self.logger.info(f"ğŸ§  [MASTER] DescripciÃ³n: {search_description.strip()}")

            # Si no hay SQL o no se encontraron patrones, usar fallback inteligente
            if not relevant_fields:
                # Analizar parÃ¡metros de la acciÃ³n como fallback
                action_params = student_data.get("action_params", {})
                criterio_principal = action_params.get("criterio_principal", {})
                campo = criterio_principal.get("campo", "")

                if campo:
                    relevant_fields.append(campo)
                    search_description = f"bÃºsqueda por {campo}"
                    self.logger.info(f"ğŸ§  [MASTER] Campo principal: {campo}")

            # Incluir campos bÃ¡sicos dinÃ¡micamente desde configuraciÃ³n
            from app.core.config import Config
            basic_fields = getattr(Config, 'BASIC_DISPLAY_FIELDS', ['nombre', 'curp'])
            all_fields = basic_fields + [field for field in relevant_fields if field not in basic_fields]

            return {
                "fields_to_show": all_fields,
                "search_description": search_description.strip(),
                "has_specific_criteria": len(relevant_fields) > 0
            }

        except Exception as e:
            self.logger.error(f"Error extrayendo criterios dinÃ¡micamente: {e}")
            return {
                "fields_to_show": ['nombre', 'curp', 'turno'],  # Fallback por defecto
                "search_description": "",
                "has_specific_criteria": False
            }




    def _generate_master_response_with_llm(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        ğŸ—£ï¸ MASTER GENERA RESPUESTA HUMANIZADA CON CONTEXTO CONVERSACIONAL

        MEJORA: Ahora incluye contexto conversacional para respuestas contextuales
        El Master usa su propio prompt especializado en comunicaciÃ³n para generar
        respuestas humanizadas basÃ¡ndose en los datos tÃ©cnicos del Student.
        """
        try:
            # ğŸ¯ OBTENER CONTEXTO CONVERSACIONAL COMPLETO (IGUAL QUE MASTER INICIAL)
            conversation_stack = getattr(self, 'current_conversation_stack', [])
            context_info = self._create_context_summary(conversation_stack)

            # Crear prompt con contexto completo
            master_prompt = self._create_master_response_prompt_with_context(
                student_data, user_query, action_used, context_info
            )

            # Llamar al LLM para generar respuesta humanizada
            response = self.gemini_client.send_prompt_sync(master_prompt)

            if response and response.strip():
                self.logger.info(f"âœ… Master generÃ³ respuesta contextual exitosamente")
                return response.strip()
            else:
                self.logger.warning(f"âŒ Master LLM no generÃ³ respuesta, usando fallback")
                return self._generate_fallback_response(student_data, action_used)

        except Exception as e:
            self.logger.error(f"Error generando respuesta con Master LLM: {e}")
            return self._generate_fallback_response(student_data, action_used)

    def _generate_fallback_response(self, student_data: dict, action_used: str) -> str:
        """Genera respuesta de fallback si el LLM del Master falla"""
        row_count = student_data.get("row_count", 0)

        if action_used in ["BUSCAR_UNIVERSAL", "GENERAR_LISTADO_COMPLETO"]:
            if row_count == 0:
                return "No encontrÃ© resultados para tu bÃºsqueda. Â¿PodrÃ­as intentar con otros criterios?"
            elif row_count == 1:
                return "EncontrÃ© un resultado que coincide con tu bÃºsqueda."
            else:
                return f"EncontrÃ© {row_count} resultados que coinciden con tu bÃºsqueda."
        elif action_used in ["CONTAR_ALUMNOS", "CONTAR_UNIVERSAL"]:
            # Extraer el valor real del conteo desde los datos
            data = student_data.get("data", [])
            if data and isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
                total_count = data[0].get("total", row_count)
                return f"El conteo se completÃ³ exitosamente: {total_count} elementos."
            else:
                return f"El conteo se completÃ³ exitosamente: {row_count} elementos."
        else:
            return "Consulta procesada exitosamente."

    def _create_master_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        ğŸ¯ CREA PROMPT ESPECIALIZADO DINÃMICO SEGÃšN TIPO DE CONSULTA

        Diferentes tipos de consulta requieren diferentes enfoques de respuesta.
        INCLUYE INTERPRETACIÃ“N INTELIGENTE DEL REPORTE DEL STUDENT.
        """
        # ğŸ§  INTERPRETACIÃ“N INTELIGENTE DEL REPORTE
        intelligent_interpretation = self._interpret_student_report_intelligently(student_data, user_query)

        # ğŸ›‘ PAUSA ESTRATÃ‰GICA #5: INTERPRETACIÃ“N INTELIGENTE DEL REPORTE
        import os
        if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
            print(f"\nğŸ›‘ [MASTER-BRAIN] PASO 5: INTERPRETACIÃ“N INTELIGENTE DEL REPORTE")
            print(f"    â”œâ”€â”€ ğŸ§  PREGUNTA: Â¿QuÃ© pasÃ³ y cÃ³mo respondo al usuario?")
            print(f"    â”œâ”€â”€ ğŸ“ Consulta original: '{user_query}'")
            print(f"    â”œâ”€â”€ âš¡ AcciÃ³n ejecutada por Student: {action_used}")
            print(f"    â”œâ”€â”€ ğŸ“Š Resultados obtenidos: {student_data.get('row_count', 0)} elementos")
            print(f"    â”œâ”€â”€ âœ… Ã‰xito de la operaciÃ³n: {student_data.get('success', True)}")
            print(f"    â”œâ”€â”€ ğŸ§  InterpretaciÃ³n del conocimiento:")
            print(f"    â”‚   {intelligent_interpretation}")
            print(f"    â”œâ”€â”€ âš¡ DECISIÃ“N: Generar respuesta humanizada para el usuario")
            print(f"    â””â”€â”€ Presiona ENTER para generar respuesta final...")
            input()

        # Detectar tipo de consulta
        query_type = self._detect_query_type(action_used, student_data, user_query)

        # Crear prompt especÃ­fico segÃºn el tipo
        base_prompt = ""
        if query_type == "search":
            base_prompt = self._create_search_response_prompt(student_data, user_query, action_used)
        elif query_type == "constancia":
            base_prompt = self._create_constancia_response_prompt(student_data, user_query, action_used)
        elif query_type == "transformation":
            base_prompt = self._create_transformation_response_prompt(student_data, user_query, action_used)
        elif query_type == "statistics":
            base_prompt = self._create_statistics_response_prompt(student_data, user_query, action_used)
        elif query_type == "help":
            base_prompt = self._create_help_response_prompt(student_data, user_query, action_used)
        else:
            base_prompt = self._create_generic_response_prompt(student_data, user_query, action_used)

        # ğŸ§  AGREGAR INTERPRETACIÃ“N INTELIGENTE AL PROMPT
        enhanced_prompt = f"""
{base_prompt}

ğŸ§  INTERPRETACIÃ“N INTELIGENTE DEL MASTER:
{intelligent_interpretation}

ğŸ¯ INSTRUCCIONES ADICIONALES:
- Usa la interpretaciÃ³n inteligente para mejorar tu respuesta
- Si hay sugerencias, incorpÃ³ralas naturalmente
- Si hay limitaciones, explÃ­calas de manera empÃ¡tica
- MantÃ©n un tono profesional pero amigable

RESPONDE ÃšNICAMENTE con la respuesta conversacional final mejorada.
"""

        return enhanced_prompt

    def _create_master_response_prompt_with_context(self, student_data: dict, user_query: str, action_used: str, context_info: str) -> str:
        """
        ğŸ—£ï¸ MASTER RESPUESTA CON CONTEXTO CONVERSACIONAL

        NUEVO: Prompt de respuesta que incluye contexto conversacional completo
        """
        # Detectar tipo de consulta
        query_type = self._detect_query_type(action_used, student_data, user_query)

        # Crear prompt base segÃºn el tipo
        base_prompt = self._create_master_response_prompt(student_data, user_query, action_used)

        # Agregar contexto conversacional al prompt
        contextual_prompt = f"""
ğŸ—£ï¸ MASTER COMO VOCERO - RESPUESTA CONTEXTUAL INTELIGENTE

CONTEXTO CONVERSACIONAL:
{context_info}

CONSULTA ORIGINAL: "{user_query}"
RESULTADO DEL STUDENT: {action_used} - {student_data.get('row_count', 0)} resultados

ğŸ¯ GENERAR RESPUESTA NATURAL Y CONTEXTUAL:
1. Reconocer contexto conversacional cuando sea relevante
2. Comunicar resultado de manera clara
3. Mantener personalidad consistente
4. Conectar con consultas anteriores cuando sea natural

EJEMPLOS:
- ContinuaciÃ³n: "Perfecto! BasÃ¡ndome en [contexto]..."
- Referencia resuelta: "He generado la constancia para [nombre resuelto]..."
- Filtro aplicado: "De los resultados anteriores, encontrÃ©..."

{base_prompt}

IMPORTANTE: Si hay contexto conversacional relevante, conÃ©ctalo naturalmente en tu respuesta.
"""

        return contextual_prompt

    def _detect_query_type(self, action_used: str, student_data: dict, user_query: str) -> str:
        """Detecta el tipo especÃ­fico de consulta para usar el prompt correcto"""
        # ğŸ¯ AYUDA DEL SISTEMA (SIMPLIFICADO A 2 ACCIONES)
        if action_used in ["AYUDA_CAPACIDADES", "AYUDA_TUTORIAL"] or student_data.get("query_category") == "ayuda_sistema":
            return "help"

        # Constancias
        if action_used in ["constancia_generada", "PREPARAR_DATOS_CONSTANCIA"] or "constancia" in user_query.lower():
            return "constancia"

        # Transformaciones
        if action_used in ["transformation_completed", "transformation_preview"] or "transform" in user_query.lower():
            return "transformation"

        # EstadÃ­sticas
        if action_used in ["CONTAR_ALUMNOS", "CONTAR_UNIVERSAL", "CALCULAR_ESTADISTICA"] or any(word in user_query.lower() for word in ["cuÃ¡ntos", "total", "estadÃ­stica", "conteo"]):
            return "statistics"

        # BÃºsquedas (por defecto)
        return "search"

    def _create_search_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para bÃºsquedas y listados"""
        row_count = student_data.get("row_count", 0)
        data = student_data.get("data", [])
        ambiguity_level = student_data.get("ambiguity_level", "low")

        # ğŸ¯ MANEJO INTELIGENTE DE "NO ENCONTRADO" (row_count = 0)
        if row_count == 0:
            return self._create_no_results_response_prompt(student_data, user_query, action_used)

        # ğŸ”§ MANEJO SEGURO DE DATOS - verificar que data sea una lista
        data_context = ""
        if data and isinstance(data, list) and len(data) > 0:
            if len(data) <= 3:
                data_context = "RESULTADOS ENCONTRADOS:\n"
                for i, item in enumerate(data, 1):
                    if isinstance(item, dict):
                        # ğŸ¯ MANEJO DINÃMICO DE CAMPOS - usar los campos que realmente existen
                        item_info = []
                        if "nombre" in item:
                            item_info.append(item["nombre"])
                        if "matricula" in item:
                            item_info.append(f"MatrÃ­cula: {item['matricula']}")
                        if "grado" in item and "grupo" in item:
                            item_info.append(f"{item['grado']}Â° {item['grupo']}")
                        elif "grado" in item:
                            item_info.append(f"Grado: {item['grado']}")
                        elif "grupo" in item:
                            item_info.append(f"Grupo: {item['grupo']}")
                        if "curp" in item:
                            item_info.append(f"CURP: {item['curp']}")

                        # Si no hay campos reconocidos, mostrar todos los disponibles
                        if not item_info:
                            item_info = [f"{k}: {v}" for k, v in item.items() if v is not None]

                        data_context += f"{i}. {' - '.join(item_info)}\n"
            else:
                data_context = f"PRIMEROS 3 DE {len(data)} RESULTADOS:\n"
                for i in range(min(3, len(data))):
                    item = data[i]
                    if isinstance(item, dict):
                        # ğŸ¯ MANEJO DINÃMICO DE CAMPOS - usar los campos que realmente existen
                        item_info = []
                        if "nombre" in item:
                            item_info.append(item["nombre"])
                        if "matricula" in item:
                            item_info.append(f"MatrÃ­cula: {item['matricula']}")
                        if "grado" in item and "grupo" in item:
                            item_info.append(f"{item['grado']}Â° {item['grupo']}")
                        elif "grado" in item:
                            item_info.append(f"Grado: {item['grado']}")
                        elif "grupo" in item:
                            item_info.append(f"Grupo: {item['grupo']}")
                        if "curp" in item:
                            item_info.append(f"CURP: {item['curp']}")

                        # Si no hay campos reconocidos, mostrar todos los disponibles
                        if not item_info:
                            item_info = [f"{k}: {v}" for k, v in item.items() if v is not None]

                        data_context += f"{i+1}. {' - '.join(item_info)}\n"

        # Detectar si hay contexto conversacional previo
        reflexion = student_data.get("auto_reflexion", {})
        datos_recordar = reflexion.get("datos_recordar", {})
        conversation_context = datos_recordar.get("context", "")
        query_anterior = datos_recordar.get("query", "")

        # ğŸ¯ MEJORAR DETECCIÃ“N DE CONTINUACIÃ“N
        # Verificar mÃºltiples fuentes para detectar continuaciÃ³n
        master_intention = student_data.get("master_intention", {})
        categoria_master = master_intention.get("categoria", "")

        # Es continuaciÃ³n si:
        # 1. Hay contexto conversacional previo, O
        # 2. El Master categorizÃ³ como "continuacion", O
        # 3. La consulta tiene palabras de referencia contextual
        palabras_continuacion = ["ellos", "esos", "esas", "de ellos", "de esas", "ahora", "tambiÃ©n"]
        tiene_referencia = any(palabra in user_query.lower() for palabra in palabras_continuacion)

        es_continuacion = bool(
            conversation_context or
            query_anterior or
            categoria_master == "continuacion" or
            tiene_referencia
        )

        return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ğŸ«

ğŸ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- RESULTADOS: {row_count} estudiantes encontrados
- AMBIGÃœEDAD: {ambiguity_level}
- ES CONTINUACIÃ“N: {es_continuacion}
- CONTEXTO PREVIO: {conversation_context}
- CONSULTA ANTERIOR: {query_anterior}

{data_context}

ğŸ­ TU PERSONALIDAD:
- Entusiasta y humano (usa emojis apropiados)
- Profesional pero cercano
- EmpÃ¡tico y comprensivo
- Proactivo en sugerencias

ğŸ¯ TU TAREA PARA BÃšSQUEDAS:
Generar una respuesta HUMANA y CONECTADA que:

1. ğŸ‰ SALUDA con entusiasmo apropiado
2. ğŸ“Š PRESENTA los resultados de manera atractiva
3. ğŸ” EXPLICA quÃ© buscaste de forma natural
4. ğŸ¤” MANEJA la ambigÃ¼edad con empatÃ­a
5. ğŸ’¡ SUGIERE prÃ³ximos pasos Ãºtiles
6. ğŸ”„ CONECTA con el contexto conversacional si existe

ğŸ¯ MANEJO DE AMBIGÃœEDAD CON EMPATÃA:
- HIGH (10+ resultados): "Â¡EncontrÃ© muchos estudiantes! ğŸ˜Š Como [apellido] es comÃºn, te muestro todos para que encuentres al que necesitas. Â¿PodrÃ­as ser mÃ¡s especÃ­fico con el nombre o grado?"
- MEDIUM (4-9 resultados): "Â¡Perfecto! ğŸ‘ EncontrÃ© varios estudiantes que coinciden. Â¿Necesitas informaciÃ³n especÃ­fica de alguno?"
- LOW (1-3 resultados): "Â¡Excelente! âœ… AquÃ­ tienes [lo que encontrÃ©]..."

ğŸ”„ CONTINUIDAD CONVERSACIONAL (MUY IMPORTANTE):
- Si ES_CONTINUACIÃ“N = True: NUNCA digas "Â¡Hola!" - usa "Â¡Perfecto! ğŸ‘", "Â¡Excelente! âœ…", "Siguiendo con tu bÃºsqueda anterior..."
- Si ES_CONTINUACIÃ“N = False: Puedes saludar con "Â¡Hola! ğŸ‘‹"
- SIEMPRE conecta con la consulta anterior cuando hay contexto
- Menciona especÃ­ficamente quÃ© filtros se aplicaron sobre los datos previos

âœ… ENFOQUE HUMANO:
- Resultados presentados con entusiasmo
- Criterios explicados naturalmente
- Sugerencias Ãºtiles y empÃ¡ticas
- Tono conversacional y amigable

ğŸ“ FORMATO HUMANO:
- Saludo apropiado con emoji
- MÃ¡ximo 3-4 lÃ­neas pero con personalidad
- Cierre que invite a continuar la conversaciÃ³n

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_no_results_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        ğŸ¯ PROMPT ESPECIALIZADO PARA CASOS DE "NO ENCONTRADO" (row_count = 0)

        Master interpreta inteligentemente cuando Student reporta 0 resultados
        y genera respuestas humanas con sugerencias Ãºtiles.
        """
        # Detectar tipo de bÃºsqueda para respuesta especÃ­fica
        search_criteria = student_data.get("search_criteria", "")

        # Detectar si es bÃºsqueda por CURP
        is_curp_search = "curp" in user_query.lower() or "curp" in search_criteria.lower()

        # Detectar si es bÃºsqueda por matrÃ­cula
        is_matricula_search = "matrÃ­cula" in user_query.lower() or "matricula" in user_query.lower()

        # Detectar si es bÃºsqueda por nombre
        is_name_search = any(word in user_query.lower() for word in ["buscar", "encontrar", "dame", "informaciÃ³n"]) and not is_curp_search and not is_matricula_search

        # Detectar contexto conversacional
        reflexion = student_data.get("auto_reflexion", {})
        datos_recordar = reflexion.get("datos_recordar", {})
        conversation_context = datos_recordar.get("context", "")
        es_continuacion = bool(conversation_context)

        return f"""
Eres el asistente empÃ¡tico y Ãºtil de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ğŸ«

ğŸ¯ SITUACIÃ“N CRÃTICA - NO SE ENCONTRARON RESULTADOS:
- CONSULTA: "{user_query}"
- CRITERIOS BUSCADOS: {search_criteria}
- RESULTADOS: 0 estudiantes encontrados
- ES CONTINUACIÃ“N: {es_continuacion}
- CONTEXTO PREVIO: {conversation_context}
- BÃšSQUEDA POR CURP: {is_curp_search}
- BÃšSQUEDA POR MATRÃCULA: {is_matricula_search}
- BÃšSQUEDA POR NOMBRE: {is_name_search}

ğŸ­ TU PERSONALIDAD EMPÃTICA:
- Comprensivo y Ãºtil (NO frustrante)
- Profesional pero humano
- Proactivo en soluciones
- Educativo sin ser condescendiente

ğŸ¯ TU TAREA ESPECÃFICA PARA "NO ENCONTRADO":
Generar una respuesta EMPÃTICA Y ÃšTIL que:

1. ğŸ¤” RECONOCE que no encontraste nada (sin culpar al usuario)
2. ğŸ’¡ EXPLICA posibles causas de manera educativa
3. ğŸ” SUGIERE alternativas especÃ­ficas y Ãºtiles
4. ğŸ¯ OFRECE prÃ³ximos pasos concretos
5. ğŸ”„ MANTIENE continuidad conversacional si existe

ğŸ¯ RESPUESTAS ESPECÃFICAS POR TIPO DE BÃšSQUEDA:

**Para BÃšSQUEDAS POR CURP:**
- "No encontrÃ© ningÃºn alumno con esa CURP en nuestra base de datos."
- "Las CURPs tienen exactamente 18 caracteres. Â¿PodrÃ­as verificar que estÃ© completa?"
- "TambiÃ©n puedes buscar por nombre si prefieres: 'buscar [nombre del alumno]'"

**Para BÃšSQUEDAS POR MATRÃCULA:**
- "No encontrÃ© esa matrÃ­cula en nuestros registros."
- "Â¿PodrÃ­as verificar el nÃºmero? TambiÃ©n puedes buscar por nombre del alumno."

**Para BÃšSQUEDAS POR NOMBRE:**
- "No encontrÃ© ningÃºn alumno con ese nombre."
- "Â¿PodrÃ­as intentar con el apellido? Por ejemplo: 'buscar GarcÃ­a'"
- "O puedes ser mÃ¡s especÃ­fico: 'buscar MarÃ­a GarcÃ­a de 3er grado'"

ğŸ”„ CONTINUIDAD CONVERSACIONAL:
- Si ES_CONTINUACIÃ“N = True: "Siguiendo con tu bÃºsqueda anterior, no encontrÃ©..."
- Si ES_CONTINUACIÃ“N = False: "No encontrÃ©..."
- SIEMPRE ofrecer alternativas basadas en el contexto

âœ… PATRONES DE RESPUESTA EMPÃTICA:
- "No encontrÃ© [lo que buscaste], pero puedes intentar..."
- "Hmm, no hay resultados para [criterio]. Â¿Te ayudo de otra forma?"
- "No localicÃ© [lo especÃ­fico]. Â¿Quieres que busque por [alternativa]?"

âŒ EVITA RESPUESTAS TÃ‰CNICAS O FRÃAS:
- "0 resultados encontrados"
- "La consulta no devolviÃ³ datos"
- "No hay coincidencias en la base de datos"

ğŸ“ FORMATO EMPÃTICO Y ÃšTIL:
- Reconocimiento empÃ¡tico del problema
- ExplicaciÃ³n breve y educativa
- 2-3 sugerencias concretas y especÃ­ficas
- InvitaciÃ³n amigable a continuar
- MÃ¡ximo 4-5 lÃ­neas con personalidad humana

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_constancia_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para constancias generadas"""
        # ğŸ”§ MANEJO SEGURO DE DATOS - puede ser lista o string
        data = student_data.get("data", [])
        if isinstance(data, list) and len(data) > 0:
            alumno_data = data[0]
        elif isinstance(data, dict):
            alumno_data = data
        else:
            alumno_data = {}

        # Obtener nombre del alumno de mÃºltiples fuentes posibles
        alumno_nombre = (
            alumno_data.get("nombre") or
            student_data.get("alumno", {}).get("nombre") if isinstance(student_data.get("alumno"), dict) else
            student_data.get("alumno") if isinstance(student_data.get("alumno"), str) else
            "el estudiante"
        )

        tipo_constancia = student_data.get("tipo_constancia", "constancia")

        # Detectar contexto conversacional
        reflexion = student_data.get("reflexion_conversacional", {})
        conversation_context = reflexion.get("datos_recordar", {}).get("context", "")

        return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ğŸ«

ğŸ¯ SITUACIÃ“N:
- CONSTANCIA GENERADA: {tipo_constancia}
- PARA ESTUDIANTE: {alumno_nombre}
- CONSULTA ORIGINAL: "{user_query}"
- CONTEXTO PREVIO: {conversation_context}

ğŸ­ TU PERSONALIDAD:
- Entusiasta y celebrativo (usa emojis de Ã©xito)
- Profesional pero cercano
- GuÃ­a claro y Ãºtil
- EmpÃ¡tico y comprensivo

ğŸ¯ TU TAREA PARA CONSTANCIAS:
Generar una respuesta HUMANA y CELEBRATIVA que:

1. ğŸ‰ CELEBRA el Ã©xito de la generaciÃ³n
2. ğŸ“± EXPLICA el panel derecho de manera amigable
3. ğŸ›ï¸ MENCIONA botones especÃ­ficos Ãºtiles
4. ğŸ’¡ GUÃA prÃ³ximos pasos claramente
5. ğŸ”„ CONECTA con contexto conversacional si existe

ğŸ›ï¸ FUNCIONALIDADES DEL PANEL (explica amigablemente):
- BotÃ³n superior izquierdo: "puedes abrir/cerrar el panel"
- Vista previa: "visor PDF con zoom para revisar tu constancia"
- "Ver datos del alumno": "revisa la informaciÃ³n extraÃ­da"
- "Quitar PDF": "si quieres subir otro documento"
- "Abrir navegador/imprimir": "para guardar o imprimir"
- IMPORTANTE: "Solo vista previa - para guardar usa el navegador"

ğŸ”„ CONTINUIDAD CONVERSACIONAL:
- Si hay contexto previo, reconÃ³celo: "Â¡Perfecto! Siguiendo con [contexto]..."
- Si es nueva constancia, celebra: "Â¡Excelente! ğŸ‰"
- Siempre invita a continuar: "Â¿Necesitas algo mÃ¡s?"

ğŸ“ FORMATO HUMANO Y CELEBRATIVO:
- ConfirmaciÃ³n entusiasta con emojis
- ExplicaciÃ³n clara pero amigable del panel
- MÃ¡ximo 4-5 lÃ­neas con personalidad
- Cierre que invite a continuar

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_transformation_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para transformaciones de PDF"""
        transformation_info = student_data.get("transformation_info", {})
        tipo_constancia = transformation_info.get("tipo_constancia", "constancia")
        alumno_info = transformation_info.get("alumno", {})
        alumno_nombre = alumno_info.get("nombre", "el alumno")

        return f"""
Eres el asistente de la escuela especializado en TRANSFORMACIÃ“N DE PDFs.

ğŸ¯ SITUACIÃ“N:
- TRANSFORMACIÃ“N COMPLETADA: PDF â†’ {tipo_constancia}
- PARA ESTUDIANTE: {alumno_nombre}
- CONSULTA ORIGINAL: "{user_query}"

ğŸ¯ TU TAREA ESPECÃFICA PARA TRANSFORMACIONES:
Generar una respuesta ENFOCADA EN COMPARACIÃ“N que:

1. âœ… CONFIRME que la transformaciÃ³n se completÃ³
2. ğŸ”„ EXPLIQUE las funciones de comparaciÃ³n
3. ğŸ“± MENCIONE botones especÃ­ficos de transformaciÃ³n
4. ğŸ’¡ GUÃE sobre cÃ³mo comparar y decidir

ğŸ”„ FUNCIONALIDADES ESPECÃFICAS DE TRANSFORMACIÃ“N:
- Todo lo del panel normal MÃS:
- "Ver PDF original": muestra el que subiste
- "Ver PDF transformado": muestra el resultado
- ComparaciÃ³n rÃ¡pida: alternar entre ambos
- Misma lÃ³gica: solo vista previa, guardar desde navegador

ğŸ“ FORMATO PARA TRANSFORMACIONES:
- ConfirmaciÃ³n de transformaciÃ³n exitosa
- ExplicaciÃ³n de comparaciÃ³n
- GuÃ­a para decidir prÃ³ximos pasos
- MÃ¡ximo 4-5 lÃ­neas

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_statistics_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para estadÃ­sticas y conteos"""
        row_count = student_data.get("row_count", 0)

        # ğŸ” DEBUG: Analizar datos recibidos del Student
        data = student_data.get("data", [])
        self.logger.info(f"ğŸ” [MASTER-STATS] Analizando datos del Student:")
        self.logger.info(f"    â”œâ”€â”€ row_count: {row_count}")
        self.logger.info(f"    â”œâ”€â”€ data type: {type(data)}")
        self.logger.info(f"    â”œâ”€â”€ data length: {len(data) if isinstance(data, list) else 'N/A'}")
        if isinstance(data, list) and len(data) > 0:
            self.logger.info(f"    â”œâ”€â”€ data[0]: {data[0]}")
            self.logger.info(f"    â””â”€â”€ data[0] keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'N/A'}")

        # ğŸ¯ DETECTAR TIPO DE ESTADÃSTICA BASADO EN ESTRUCTURA DE DATOS
        is_distribution = False
        total_sum = 0

        if data and isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
            # ğŸ¯ DETECTAR DISTRIBUCIONES: mÃºltiples registros con campo de agrupaciÃ³n + cantidad
            distribution_fields = ['grado', 'grupo', 'turno', 'ciclo_escolar']
            has_distribution_field = any(field in data[0] for field in distribution_fields)
            has_cantidad = 'cantidad' in data[0]

            if len(data) > 1 and has_distribution_field and has_cantidad:
                is_distribution = True
                total_sum = sum(item.get('cantidad', 0) for item in data)

                # Detectar tipo de distribuciÃ³n
                distribution_type = next((field for field in distribution_fields if field in data[0]), 'campo')
                self.logger.info(f"ğŸ” [MASTER-STATS] DISTRIBUCIÃ“N detectada: {len(data)} {distribution_type}s, {total_sum} alumnos total")
            else:
                # Conteo simple
                actual_count = data[0].get("total", row_count)
                self.logger.info(f"ğŸ” [MASTER-STATS] CONTEO SIMPLE: {actual_count}")
        else:
            actual_count = row_count
            self.logger.info(f"ğŸ” [MASTER-STATS] usando row_count como actual_count: {actual_count}")

        # Preparar datos para el prompt
        if is_distribution:
            # Para distribuciones, usar datos completos
            distribution_summary = f"{len(data)} grados con {total_sum} alumnos total"
            self.logger.info(f"ğŸ” [MASTER-STATS] Resumen distribuciÃ³n: {distribution_summary}")
        else:
            # Para conteos simples, usar valor individual
            total_alumnos = 211  # Valor conocido de la base de datos
            porcentaje = round((actual_count / total_alumnos) * 100, 1) if total_alumnos > 0 else 0

        if is_distribution:
            # ğŸ¯ DETECTAR TIPO DE DISTRIBUCIÃ“N (SIN INCLUIR DATOS EN EL PROMPT)
            distribution_type = next((field for field in ['grado', 'grupo', 'turno', 'ciclo_escolar'] if field in data[0]), 'campo')

            return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ğŸ«

ğŸ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- TIPO: DISTRIBUCIÃ“N por {distribution_type}s
- RESULTADOS: {len(data)} {distribution_type}s diferentes
- TOTAL ALUMNOS: {total_sum} estudiantes

ğŸ­ TU PERSONALIDAD:
- Entusiasta y humano (usa emojis apropiados)
- Profesional pero cercano
- EmpÃ¡tico y comprensivo
- Celebra los datos interesantes

ğŸ¯ TU TAREA PARA DISTRIBUCIONES:
Generar una respuesta HUMANA y ENTUSIASTA que:

1. ğŸ‰ SALUDA con entusiasmo apropiado
2. ğŸ“Š PRESENTA la distribuciÃ³n de manera atractiva
3. ğŸ”¢ DESTACA datos interesantes (total de estudiantes y categorÃ­as)
4. ğŸ’¡ INVITA a ver los detalles visuales abajo

âœ… PATRONES DE RESPUESTA HUMANA:
- "Â¡Perfecto! ğŸ“Š AquÃ­ tienes la distribuciÃ³n..."
- "Â¡Excelente consulta! ğŸ‘ Te muestro cÃ³mo se distribuyen..."
- "Â¡Genial! ğŸ¯ Los {total_sum} estudiantes se reparten en {len(data)} categorÃ­as..."
- "Â¡QuÃ© buena pregunta! ğŸ¤© AquÃ­ estÃ¡ la informaciÃ³n..."

âŒ EVITA LENGUAJE TÃ‰CNICO:
- "La distribuciÃ³n de alumnos por [campo] nos muestra..."
- "Los datos detallados se presentan a continuaciÃ³n"
- "Tenemos un total de X [categorÃ­as] distintos"

ğŸ“ FORMATO HUMANO Y ENTUSIASTA:
- Saludo entusiasta con emoji apropiado
- PresentaciÃ³n natural de los nÃºmeros clave
- InvitaciÃ³n amigable a explorar los detalles
- MÃ¡ximo 2-3 lÃ­neas con personalidad autÃ©ntica
- Adapta el lenguaje al tipo de distribuciÃ³n automÃ¡ticamente

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""
        else:
            return f"""
Eres el asistente de la escuela especializado en ESTADÃSTICAS Y CONTEOS.

ğŸ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- RESULTADO: {actual_count} alumnos
- PORCENTAJE: {porcentaje}% del total ({total_alumnos} alumnos)
- TIPO: Conteo simple

ğŸ¯ TU TAREA ESPECÃFICA PARA CONTEOS:
Generar una respuesta ENFOCADA EN NÃšMEROS que:

1. ğŸ“Š PRESENTE el resultado claramente
2. ğŸ” CONTEXTUALICE el nÃºmero (quÃ© significa)
3. ğŸ’¡ SUGIERA anÃ¡lisis relacionados
4. ğŸ¯ MANTENGA enfoque en datos

ğŸ“ FORMATO CONCISO:
- Resultado directo
- Contexto breve
- MÃ¡ximo 2-3 lÃ­neas

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_generic_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt genÃ©rico para casos no especÃ­ficos"""
        message = student_data.get("message", "Consulta procesada exitosamente")

        return f"""
Eres el asistente de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ".

ğŸ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- ACCIÃ“N: {action_used}
- MENSAJE: {message}

ğŸ¯ TU TAREA:
Generar una respuesta profesional y Ãºtil basada en la informaciÃ³n disponible.

ğŸ“ FORMATO:
- Respuesta clara y directa
- Tono profesional pero amigable
- MÃ¡ximo 2-3 lÃ­neas

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_help_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para respuestas de ayuda del sistema"""
        help_data = student_data.get("data", {})
        help_type = help_data.get("tipo", "ayuda_general")
        titulo = help_data.get("titulo", "Ayuda del Sistema")
        contenido = help_data.get("contenido", {})

        # ğŸ¯ EXTRAER INFORMACIÃ“N ESPECÃFICA DEL HELPINTERPRETER
        info_especifica = ""
        if help_type == "capacidades_sistema":
            # ğŸ¯ EXTRAER TODAS LAS CAPACIDADES DETALLADAS
            busquedas_apellido = contenido.get("busquedas_por_apellido", {})
            busquedas_nombre = contenido.get("busquedas_por_nombre_completo", {})
            busquedas_criterios = contenido.get("busquedas_por_criterios_academicos", {})
            constancias = contenido.get("constancias_pdf_completas", {})
            estadisticas = contenido.get("estadisticas_y_conteos", {})
            continuaciones = contenido.get("continuaciones_contextuales", {})
            filtros_calif = contenido.get("filtros_de_calificaciones", {})

            info_especifica = f"""
ğŸ“Š CAPACIDADES COMPLETAS DEL SISTEMA (PROBADAS):

ğŸ” **BÃšSQUEDAS POR APELLIDO**: {busquedas_apellido.get('descripcion', '')}
  Ejemplos: {', '.join(busquedas_apellido.get('ejemplos_reales', [])[:2])}

ğŸ‘¤ **BÃšSQUEDAS POR NOMBRE COMPLETO**: {busquedas_nombre.get('descripcion', '')}
  Ejemplos: {', '.join(busquedas_nombre.get('ejemplos_reales', [])[:2])}

ğŸ“ **BÃšSQUEDAS POR CRITERIOS ACADÃ‰MICOS**: {busquedas_criterios.get('descripcion', '')}
  Ejemplos: {', '.join(busquedas_criterios.get('ejemplos_reales', [])[:2])}

ğŸ“„ **CONSTANCIAS PDF**: {constancias.get('descripcion', '')}
  Ejemplos: {', '.join(constancias.get('ejemplos_reales', [])[:2])}

ğŸ“Š **ESTADÃSTICAS Y CONTEOS**: {estadisticas.get('descripcion', '')}
  Ejemplos: {', '.join(estadisticas.get('ejemplos_reales', [])[:2])}

ğŸ”„ **CONTINUACIONES CONTEXTUALES**: {continuaciones.get('descripcion', '')}
  Ejemplo: {continuaciones.get('ejemplos_reales', [''])[0] if continuaciones.get('ejemplos_reales') else ''}

ğŸ“ **FILTROS DE CALIFICACIONES**: {filtros_calif.get('descripcion', '')}
  Ejemplos: {', '.join(filtros_calif.get('ejemplos_reales', [])[:2])}
"""

        elif help_type == "tutorial_uso":
            pasos = contenido.get("pasos", [])
            consejos = contenido.get("consejos", [])

            pasos_info = ""
            for paso in pasos[:4]:  # MÃ¡ximo 4 pasos
                titulo = paso.get("titulo", "")
                descripcion = paso.get("descripcion", "")
                ejemplos = paso.get("ejemplos_reales", [])
                pasos_info += f"- {titulo}: {descripcion}\n  Ejemplos: {', '.join(ejemplos[:2])}\n"

            info_especifica = f"""
ğŸ“š TUTORIAL PASO A PASO - CASOS REALES PROBADOS:
{pasos_info}
ğŸ’¡ CONSEJOS IMPORTANTES:
{chr(10).join(consejos[:3])}
"""

        # ğŸ—‘ï¸ TIPOS ELIMINADOS - SOLO MANTENEMOS CAPACIDADES Y TUTORIAL

        return f"""
Eres el asistente amigable y experto de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ğŸ«

ğŸ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- TIPO DE AYUDA: {help_type}
- ACCIÃ“N: {action_used}
- TÃTULO: {titulo}

{info_especifica}

ğŸ­ TU TAREA ESPECÃFICA:
Generar una respuesta ÃšTIL Y ESPECÃFICA que:

1. ğŸ‘‹ SALUDA de manera apropiada
2. ğŸ“š EXPLICA la informaciÃ³n REAL con ejemplos especÃ­ficos
3. ğŸ’¡ USA ÃšNICAMENTE los ejemplos concretos proporcionados arriba
4. ğŸ¯ INVITA a probar con ejemplos especÃ­ficos

âœ… PATRONES DE RESPUESTA SEGÃšN TIPO (SIMPLIFICADO):
**Para capacidades_sistema:**
- "Â¡Hola! ğŸ‘‹ Â¡Perfecto! Te explico quÃ© puedo hacer..."
- "Â¡Excelente pregunta! ğŸ¤” Estas son mis capacidades principales..."

**Para tutorial_uso:**
- "Â¡Hola! ğŸ‘‹ Â¡Perfecto! Te explico cÃ³mo usar el sistema paso a paso..."
- "Â¡Excelente! ğŸ¤” AquÃ­ tienes un tutorial con casos reales probados..."

ğŸ“ FORMATO ESPECÃFICO Y OBLIGATORIO:
- Saludo entusiasta apropiado
- ENUMERA TODAS las capacidades de la informaciÃ³n especÃ­fica arriba
- INCLUYE AL MENOS 1 EJEMPLO de cada tipo de bÃºsqueda/funcionalidad
- MENCIONA los tipos especÃ­ficos: apellido, nombre completo, criterios acadÃ©micos, etc.
- USA los nombres reales de los ejemplos (MARTINEZ TORRES, SOPHIA ROMERO GARCIA, etc.)
- Menciona que son casos PROBADOS y validados
- InvitaciÃ³n a probar con ejemplos concretos especÃ­ficos
- Tono conversacional y humano
- INCLUYE SALTOS DE LÃNEA entre cada capacidad para mejor legibilidad
- MÃ¡ximo 8-10 lÃ­neas para incluir TODOS los tipos

ğŸš¨ OBLIGATORIO - INCLUYE TODOS LOS TIPOS CON SALTOS DE LÃNEA:
- BÃšSQUEDAS POR APELLIDO: Al menos 1 ejemplo + SALTO DE LÃNEA
- BÃšSQUEDAS POR NOMBRE COMPLETO: Al menos 1 ejemplo + SALTO DE LÃNEA
- BÃšSQUEDAS POR CRITERIOS ACADÃ‰MICOS: Al menos 1 ejemplo + SALTO DE LÃNEA
- CONSTANCIAS PDF: Al menos 1 ejemplo + SALTO DE LÃNEA
- ESTADÃSTICAS: Al menos 1 ejemplo + SALTO DE LÃNEA
- CONTINUACIONES: Al menos 1 ejemplo + SALTO DE LÃNEA
- FILTROS DE CALIFICACIONES: Al menos 1 ejemplo + SALTO DE LÃNEA
- FORMATEA con nÃºmeros (1., 2., 3., etc.) y SALTO DE LÃNEA despuÃ©s de cada punto

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _detect_user_interaction_needed(self, action_used: str, student_data: dict) -> bool:
        """
        ğŸ¯ DETECTA SI EL STUDENT INDICA QUE NECESITA INTERACCIÃ“N CON EL USUARIO

        Analiza la respuesta del Student para determinar si requiere:
        - Aclaraciones
        - Confirmaciones
        - Selecciones
        - Especificaciones adicionales
        """
        # 1. ACCIONES QUE EXPLÃCITAMENTE REQUIEREN INTERACCIÃ“N
        interactive_actions = [
            "constancia_requiere_aclaracion",
            "seleccion_requerida",
            "confirmacion_requerida",
            "especificacion_requerida"
        ]

        if action_used in interactive_actions:
            return True

        # 2. VERIFICAR SI EL STUDENT INDICA ESPERA DE CONTINUACIÃ“N
        reflexion = student_data.get("reflexion_conversacional", {})
        if reflexion.get("espera_continuacion", False):
            continuation_type = reflexion.get("tipo_esperado", "")
            if continuation_type in ["confirmation", "specification", "selection"]:
                return True

        # 3. VERIFICAR PATRONES EN EL MENSAJE QUE INDICAN PREGUNTA
        message = student_data.get("human_response", "")
        question_patterns = [
            "Â¿cuÃ¡l necesitas?",
            "Â¿te refieres a",
            "necesito que especifiques",
            "Â¿quÃ© tipo de",
            "Â¿confirmas que",
            "Â¿estÃ¡s seguro"
        ]

        for pattern in question_patterns:
            if pattern.lower() in message.lower():
                return True

        return False

    def get_available_modules(self) -> dict:
        """Retorna informaciÃ³n sobre los mÃ³dulos disponibles"""
        return {
            "consulta_alumnos": {
                "disponible": True,
                "descripcion": "Consultas sobre informaciÃ³n de estudiantes",
                "ejemplos": ["cuÃ¡ntos alumnos hay", "buscar a Juan", "alumnos de 3er grado"]
            },
            "transformacion_pdf": {
                "disponible": True,
                "descripcion": "TransformaciÃ³n de constancias entre formatos",
                "ejemplos": ["convertir constancia", "cambiar formato PDF", "transformar a estudios"]
            },
            "ayuda_sistema": {
                "disponible": True,
                "descripcion": "Ayuda sobre el uso del sistema",
                "ejemplos": ["cÃ³mo usar el sistema", "quÃ© puedo hacer", "ayuda con consultas"]
            },
            "conversacion_general": {
                "disponible": False,
                "descripcion": "Chat general y conversaciÃ³n casual",
                "ejemplos": ["hola", "Â¿cÃ³mo estÃ¡s?"]
            }
        }

    # ğŸ§  MÃ‰TODOS DE CONOCIMIENTO PROFUNDO (FASE 2)

    def _validate_feasibility_with_knowledge(self, intention, user_query: str) -> dict:
        """
        ğŸ§  VALIDAR FACTIBILIDAD CON CONOCIMIENTO PROFUNDO
        Usa MasterKnowledge para evaluar si la consulta es factible
        """
        try:
            query_details = {"original_query": user_query}

            feasibility = self.knowledge.can_handle_request(
                intention.intention_type,
                intention.sub_intention,
                query_details
            )

            if feasibility["can_handle"]:
                self.logger.info(f"âœ… [MASTER-KNOWLEDGE] Consulta factible: {feasibility['explanation']}")
                if feasibility["limitations"]:
                    self.logger.info(f"âš ï¸ [MASTER-KNOWLEDGE] Limitaciones: {feasibility['limitations']}")
            else:
                self.logger.warning(f"âŒ [MASTER-KNOWLEDGE] Consulta no factible: {feasibility['explanation']}")
                self.logger.info(f"ğŸ’¡ [MASTER-KNOWLEDGE] Alternativas: {feasibility['alternatives']}")

            return feasibility

        except Exception as e:
            self.logger.error(f"Error validando factibilidad: {e}")
            # Fallback: asumir que es factible
            return {
                "can_handle": True,
                "confidence": 0.5,
                "limitations": [],
                "alternatives": [],
                "explanation": "ValidaciÃ³n de factibilidad fallÃ³, procediendo con precauciÃ³n"
            }

    def _create_limitation_response(self, feasibility: dict, user_query: str) -> 'InterpretationResult':
        """
        ğŸ’¡ CREAR RESPUESTA INTELIGENTE CUANDO ALGO NO ES FACTIBLE
        """
        try:
            from app.core.ai.interpretation.base_interpreter import InterpretationResult

            explanation = feasibility.get("explanation", "Esta funcionalidad no estÃ¡ disponible")
            alternatives = feasibility.get("alternatives", [])

            # Crear respuesta empÃ¡tica con alternativas
            response_parts = [
                f"ğŸ¤” {explanation}.",
                "",
                "ğŸ’¡ **Pero puedo ayudarte con estas alternativas:**"
            ]

            for i, alternative in enumerate(alternatives, 1):
                response_parts.append(f"{i}. {alternative}")

            if not alternatives:
                response_parts.extend([
                    "",
                    "ğŸ“‹ **Capacidades disponibles:**",
                    "â€¢ Buscar informaciÃ³n de alumnos",
                    "â€¢ Generar estadÃ­sticas bÃ¡sicas",
                    "â€¢ Crear constancias oficiales",
                    "â€¢ Transformar documentos PDF"
                ])

            response_parts.append("\nÂ¿Te gustarÃ­a probar alguna de estas opciones? ğŸ˜Š")

            response_text = "\n".join(response_parts)

            self.logger.info(f"ğŸ’¡ [MASTER-KNOWLEDGE] Respuesta de limitaciÃ³n generada")

            return InterpretationResult(
                action="limitation_explanation",
                parameters={
                    "response": response_text,
                    "explanation": explanation,
                    "alternatives": alternatives,
                    "user_query": user_query,
                    "human_response": response_text,
                    "success": True
                },
                confidence=1.0,
                reasoning=f"Consulta no factible: {explanation}"
            )

        except Exception as e:
            self.logger.error(f"Error creando respuesta de limitaciÃ³n: {e}")
            return None

    def _debug_pause(self, title: str, data: dict):
        """MÃ©todo de debug para mostrar informaciÃ³n en --debug-pauses"""
        import os
        if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
            print(f"\nğŸ›‘ {title}")
            for key, value in data.items():
                if isinstance(value, list) and len(value) > 3:
                    print(f"    â”œâ”€â”€ {key}: {value[:3]}... ({len(value)} total)")
                elif isinstance(value, str) and len(value) > 50:
                    print(f"    â”œâ”€â”€ {key}: {value[:50]}...")
                else:
                    print(f"    â”œâ”€â”€ {key}: {value}")
            print(f"    â””â”€â”€ Presiona ENTER para continuar...")
            input()

    def _interpret_student_report_intelligently(self, student_data: dict, original_query: str) -> str:
        """
        ğŸ” INTERPRETAR REPORTES DEL STUDENT CON CONOCIMIENTO PROFUNDO
        Usa MasterKnowledge para analizar quÃ© pasÃ³ y sugerir mejoras
        """
        try:
            interpretation = self.knowledge.interpret_student_report(student_data, original_query)

            user_explanation = interpretation.get("user_explanation", "")
            suggestions = interpretation.get("suggestions", [])

            self.logger.info(f"ğŸ” [MASTER-KNOWLEDGE] InterpretaciÃ³n: {interpretation.get('interpretation', '')}")

            if suggestions:
                self.logger.info(f"ğŸ’¡ [MASTER-KNOWLEDGE] Sugerencias: {suggestions}")

            return user_explanation

        except Exception as e:
            self.logger.error(f"Error interpretando reporte del Student: {e}")
            return "OperaciÃ³n completada."
