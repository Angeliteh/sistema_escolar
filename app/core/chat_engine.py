# app/core/chat_engine.py
"""
Motor de Chat Independiente de la Interfaz
Maneja toda la lÃ³gica de procesamiento sin depender de UI especÃ­fica
"""

import os
import json
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime

from app.core.service_provider import ServiceProvider

from app.ui.ai_chat.gemini_client import GeminiClient
from app.ui.ai_chat.message_processor import MessageProcessor
from app.core.logging import get_logger

# ðŸ†• IMPORTAR NUEVO SISTEMA DE ORQUESTACIÃ“N (OPCIONAL)
try:
    from app.core.ai.orchestration.master_orchestrator import MasterOrchestrator
    ORCHESTRATOR_AVAILABLE = True
except ImportError:
    ORCHESTRATOR_AVAILABLE = False

class ChatResponse:
    """Respuesta estructurada del chat"""

    def __init__(self,
                 text: str = "",
                 success: bool = True,
                 action: Optional[str] = None,
                 data: Optional[Dict] = None,
                 files: Optional[List[str]] = None,
                 requires_confirmation: bool = False):
        self.text = text
        self.success = success
        self.action = action  # "open_file", "show_data", "generate_pdf", etc.
        self.data = data or {}
        self.files = files or []
        self.requires_confirmation = requires_confirmation
        self.timestamp = datetime.now()

    def to_dict(self) -> Dict[str, Any]:
        """Convierte a diccionario para serializaciÃ³n"""
        return {
            "text": self.text,
            "success": self.success,
            "action": self.action,
            "data": self.data,
            "files": self.files,
            "requires_confirmation": self.requires_confirmation,
            "timestamp": self.timestamp.isoformat()
        }

class ChatEngine:
    """Motor de chat independiente de la interfaz"""

    def __init__(self,
                 file_handler: Optional[Callable] = None,
                 confirmation_handler: Optional[Callable] = None,
                 pdf_panel = None,
                 use_orchestrator: bool = False):
        """
        Args:
            file_handler: FunciÃ³n para manejar archivos (abrir PDFs, etc.)
            confirmation_handler: FunciÃ³n para manejar confirmaciones
            pdf_panel: Panel de PDF para transformaciones
            use_orchestrator: Si usar el nuevo sistema de orquestaciÃ³n (experimental)
        """
        self.logger = get_logger(__name__)
        self.service_provider = ServiceProvider.get_instance()
        self.gemini_client = GeminiClient()

        # ðŸ†• GUARDAR PDF_PANEL COMO ATRIBUTO PARA ACCESO EN WORKER THREADS
        self.pdf_panel = pdf_panel

        # ðŸ†• SISTEMA DE ORQUESTACIÃ“N OPCIONAL
        self.use_orchestrator = use_orchestrator and ORCHESTRATOR_AVAILABLE
        if self.use_orchestrator:
            self.master_orchestrator = MasterOrchestrator()
            self.logger.info("ðŸŽ¯ MasterOrchestrator habilitado")
        else:
            self.master_orchestrator = None

        self.message_processor = MessageProcessor(self.gemini_client, pdf_panel)

        # Handlers opcionales para diferentes interfaces
        self.file_handler = file_handler or self._default_file_handler
        self.confirmation_handler = confirmation_handler or self._default_confirmation_handler

        # Estado del chat (historial manejado por MessageProcessor)
        self.context = {}

        orchestrator_status = "con MasterOrchestrator" if self.use_orchestrator else "sistema tradicional"
        self.logger.info(f"ChatEngine inicializado ({orchestrator_status})")

    def process_message(self, message: str, user_context: Optional[Dict] = None) -> ChatResponse:
        """
        Procesa un mensaje y devuelve una respuesta estructurada

        Args:
            message: Mensaje del usuario
            user_context: Contexto adicional del usuario

        Returns:
            ChatResponse con la respuesta procesada
        """
        try:
            self.logger.info(f"ðŸŽ¯ [CHATENGINE] Procesando: '{message[:50]}...'")

            # Actualizar contexto
            if user_context:
                self.context.update(user_context)

            # Procesar con IA (historial manejado por MessageProcessor)
            response = self._process_with_ai(message)

            return response

        except Exception as e:
            self.logger.error(f"Error procesando mensaje: {str(e)}")
            return ChatResponse(
                text=f"âŒ Error procesando mensaje: {str(e)}",
                success=False
            )

    def _process_with_ai(self, message: str) -> ChatResponse:
        """Procesa el mensaje con el servicio de IA usando GeminiClient centralizado"""
        try:
            # ðŸŽ¯ NUEVO: USAR ORCHESTRATOR SI ESTÃ HABILITADO
            if self.use_orchestrator and self.master_orchestrator:
                return self._process_with_orchestrator(message)

            # ðŸ”„ FLUJO TRADICIONAL: Sin create_prompt, directo a MasterInterpreter
            # Crear comando directo para el MessageProcessor
            command_data = {
                "accion": "consulta_directa",
                "parametros": {"consulta_original": message}
            }

            # 4. Procesar comando
            success, response_text, data = self.message_processor.process_command(
                command_data,
                None,  # current_pdf
                message,  # original_query
                self.context  # conversation_context
            )

            # 5. Analizar respuesta para determinar acciones
            return self._analyze_ai_response(response_text, data, success)

        except Exception as e:
            self.logger.error(f"Error en procesamiento IA: {str(e)}")
            return ChatResponse(
                text=f"âŒ Error en el servicio de IA: {str(e)}",
                success=False
            )

    def _process_with_orchestrator(self, message: str) -> ChatResponse:
        """ðŸŽ¯ NUEVO: Procesa con MasterOrchestrator"""
        try:
            self.logger.info(f"ðŸŽ¯ [ORCHESTRATOR] Procesando: {message[:50]}...")

            # TODO: Crear contexto apropiado para el orchestrator
            # Por ahora, usar un contexto bÃ¡sico
            from app.core.ai.interpretation.base_interpreter import InterpretationContext

            context = InterpretationContext(
                user_message=message,
                conversation_stack=getattr(self.message_processor, 'conversation_stack', []),
                additional_context=self.context
            )

            # Procesar con orchestrator
            result = self.master_orchestrator.process_query(message, context)

            # Convertir resultado a ChatResponse
            return ChatResponse(
                text=result.get('message', result.get('text', 'Procesado por orchestrator')),
                success=result.get('success', True),
                action=result.get('action'),
                data=result.get('data', {}),
                files=result.get('files', [])
            )

        except Exception as e:
            self.logger.error(f"âŒ [ORCHESTRATOR] Error: {e}")
            # Fallback al sistema tradicional
            self.logger.info("ðŸ”„ Fallback al sistema tradicional")
            return self._process_with_traditional_system(message)

    def _process_with_traditional_system(self, message: str) -> ChatResponse:
        """ðŸ”„ Sistema tradicional como fallback"""
        command_data = {
            "accion": "consulta_directa",
            "parametros": {"consulta_original": message}
        }

        success, response_text, data = self.message_processor.process_command(
            command_data,
            None,  # current_pdf
            message,  # original_query
            self.context  # conversation_context
        )

        return self._analyze_ai_response(response_text, data, success)

    def _analyze_ai_response(self, ai_response: str, command_data: dict, command_success: bool) -> ChatResponse:
        """Analiza la respuesta de IA para determinar acciones necesarias"""

        # Validar que ai_response no sea None
        if ai_response is None:
            self.logger.error("Error determinando tipo de respuesta: ai_response es None")
            ai_response = "Error procesando respuesta"

        # Detectar si se generÃ³ un archivo
        generated_files = []
        action = None

        # Buscar patrones de archivos generados en logs y respuesta

        # Buscar en la respuesta de IA
        if "PDF generado:" in ai_response or "Constancia generada:" in ai_response or "archivo:" in ai_response.lower():
            lines = ai_response.split('\n')
            for line in lines:
                if "PDF generado:" in line or "archivo:" in line.lower() or "constancia" in line.lower():
                    parts = line.split(':')
                    if len(parts) > 1:
                        file_path = parts[1].strip()
                        if os.path.exists(file_path):
                            generated_files.append(file_path)
                            action = "open_file"

        # ðŸŽ¯ DETECCIÃ“N INTELIGENTE DE PDF - SOLO PARA CONSTANCIAS GENERADAS
        # Solo buscar archivos PDF si command_data indica que se generÃ³ una constancia
        if (command_data and isinstance(command_data, dict) and
            (command_data.get('action') == 'constancia_preview' or
             'constancia' in str(command_data.get('message', '')).lower() or
             'ruta_archivo' in command_data)):

            import tempfile
            import time

            temp_dir = tempfile.gettempdir()
            current_time = time.time()

            # Buscar archivos PDF recientes SOLO cuando se generÃ³ una constancia
            for root, _, files in os.walk(temp_dir):
                for file in files:
                    if file.endswith('.pdf') and 'constancia' in file.lower():
                        file_path = os.path.join(root, file)
                        try:
                            # Verificar si el archivo es muy reciente (Ãºltimos 5 segundos)
                            file_time = os.path.getmtime(file_path)
                            if current_time - file_time < 5:
                                if file_path not in generated_files:
                                    generated_files.append(file_path)
                                    action = "open_file"
                                    self.logger.info(f"Archivo PDF de constancia detectado: {file_path}")
                        except:
                            pass

        # ðŸ†• DETECTAR CONSTANCIAS EN COMMAND_DATA
        # (La integraciÃ³n completa con interpretadores se harÃ¡ despuÃ©s)

        # Detectar archivos en command_data (mÃ©todo original)
        if command_data and isinstance(command_data, dict):
            # Buscar archivos en diferentes campos
            file_fields = ["archivo_generado", "ruta_archivo", "file_path"]
            for field in file_fields:
                if field in command_data:
                    file_path = command_data[field]
                    if file_path and os.path.exists(file_path):
                        generated_files.append(file_path)

                        # ðŸŽ¯ DETECTAR TIPO DE ACCIÃ“N SEGÃšN CONTEXTO
                        if "constancia" in file_path.lower() or "constancia" in ai_response.lower():
                            if "alumno" in command_data:
                                action = "constancia_preview"  # Vista previa de constancia generada
                            else:
                                action = "pdf_transformation"  # TransformaciÃ³n de PDF
                        else:
                            action = "open_file"

        # Detectar si necesita confirmaciÃ³n
        requires_confirmation = any(phrase in ai_response.lower() for phrase in [
            "Â¿confirmas?", "Â¿proceder?", "Â¿continuar?", "Â¿estÃ¡s seguro?"
        ])

        # Detectar datos estructurados (listas, tablas)
        data = command_data if command_data else {}

        # ðŸ”§ PRIORIZAR ACTION DE COMMAND_DATA (configurado por MessageProcessor)
        if command_data and isinstance(command_data, dict) and "action" in command_data:
            action = action or command_data["action"]
            self.logger.info(f"ðŸ”§ Usando action de command_data: {action}")

        # Fallback: detectar por contenido de respuesta
        if not action and ("ðŸ“Š" in ai_response or "ðŸ“‹" in ai_response):
            action = "show_data"

        # ðŸ”§ MENSAJE CONSOLIDADO Y LIMPIO para archivos generados
        final_text = ai_response
        if generated_files:
            # Usar el mensaje del ConstanciaProcessor si estÃ¡ disponible
            if hasattr(data, 'get') and data.get('message'):
                final_text = data['message']
            else:
                final_text = f"âœ… Constancia generada exitosamente. La vista previa estÃ¡ disponible en el panel derecho."

            if not command_success:
                # Si el comando fallÃ³ pero encontramos archivos, es Ã©xito
                command_success = True

        # ðŸŽ¯ CONSTANCIAS COMO ACCIONES COMPLETAS (NO REQUIEREN CONFIRMACIÃ“N)
        if generated_files and any(f.lower().endswith('.pdf') for f in generated_files):
            requires_confirmation = False  # Las constancias no requieren confirmaciÃ³n
            action = "constancia_generated"  # AcciÃ³n especÃ­fica para constancias

        return ChatResponse(
            text=final_text,
            success=command_success,
            action=action,
            files=generated_files,
            requires_confirmation=requires_confirmation,
            data=data
        )

    def _default_file_handler(self, file_path: str) -> bool:
        """Handler por defecto para archivos (no hace nada)"""
        self.logger.info(f"Archivo generado: {file_path}")
        return True

    def _default_confirmation_handler(self, message: str) -> bool:
        """Handler por defecto para confirmaciones (siempre sÃ­)"""
        self.logger.info(f"ConfirmaciÃ³n requerida: {message}")
        return True

    def get_conversation_history(self) -> List[Dict]:
        """Obtiene el historial de conversaciÃ³n desde MessageProcessor"""
        return self.message_processor.conversation_history.copy()

    def clear_history(self):
        """Limpia el historial de conversaciÃ³n"""
        self.message_processor.conversation_history.clear()
        self.message_processor.conversation_stack.clear()
        self.context.clear()
        self.logger.info("Historial de conversaciÃ³n limpiado")

    def export_conversation(self, file_path: str) -> bool:
        """Exporta la conversaciÃ³n a un archivo JSON"""
        try:
            export_data = {
                "conversation": self.message_processor.conversation_history,
                "conversation_stack": self.message_processor.conversation_stack,
                "context": self.context,
                "exported_at": datetime.now().isoformat()
            }

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)

            self.logger.info(f"ConversaciÃ³n exportada a: {file_path}")
            return True

        except Exception as e:
            self.logger.error(f"Error exportando conversaciÃ³n: {str(e)}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """Obtiene estadÃ­sticas del chat"""
        conversation_history = self.message_processor.conversation_history
        return {
            "total_messages": len(conversation_history),
            "user_messages": len([m for m in conversation_history if m["role"] == "user"]),
            "assistant_messages": len([m for m in conversation_history if m["role"] in ["assistant", "system"]]),
            "context_size": len(self.context),
            "conversation_stack_size": len(self.message_processor.conversation_stack),
            "session_start": conversation_history[0]["timestamp"] if conversation_history else None
        }


