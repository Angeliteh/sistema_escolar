"""
IntÃ©rprete maestro - Coordina todos los mÃ³dulos de interpretaciÃ³n
"""
import os
from typing import Optional, Dict, Any
from dataclasses import dataclass
from app.core.ai.interpretation.base_interpreter import InterpretationContext, InterpretationResult
from app.core.ai.interpretation.student_query_interpreter import StudentQueryInterpreter
from app.core.ai.interpretation.master_knowledge import MasterKnowledge
from app.core.logging import get_logger
from app.core.config import Config

@dataclass
class IntentionResult:
    """Resultado de la detecciÃ³n de intenciÃ³n (LOCAL - reemplaza intention_detector obsoleto)"""
    intention_type: str  # "consulta_alumnos", "ayuda_sistema", "conversacion_general"
    sub_intention: str   # "busqueda_simple", "generar_constancia", etc.
    confidence: float
    reasoning: str
    detected_entities: Dict[str, Any]
    categoria: str = ""           # busqueda|estadistica|reporte|constancia|transformacion|continuacion
    sub_tipo: str = ""            # simple|complejo|listado|conteo|generacion|conversion
    complejidad: str = ""         # baja|media|alta
    requiere_contexto: bool = False
    flujo_optimo: str = ""        # sql_directo|analisis_datos|listado_completo

class MasterInterpreter:
    """
    ðŸŽ¯ INTÃ‰RPRETE MAESTRO - LÃDER INTELIGENTE DEL SISTEMA

    RESPONSABILIDADES:
    - Detectar intenciones con contexto estratÃ©gico completo
    - Dirigir al especialista correcto con informaciÃ³n precisa
    - Mantener memoria de interacciones para retroalimentaciÃ³n
    - ComunicaciÃ³n bidireccional con especialistas
    """

    def __init__(self, gemini_client):
        self.gemini_client = gemini_client
        self.logger = get_logger(__name__)

        # ðŸ§  INICIALIZAR CEREBRO DEL MASTER (CONOCIMIENTO PROFUNDO)
        self.knowledge = MasterKnowledge()
        self.logger.info("ðŸ§  [MASTER] Cerebro inicializado con conocimiento profundo del sistema")

        # ðŸŽ¯ INICIALIZAR PROMPT MANAGER PARA ROUTING FORZADO
        from app.core.ai.prompts.master_prompt_manager import MasterPromptManager
        self.prompt_manager = MasterPromptManager()
        self.logger.info("ðŸŽ¯ [MASTER] PromptManager inicializado para routing forzado")

        # ðŸŽ¯ CONTEXTO ESTRATÃ‰GICO DEL SISTEMA (SEGÃšN INTENCIONES_ACCIONES_DEFINITIVAS.md)
        self.system_map = {
            "StudentQueryInterpreter": {
                "handles": ["consulta_alumnos"],  # âœ… Solo intenciÃ³n principal
                "sub_intentions": ["busqueda_simple", "busqueda_compleja", "estadisticas", "generar_constancia", "transformacion_pdf"],
                "capabilities": "Consultas de BD, documentos, anÃ¡lisis de alumnos",
                "description": "Especialista en datos de alumnos y generaciÃ³n de documentos"
            },
            "HelpInterpreter": {
                "handles": ["ayuda_sistema"],
                "sub_intentions": [
                    "explicacion_general", "tutorial_funciones", "sobre_creador",
                    "auto_consciencia", "ventajas_sistema", "casos_uso_avanzados",
                    "limitaciones_honestas", "pregunta_capacidades", "pregunta_tecnica"
                ],
                "capabilities": "Asistente de IA consciente, persuasivo y experto en el sistema",
                "description": "Especialista en explicar el sistema con personalidad y conocimiento sobre Angel"
            },
            "MasterInterpreter": {
                "handles": ["aclaracion_requerida"],
                "sub_intentions": ["multiple_interpretations", "incomplete_query", "ambiguous_reference"],
                "capabilities": "DetecciÃ³n de ambigÃ¼edades y comunicaciÃ³n directa con usuario",
                "description": "Master se delega a sÃ­ mismo para consultas ambiguas"
            },
            "GeneralInterpreter": {
                "handles": ["conversacion_general"],
                "sub_intentions": ["saludo", "chat_casual", "despedida", "redireccion_educada"],
                "capabilities": "ConversaciÃ³n natural, saludos, temas no escolares",
                "description": "Especialista en conversaciÃ³n general con identidad escolar sutil"
            }
        }

        # ðŸ’­ MEMORIA DE INTERACCIONES (RETROALIMENTACIÃ“N)
        self.interaction_memory = {
            "last_specialist": None,
            "last_result_summary": None,
            "conversation_flow": None,
            "specialist_feedback": None,
            "awaiting_continuation": False,
            "continuation_type": None
        }

        # ðŸ”§ COMPONENTES ELIMINADOS: IntentionDetector (ahora todo es unificado en _analyze_and_delegate_intelligently)

        # ðŸŽ¯ LOGS DE DEPURACIÃ“N FORZADOS - CONTEXTO ESTRATÃ‰GICO COMPLETO
        self.logger.info("ðŸŽ¯ [MASTER] INICIALIZADO CON CONTEXTO ESTRATÃ‰GICO")
        self.logger.info(f"   â”œâ”€â”€ Especialistas disponibles: {len(self.system_map)}")
        self.logger.info(f"   â”œâ”€â”€ StudentQueryInterpreter: {self.system_map['StudentQueryInterpreter']['capabilities']}")
        self.logger.info(f"   â”œâ”€â”€ HelpInterpreter: {self.system_map['HelpInterpreter']['capabilities']}")
        self.logger.info(f"   â””â”€â”€ GeneralInterpreter: {self.system_map['GeneralInterpreter']['capabilities']}")

        # ðŸ§  [MASTER] Contexto estratÃ©gico inicializado
        self._log_strategic_context()

        # ðŸŽ¯ INICIALIZAR ESPECIALISTAS (DESPUÃ‰S DE MOSTRAR CONTEXTO MASTER)
        self.logger.info("ðŸŽ¯ [MASTER] Inicializando especialistas...")
        from app.core.config import Config
        db_path = Config.DB_PATH

        # ðŸŽ¯ INICIALIZAR SCHOOL CONFIG MANAGER CON BD PARA AUTO-DETECCIÃ“N
        from app.core.config.school_config_manager import get_school_config_manager
        school_config = get_school_config_manager(db_path=db_path)
        self.logger.info(f"ðŸ« [MASTER] ConfiguraciÃ³n escolar: {school_config.get_school_name()} ({school_config.get_total_students()} alumnos)")

        self.student_interpreter = StudentQueryInterpreter(db_path, gemini_client)

        from app.core.ai.interpretation.help_interpreter import HelpInterpreter
        self.help_interpreter = HelpInterpreter(gemini_client)

        from app.core.ai.interpretation.general_interpreter import GeneralInterpreter
        self.general_interpreter = GeneralInterpreter(gemini_client)

        self.logger.info("âœ… [MASTER] Especialistas inicializados correctamente (Student, Help, General)")

    def interpret(self, context: InterpretationContext, conversation_stack=None, current_pdf=None) -> Optional[InterpretationResult]:
        """
        ðŸŽ¯ INTERPRETACIÃ“N MAESTRO CON CONTEXTO ESTRATÃ‰GICO COMPLETO

        FLUJO MEJORADO:
        1. AnÃ¡lisis con contexto estratÃ©gico completo
        2. DetecciÃ³n de intenciÃ³n con memoria de interacciones
        3. DelegaciÃ³n inteligente al especialista correcto
        4. ComunicaciÃ³n bidireccional y retroalimentaciÃ³n
        """
        try:
            # ðŸŽ¯ LOGS DE DEPURACIÃ“N FORZADOS
            self.logger.info("ðŸŽ¯ [MASTER] INICIANDO INTERPRETACIÃ“N CON CONTEXTO ESTRATÃ‰GICO")
            self.logger.info(f"   â”œâ”€â”€ Consulta: '{context.user_message}'")
            self.logger.info(f"   â”œâ”€â”€ Conversation_stack: {len(conversation_stack) if conversation_stack else 0} niveles")
            self.logger.info(f"   â””â”€â”€ Memoria anterior: {self.interaction_memory}")

            # ðŸŽ¯ ALMACENAR CONVERSATION_STACK PARA USO EN RESPUESTA FINAL
            self.current_conversation_stack = conversation_stack or []

            # ðŸŽ¯ PROCESAMIENTO CON CONTEXTO CONVERSACIONAL ACTIVADO
            context.conversation_stack = conversation_stack or []
            if context.conversation_stack:
                self.logger.info(f"ðŸŽ¯ [MASTER] Procesando con contexto - {len(context.conversation_stack)} niveles disponibles")
            else:
                self.logger.info("ðŸŽ¯ [MASTER] Procesando consulta individual")

            # ðŸ§  ANÃLISIS UNIFICADO MAESTRO - UN SOLO PROMPT PARA TODO
            # Reemplaza: detecciÃ³n de intenciÃ³n + resoluciÃ³n de contexto + anÃ¡lisis
            analysis_result = self._analyze_and_delegate_intelligently(context.user_message, context.conversation_stack)

            if not analysis_result:
                self.logger.error("âŒ [MASTER] Error en anÃ¡lisis unificado")
                return None

            # Convertir anÃ¡lisis unificado a IntentionResult para compatibilidad
            intention = self._convert_analysis_to_intention(analysis_result)

            # ðŸ§  [MASTER] IntenciÃ³n detectada y categorizada
            self.logger.info(f"ðŸ§  [MASTER] Analizando: \"{context.user_message}\" â†’ {intention.intention_type} ({intention.confidence})")

            # ðŸ”§ DEBUG: InformaciÃ³n detallada solo en modo debug
            from app.core.logging import debug_detailed
            debug_detailed(self.logger, f"ðŸ”§ [MASTER] Detalles: {intention.intention_type}/{intention.sub_intention}")
            debug_detailed(self.logger, f"ðŸ”§ [MASTER] CategorÃ­a: {intention.categoria}, Sub-tipo: {intention.sub_tipo}")
            debug_detailed(self.logger, f"ðŸ”§ [MASTER] Complejidad: {intention.complejidad}, Flujo: {intention.flujo_optimo}")

            # PASO 3: VALIDAR INTENCIÃ“N CON SISTEMA MAP
            validated_intention = self._validate_intention_with_system_map(intention)
            if validated_intention != intention:
                self.logger.info(f"ðŸ”§ [MASTER] IntenciÃ³n corregida por system_map")

            # ðŸ§  PASO 2: ANÃLISIS DE CONOCIMIENTO (Â¿PUEDO HACERLO?)
            feasibility = self._validate_feasibility_with_knowledge(validated_intention, context.user_message)

            # Si no es factible, crear respuesta de limitaciÃ³n inmediatamente
            if not feasibility["can_handle"]:
                return self._create_limitation_response(feasibility, context.user_message)

            # ðŸ§  PASO 3: ANÃLISIS DE CONTEXTO (Â¿HAY INFORMACIÃ“N PREVIA RELEVANTE?)
            context_analysis = self._analyze_context_relevance(validated_intention, context.conversation_stack, context.user_message)

            # PASO 4: VERIFICAR SI NECESITA ACLARACIÃ“N
            # ðŸ”§ ARREGLO: Verificar si es InterpretationResult con action aclaracion_requerida
            if hasattr(validated_intention, 'action') and validated_intention.action == "aclaracion_requerida":
                return validated_intention
            elif hasattr(validated_intention, 'intention_type') and validated_intention.intention_type == "aclaracion_requerida":
                return self._handle_ambiguous_query(context, validated_intention)

            # PASO 5: DIRIGIR AL ESPECIALISTA DIRECTAMENTE
            result = self._delegate_to_specialist_direct(context, validated_intention, current_pdf)

            # PASO 5: ANALIZAR RESULTADOS Y DECIDIR SI NECESITA COMUNICACIÃ“N BIDIRECCIONAL
            # ðŸ”§ ARREGLO: Solo si validated_intention no es InterpretationResult
            if (result and hasattr(validated_intention, 'intention_type') and
                self._should_ask_user_about_results(result, context.user_message)):
                return self._handle_results_analysis(context, validated_intention, result)

            # PASO 6: PROCESAR RETROALIMENTACIÃ“N DEL ESPECIALISTA
            # ðŸ”§ ARREGLO: Solo si validated_intention no es InterpretationResult
            if hasattr(validated_intention, 'intention_type'):
                self._process_specialist_feedback(validated_intention, result)

            return result

        except Exception as e:
            self.logger.error(f"âŒ [MASTER] Error en interpretaciÃ³n: {e}")
            return None

    def _convert_analysis_to_intention(self, analysis_result: dict):
        """
        ðŸ”§ CONVERTIR ANÃLISIS UNIFICADO A INTENTIONRESULT
        Mantiene compatibilidad con el resto del sistema
        """
        try:
            # Usar IntentionResult local (reemplaza intention_detector obsoleto)

            # Extraer informaciÃ³n del anÃ¡lisis
            intention_type = analysis_result.get('intention_type', 'consulta_alumnos')
            sub_intention = analysis_result.get('sub_intention', 'busqueda_simple')
            usar_contexto = analysis_result.get('usar_contexto', False)
            alumno_resuelto = analysis_result.get('detected_entities', {}).get('alumno_resuelto')

            # ðŸ”§ NORMALIZAR INTENCIONES: Convertir a minÃºsculas para compatibilidad con system_map
            intention_type = intention_type.lower() if intention_type else 'consulta_alumnos'
            sub_intention = sub_intention.lower() if sub_intention else 'busqueda_simple'

            self.logger.info(f"ðŸ”§ [MASTER] AnÃ¡lisis convertido: {intention_type}/{sub_intention} (contexto: {usar_contexto})")

            # ðŸŽ¯ TRANSFERIR TODAS LAS DETECTED_ENTITIES DEL LLM AL STUDENT
            detected_entities = analysis_result.get('detected_entities', {})

            # Agregar alumno_resuelto si existe (compatibilidad)
            if alumno_resuelto:
                detected_entities['alumno_resuelto'] = alumno_resuelto

            self.logger.info(f"ðŸŽ¯ [MASTER] Entidades transferidas al Student: {list(detected_entities.keys())}")
            if 'limite_resultados' in detected_entities:
                self.logger.info(f"ðŸŽ¯ [MASTER] LÃ­mite detectado: {detected_entities['limite_resultados']}")
            if 'filtros' in detected_entities:
                self.logger.info(f"ðŸŽ¯ [MASTER] Filtros detectados: {detected_entities['filtros']}")

            # Crear IntentionResult compatible
            intention = IntentionResult(
                intention_type=intention_type,
                sub_intention=sub_intention,
                confidence=0.95,  # Alta confianza del anÃ¡lisis unificado
                reasoning=analysis_result.get('reasoning', 'AnÃ¡lisis unificado del Master'),
                detected_entities=detected_entities,
                categoria="",  # No necesario en el flujo unificado
                sub_tipo="",
                complejidad="",
                requiere_contexto=usar_contexto,
                flujo_optimo=""
            )

            self.logger.info(f"ðŸ”§ [MASTER] AnÃ¡lisis convertido: {intention_type}/{sub_intention} (contexto: {usar_contexto})")
            return intention

        except Exception as e:
            self.logger.error(f"âŒ Error convirtiendo anÃ¡lisis a intention: {e}")
            # Fallback bÃ¡sico usando IntentionResult local
            return IntentionResult(
                intention_type='consulta_alumnos',
                sub_intention='busqueda_simple',
                confidence=0.5,
                reasoning='Fallback por error en conversiÃ³n',
                detected_entities={},
                requiere_contexto=False
            )

    # MÃ‰TODO ELIMINADO: _detect_intention_with_context
    # Ahora todo se maneja en _analyze_and_delegate_intelligently (prompt unificado)

    # MÃ‰TODO ELIMINADO: _resolve_contextual_references
    # Ahora todo se maneja directamente en el flujo principal con _analyze_and_delegate_intelligently

    # âœ… MÃ‰TODO ELIMINADO: _force_area_selection() - Reemplazado por _analyze_and_delegate_intelligently()

    # âœ… MÃ‰TODO ELIMINADO: _fallback_area_detection() - No se usa en flujo actual

    # âœ… MÃ‰TODO ELIMINADO: _delegate_to_general() - No se usa en flujo actual

    # âœ… MÃ‰TODO ELIMINADO: _convert_routing_to_student_format() - No se usa en flujo actual

    # âœ… MÃ‰TODO ELIMINADO: _convert_routing_to_help_format() - No se usa en flujo actual

    # âœ… MÃ‰TODO ELIMINADO: _update_interaction_memory() con routing - Reemplazado por actualizaciÃ³n directa en flujo principal

    def _get_dynamic_intentions_section(self) -> str:
        """
        ðŸŽ¯ GENERAR SECCIÃ“N DINÃMICA DE INTENCIONES

        Usa SystemCatalog para generar la secciÃ³n de intenciones
        que se inyecta en el prompt del Master.
        """
        try:
            from app.core.ai.system_catalog import SystemCatalog
            return SystemCatalog.generate_intentions_section()
        except Exception as e:
            self.logger.error(f"âŒ [MASTER] Error generando secciÃ³n de intenciones: {e}")
            # Fallback a secciÃ³n bÃ¡sica
            return """
ðŸŽ¯ **CONSULTA_ALUMNOS** â†’ StudentQueryInterpreter (Prioridad: 1)
   â”œâ”€â”€ DESCRIPCIÃ“N: TODO sobre alumnos de la escuela (211 alumnos activos)
   â”œâ”€â”€ SUB-INTENCIONES: busqueda_simple, estadisticas, generar_constancia

ðŸŽ¯ **AYUDA_SISTEMA** â†’ HelpInterpreter (Prioridad: 2)
   â”œâ”€â”€ DESCRIPCIÃ“N: Ayuda sobre el sistema y capacidades
   â”œâ”€â”€ SUB-INTENCIONES: explicacion_general, sobre_creador, auto_consciencia
"""

    def _get_dynamic_mapping_examples(self) -> str:
        """
        ðŸŽ¯ GENERAR EJEMPLOS DINÃMICOS DE MAPEO

        Usa SystemCatalog para generar ejemplos de mapeo
        que ayudan al LLM a entender las intenciones.
        """
        try:
            from app.core.ai.system_catalog import SystemCatalog
            return SystemCatalog.generate_mapping_examples()
        except Exception as e:
            self.logger.error(f"âŒ [MASTER] Error generando ejemplos de mapeo: {e}")
            # Fallback a ejemplos bÃ¡sicos
            return """
- "buscar GarcÃ­a" â†’ consulta_alumnos/busqueda_simple
- "cuÃ¡ntos alumnos" â†’ consulta_alumnos/estadisticas
- "constancia para Juan" â†’ consulta_alumnos/generar_constancia
- "quÃ© puedes hacer" â†’ ayuda_sistema/explicacion_general
- "quiÃ©n te creÃ³" â†’ ayuda_sistema/sobre_creador
"""

    def _generate_humanized_response(self, result, user_query: str, routing_decision: dict):
        """
        ðŸ—£ï¸ MASTER COMO VOCERO FINAL (PROMPT 4)

        Genera respuesta humanizada basada en el resultado del especialista.
        Equivale al PROMPT 4 del flujo tradicional Master-Student.

        Args:
            result: Resultado del especialista
            user_query: Consulta original del usuario
            routing_decision: DecisiÃ³n del routing forzado

        Returns:
            InterpretationResult con respuesta humanizada
        """
        try:
            self.logger.info("ðŸ—£ï¸ [MASTER] Generando respuesta final como vocero...")

            # Usar el mÃ©todo existente de generaciÃ³n de respuesta
            humanized_response = self._generate_master_response(result.parameters, user_query)

            if humanized_response:
                self.logger.info("âœ… Master generÃ³ respuesta contextual exitosamente")
                self.logger.info(f"âœ… [MASTER] Respuesta final: '{humanized_response[:50]}...'")

                # Crear nuevo resultado con respuesta humanizada
                from app.core.ai.interpretation.base_interpreter import InterpretationResult

                humanized_result = InterpretationResult(
                    action=result.action,
                    parameters={
                        **result.parameters,
                        "master_response": humanized_response,
                        "humanized": True,
                        "routing_info": routing_decision
                    },
                    confidence=result.confidence
                )

                self.logger.info("ðŸ—£ï¸ [MASTER] Respuesta final generada como vocero")
                return humanized_result
            else:
                self.logger.warning("âš ï¸ [MASTER] No se pudo generar respuesta humanizada")
                return result

        except Exception as e:
            self.logger.error(f"âŒ [MASTER] Error generando respuesta humanizada: {e}")
            return result

    def _analyze_and_delegate_intelligently(self, user_query: str, conversation_stack: list):
        """
        ðŸ§  ANÃLISIS INTELIGENTE UNIFICADO CON RAZONAMIENTO HUMANO
        Reemplaza anÃ¡lisis semÃ¡ntico + resoluciÃ³n de referencias

        ðŸ”„ AHORA USA MASTERPROMPTMANAGER MEJORADO
        """
        try:
            # ðŸ”„ USAR MASTERPROMPTMANAGER EN LUGAR DE PROMPT HARDCODEADO
            conversation_context = self.prompt_manager.format_conversation_context(conversation_stack)
            prompt = self.prompt_manager.get_intention_detection_prompt(user_query, conversation_context)
            # ðŸ” DEBUG: MOSTRAR PROMPT COMPLETO ENVIADO AL LLM
            if os.getenv('DEBUG_PAUSES') == 'true':
                print("\nðŸ›‘ [MASTER-DEBUG] PROMPT COMPLETO ENVIADO AL LLM:")
                print("=" * 80)
                print(prompt)
                print("=" * 80)
                print("â””â”€â”€ Presiona ENTER para enviar al LLM...")
                input()

            # ðŸ” DEBUG: Verificar si los ejemplos de constancia estÃ¡n en el prompt
            if "generale una constancia" in prompt:
                self.logger.info("âœ… [DEBUG] Ejemplos de constancia encontrados en prompt")
            else:
                self.logger.error("âŒ [DEBUG] Ejemplos de constancia NO encontrados en prompt")

            # ðŸ” DEBUG: Verificar si hay contexto disponible
            if "Franco Alexander" in prompt:
                self.logger.info("âœ… [DEBUG] Contexto de Franco Alexander encontrado en prompt")
            else:
                self.logger.info("ðŸ” [DEBUG] No se encontrÃ³ contexto de Franco Alexander en prompt")

            if self.gemini_client:
                response = self.gemini_client.send_prompt_sync(prompt)

                # ðŸ” DEBUG: MOSTRAR RESPUESTA CRUDA DEL LLM
                if os.getenv('DEBUG_PAUSES') == 'true':
                    print("\nðŸ›‘ [MASTER-DEBUG] RESPUESTA CRUDA DEL LLM:")
                    print("=" * 80)
                    print(response)
                    print("=" * 80)
                    print("â””â”€â”€ Presiona ENTER para parsear JSON...")
                    input()

                if response:
                    import json
                    try:
                        # Limpiar respuesta JSON
                        clean_response = response.strip()
                        if clean_response.startswith('```json'):
                            clean_response = clean_response[7:]
                        if clean_response.startswith('```'):
                            clean_response = clean_response[3:]
                        if clean_response.endswith('```'):
                            clean_response = clean_response[:-3]
                        clean_response = clean_response.strip()

                        result = json.loads(clean_response)

                        # ðŸ” DEBUG: MOSTRAR JSON PARSEADO
                        if os.getenv('DEBUG_PAUSES') == 'true':
                            print("\nðŸ›‘ [MASTER-DEBUG] JSON PARSEADO EXITOSAMENTE:")
                            print("=" * 80)
                            import json
                            print(json.dumps(result, indent=2, ensure_ascii=False))
                            print("=" * 80)
                            print("â””â”€â”€ Presiona ENTER para continuar...")
                            input()

                        self.logger.info(f"ðŸ§  [MASTER] AnÃ¡lisis unificado exitoso: {result.get('reasoning', '')[:100]}...")
                        return result

                    except json.JSONDecodeError as e:
                        self.logger.warning(f"ðŸ§  [MASTER] Error parsing JSON: {e}")
                        self.logger.warning(f"ðŸ§  [MASTER] Respuesta: {response}")

                        # ðŸ” DEBUG: MOSTRAR ERROR DE PARSING
                        if os.getenv('DEBUG_PAUSES') == 'true':
                            print(f"\nðŸ›‘ [MASTER-DEBUG] ERROR PARSING JSON:")
                            print(f"Error: {e}")
                            print(f"Respuesta que fallÃ³: {response}")
                            print("â””â”€â”€ Presiona ENTER para continuar...")
                            input()

            return None

        except Exception as e:
            self.logger.error(f"Error en anÃ¡lisis inteligente unificado: {e}")
            return None

        except Exception as e:
            self.logger.error(f"Error en anÃ¡lisis inteligente unificado: {e}")
            return None

    def _process_unified_analysis(self, analysis_result: dict, original_intention):
        """
        ðŸ”§ PROCESAR RESULTADO DEL ANÃLISIS UNIFICADO
        Convierte el anÃ¡lisis del LLM en intenciÃ³n actualizada
        """
        try:
            # Actualizar intenciÃ³n basada en el anÃ¡lisis
            original_intention.intention_type = analysis_result.get('intention_type', original_intention.intention_type)
            original_intention.sub_intention = analysis_result.get('sub_intention', original_intention.sub_intention)
            original_intention.requiere_contexto = analysis_result.get('usar_contexto', False)

            # ðŸŽ¯ VALIDAR RESOLUCIÃ“N DE ALUMNO
            detected_entities = analysis_result.get('detected_entities', {})
            alumno_resuelto = detected_entities.get('alumno_resuelto')
            referencia_encontrada = analysis_result.get('referencia_encontrada', '')

            if alumno_resuelto and isinstance(alumno_resuelto, dict):
                alumno_id = alumno_resuelto.get('id')
                alumno_nombre = alumno_resuelto.get('nombre', '')

                # ðŸ§  CONFIAR EN EL ANÃLISIS INTELIGENTE DEL LLM
                # El LLM ya validÃ³ que tiene informaciÃ³n suficiente para resolver
                if alumno_nombre and not alumno_nombre.startswith('identificar'):
                    # âœ… EL LLM ENCONTRÃ“ INFORMACIÃ“N VÃLIDA - CONFIAR EN Ã‰L
                    # ðŸŽ¯ NOMBRE COMPLETO ES SUFICIENTE - Student manejarÃ¡ el mapeo tÃ©cnico

                    # âœ… ACEPTAR LA RESOLUCIÃ“N DEL LLM (nombre completo es suficiente)
                    original_intention.detected_entities['alumno_resuelto'] = alumno_resuelto
                    original_intention.requiere_contexto = False  # Ya no necesita contexto, estÃ¡ resuelto
                    self.logger.info(f"âœ… [MASTER] InformaciÃ³n suficiente para Student: {alumno_nombre} â†’ Student manejarÃ¡ mapeo tÃ©cnico")

                    # âœ… MAPEAR SUB-INTENCIONES CORRECTAS PARA STUDENT
                    if analysis_result.get('sub_intention') in ['informacion_completa', 'busqueda_filtrada']:
                        original_intention.sub_intention = 'busqueda_simple'
                        self.logger.info(f"ðŸ”§ [MASTER] Mapeando '{analysis_result.get('sub_intention')}' â†’ 'busqueda_simple' para alumno resuelto")
                    elif analysis_result.get('sub_intention') == 'generar_constancia':
                        # ðŸŽ¯ CONSTANCIAS: Mantener como consulta_alumnos + generar_constancia
                        original_intention.intention_type = 'consulta_alumnos'
                        original_intention.sub_intention = 'generar_constancia'
                        self.logger.info(f"ðŸ”§ [MASTER] Mapeando 'generar_constancia' â†’ 'consulta_alumnos/generar_constancia' para alumno resuelto")

                else:
                    # âŒ Solo rechazar si el nombre es claramente invÃ¡lido o descriptivo
                    self.logger.warning(f"âš ï¸ [MASTER] ResoluciÃ³n invÃ¡lida: Nombre invÃ¡lido o descriptivo: '{alumno_nombre}'")
                    original_intention.requiere_contexto = True

            # ðŸ“Š LOG DEL ANÃLISIS COMPLETO
            razonamiento = analysis_result.get('reasoning', 'No especificado')
            contexto_analizado = analysis_result.get('contexto_analizado', 'No especificado')

            self.logger.info(f"ðŸ§  [MASTER] Razonamiento: {razonamiento[:100]}...")
            self.logger.info(f"ðŸ“Š [MASTER] Contexto analizado: {contexto_analizado}")
            self.logger.info(f"ðŸ” [MASTER] Referencia encontrada: {referencia_encontrada}")
            self.logger.info(f"ðŸŽ¯ [MASTER] AnÃ¡lisis unificado completado: {analysis_result.get('intention_type')}/{original_intention.sub_intention}")

            return original_intention

        except Exception as e:
            self.logger.error(f"Error procesando anÃ¡lisis unificado: {e}")
            return original_intention

    def _create_context_summary(self, conversation_stack: list) -> str:
        """
        ðŸ“‹ CREAR RESUMEN DEL CONTEXTO CONVERSACIONAL
        Genera un resumen inteligente del conversation_stack para el LLM
        """
        try:
            if not conversation_stack:
                return "Sin contexto conversacional previo."

            context_lines = []
            context_lines.append("CONTEXTO CONVERSACIONAL (niveles por prioridad, mÃ¡s reciente primero):")
            context_lines.append("")

            for i, nivel in enumerate(reversed(conversation_stack), 1):
                query = nivel.get('query', 'N/A')
                row_count = nivel.get('row_count', 0)
                awaiting = nivel.get('awaiting', 'N/A')
                data = nivel.get('data', [])

                context_lines.append(f"NIVEL {i}: '{query}'")

                if row_count == 1 and data:
                    # InformaciÃ³n especÃ­fica del alumno
                    alumno = data[0]
                    nombre = alumno.get('nombre', 'N/A')
                    id_alumno = alumno.get('id', 'N/A')
                    grado = alumno.get('grado', 'N/A')
                    grupo = alumno.get('grupo', 'N/A')
                    context_lines.append(f"â†’ Alumno especÃ­fico: {nombre} (ID: {id_alumno}) - {grado}Â° {grupo}")

                elif row_count <= 10 and data:
                    # Lista pequeÃ±a con nombres
                    if isinstance(data, list):
                        nombres = [d.get('nombre', 'N/A') for d in data[:5]]
                        context_lines.append(f"â†’ Lista pequeÃ±a ({row_count} alumnos): {', '.join(nombres)}")
                        if row_count > 5:
                            context_lines.append(f"â†’ (y {row_count - 5} mÃ¡s...)")
                    else:
                        context_lines.append(f"â†’ Datos estructurados ({row_count} elementos)")

                elif row_count > 10:
                    # Lista grande - CONTEXTO COMPLETO PARA RESOLUCIÃ“N INTELIGENTE
                    context_lines.append(f"â†’ Lista grande: {row_count} alumnos disponibles")
                    if data:
                        # âœ… MOSTRAR SUFICIENTES ELEMENTOS PARA RESOLUCIÃ“N DINÃMICA
                        elementos_mostrar = min(10, len(data))  # Mostrar hasta 10 para referencias
                        context_lines.append(f"â†’ Primeros {elementos_mostrar} alumnos (para referencias posicionales, nombres, etc.):")
                        for j, alumno in enumerate(data[:elementos_mostrar], 1):
                            nombre = alumno.get('nombre', 'N/A')
                            id_alumno = alumno.get('id', 'N/A')
                            matricula = alumno.get('matricula', 'N/A')
                            grado = alumno.get('grado', 'N/A')
                            grupo = alumno.get('grupo', 'N/A')
                            context_lines.append(f"   {j}. {nombre} (ID: {id_alumno}, Mat: {matricula}, {grado}Â°{grupo})")

                        if row_count > elementos_mostrar:
                            context_lines.append(f"   ... y {row_count - elementos_mostrar} mÃ¡s disponibles")

                        # Detectar criterios comunes
                        primer_alumno = data[0]
                        if 'grado' in primer_alumno:
                            grado = primer_alumno.get('grado')
                            context_lines.append(f"â†’ Criterio detectado: {grado}Â° grado")
                        if 'turno' in primer_alumno:
                            turno = primer_alumno.get('turno')
                            context_lines.append(f"â†’ Turno: {turno}")

                        # âœ… INFORMACIÃ“N PARA RESOLUCIÃ“N DINÃMICA
                        context_lines.append(f"â†’ LISTA COMPLETA DISPONIBLE: Puedes referenciar por posiciÃ³n, nombre, matrÃ­cula, etc.")
                else:
                    context_lines.append(f"â†’ {row_count} resultados")

                context_lines.append(f"â†’ Esperando: {awaiting}")
                context_lines.append("")

            return "\n".join(context_lines)

        except Exception as e:
            self.logger.error(f"Error creando resumen de contexto: {e}")
            return "Error procesando contexto conversacional."

    def _handle_ambiguous_reference(self, user_query: str, intention, conversation_stack: list):
        """
        ðŸš¨ MANEJA REFERENCIAS AMBIGUAS DETECTADAS POR EL LLM
        Genera una respuesta de aclaraciÃ³n inteligente basada en el contexto
        """
        try:
            from app.core.ai.interpretation.base_interpreter import InterpretationResult

            # Obtener informaciÃ³n del contexto para generar aclaraciÃ³n especÃ­fica
            ultimo_nivel = conversation_stack[-1] if conversation_stack else None
            if not ultimo_nivel:
                return intention

            row_count = ultimo_nivel.get('row_count', 0)
            query_anterior = ultimo_nivel.get('query', 'consulta anterior')

            # Generar mensaje de aclaraciÃ³n especÃ­fico
            if row_count > 1:
                human_response = f"ðŸ¤” Tu consulta '{user_query}' es ambigua. EncontrÃ© {row_count} alumnos en '{query_anterior}'. Â¿PodrÃ­as especificar a cuÃ¡l te refieres? Por ejemplo: 'el segundo', 'el tercero', o menciona el nombre especÃ­fico."
            else:
                human_response = f"ðŸ¤” Tu consulta '{user_query}' no es lo suficientemente clara. Â¿PodrÃ­as ser mÃ¡s especÃ­fico sobre quÃ© informaciÃ³n necesitas?"

            # Crear resultado de aclaraciÃ³n
            result = InterpretationResult(
                action="aclaracion_requerida",
                parameters={
                    "message": human_response,
                    "original_query": user_query,
                    "human_response": human_response,
                    "context_info": {
                        "row_count": row_count,
                        "query_anterior": query_anterior
                    }
                },
                confidence=0.9
            )

            self.logger.info(f"ðŸš¨ [MASTER] Generada aclaraciÃ³n para referencia ambigua: {row_count} candidatos")
            return result

        except Exception as e:
            self.logger.error(f"âŒ Error manejando referencia ambigua: {e}")
            return intention

    def _analyze_context_relevance(self, intention, conversation_stack: list, user_query: str) -> dict:
        """
        ðŸ§  PASO 3: ANÃLISIS DE CONTEXTO COMO HUMANO EXPERTO
        Determina si hay informaciÃ³n previa relevante para la consulta actual
        """
        try:
            # Si no hay contexto, es independiente
            if not conversation_stack:
                return {
                    "needs_context": False,
                    "analysis": "Sin contexto conversacional previo - consulta independiente",
                    "resolved_reference": None
                }

            # AnÃ¡lisis bÃ¡sico de referencias contextuales
            contextual_keywords = [
                "Ã©l", "ella", "ese", "esa", "este", "esta", "aquel", "aquella",
                "el anterior", "la anterior", "el primero", "el segundo", "el Ãºltimo",
                "sus datos", "su informaciÃ³n", "de Ã©l", "para ella",
                "tambiÃ©n", "ademÃ¡s", "igualmente"
            ]

            needs_context = any(keyword in user_query.lower() for keyword in contextual_keywords)

            if needs_context:
                # Hay referencias contextuales - necesita anÃ¡lisis profundo
                return {
                    "needs_context": True,
                    "analysis": f"Detectadas referencias contextuales en: '{user_query}'",
                    "resolved_reference": "Requiere anÃ¡lisis LLM para resoluciÃ³n"
                }
            else:
                # Consulta independiente
                return {
                    "needs_context": False,
                    "analysis": "Consulta semÃ¡nticamente independiente - no requiere contexto",
                    "resolved_reference": None
                }

        except Exception as e:
            self.logger.error(f"Error analizando relevancia de contexto: {e}")
            return {
                "needs_context": False,
                "analysis": "Error en anÃ¡lisis - procesando como independiente",
                "resolved_reference": None
            }

    def _analyze_query_semantic_independence(self, user_query: str, conversation_stack: list) -> bool:
        """
        ðŸ§  ANALIZA SI LA CONSULTA ES SEMÃNTICAMENTE INDEPENDIENTE
        Usa razonamiento LLM para determinar si necesita contexto
        """
        try:
            # Si no hay contexto, obviamente es independiente
            if not conversation_stack:
                return True

            # Crear prompt para anÃ¡lisis semÃ¡ntico
            context_summary = self._create_context_summary(conversation_stack)

            prompt = f"""
ðŸ§  ANÃLISIS CONTEXTUAL INTELIGENTE - MASTER DEL SISTEMA ESCOLAR

CONTEXTO CONVERSACIONAL COMPLETO:
{context_summary}

CONSULTA A ANALIZAR: "{user_query}"

ðŸŽ¯ MI IDENTIDAD Y CONOCIMIENTO COMPLETO:
- Sistema escolar "PROF. MAXIMO GAMIZ FERNANDEZ"
- Base de datos: 211 alumnos en grados 1Â° a 6Â°
- Especialistas: StudentQueryInterpreter, HelpInterpreter

ðŸŽ¯ ESPECIALISTAS QUE DIRIJO:
**StudentQueryInterpreter**:
- BUSCAR_UNIVERSAL: BÃºsquedas flexibles
- CONTAR_UNIVERSAL: Conteos y estadÃ­sticas
- GENERAR_CONSTANCIA_COMPLETA: Documentos PDF
- BUSCAR_Y_FILTRAR: Filtros sobre resultados

ðŸ“‹ NIVELES DE CONTEXTO:
- Nivel 1 = MÃS RECIENTE (mÃ¡s relevante)
- Listas grandes = "regenerables" (SQL + metadatos)
- Puedo usar CUALQUIER nivel para resolver referencias

ðŸ§  RAZONAMIENTO INTELIGENTE:
1. Â¿QuÃ© solicita el usuario?
2. Â¿QuÃ© informaciÃ³n tengo disponible?
3. Â¿Puedo resolver con contexto?
4. Â¿Hay referencias que resolver?

ELEMENTOS QUE INDICAN NECESIDAD DE CONTEXTO:
â€¢ Pronombres referenciales: "Ã©l", "ella", "ese", "esa", "este", "esta", "aquel", "aquella"
â€¢ Frases pronominales: "ese alumno", "esa estudiante", "ese chico", "esa persona"
â€¢ Referencias posicionales: "el primero", "el segundo", "el Ãºltimo", "la primera"
â€¢ Adjetivos demostrativos sin sustantivo: "ese", "esa", "este", "esta"
â€¢ Referencias implÃ­citas: "sus datos", "su informaciÃ³n", "de Ã©l", "para ella"
â€¢ Continuaciones: "tambiÃ©n", "ademÃ¡s", "igualmente", "del mismo modo"
â€¢ Filtros sobre resultados previos: "de esos", "entre ellos", "de los anteriores"

EJEMPLOS DE ANÃLISIS INTELIGENTE:
INDEPENDIENTES:
- "buscar GarcÃ­a" â†’ Nueva bÃºsqueda completa
- "buscar JUAN PÃ‰REZ LÃ“PEZ" â†’ Nombre completo especÃ­fico
- "constancia para MARÃA GONZÃLEZ" â†’ Nombre especÃ­fico
- "estadÃ­sticas de grupos" â†’ Consulta general
- "cuÃ¡ntos alumnos hay en total" â†’ Consulta global

NECESITAN CONTEXTO:
- "constancia para el segundo" â†’ Referencia posicional
- "de esos cuÃ¡ntos son del turno matutino" â†’ Filtro sobre anteriores
- "dame la curp de gabriela" â†’ Nombre parcial, buscar en contexto
- "constancia para ella" â†’ Pronombre, resolver referencia
- "tambiÃ©n necesito su constancia" â†’ ContinuaciÃ³n con referencia

RESPUESTA: "INDEPENDIENTE" o "NECESITA_CONTEXTO"
"""

            response = self.gemini_client.send_prompt_sync(prompt)
            result = response.strip().upper() if response else ""

            is_independent = "INDEPENDIENTE" in result

            self.logger.info(f"ðŸ§  [MASTER] AnÃ¡lisis semÃ¡ntico: '{user_query}' â†’ {'INDEPENDIENTE' if is_independent else 'NECESITA_CONTEXTO'}")

            return is_independent

        except Exception as e:
            self.logger.error(f"Error en anÃ¡lisis semÃ¡ntico: {e}")
            # ðŸ§  FALLBACK SIMPLE: Si hay error, asumir que necesita contexto si hay contexto disponible
            if conversation_stack:
                self.logger.warning(f"ðŸ§  [MASTER] Error en LLM, pero hay contexto disponible - asumiendo NECESITA_CONTEXTO")
                return False  # NECESITA CONTEXTO
            else:
                self.logger.warning(f"ðŸ§  [MASTER] Error en LLM, sin contexto disponible - asumiendo INDEPENDIENTE")
                return True  # INDEPENDIENTE

    def _resolve_reference_with_llm(self, user_query: str, conversation_stack: list) -> dict:
        """
        ðŸ§  RESOLUCIÃ“N INTELIGENTE DE REFERENCIAS CON LLM
        El LLM entiende CUALQUIER tipo de referencia sin listas hardcodeadas
        """
        try:
            if not conversation_stack:
                return None

            # Crear contexto para el LLM
            context_summary = self._create_detailed_context_for_reference(conversation_stack)

            prompt = f"""
ðŸ§  RESOLUCIÃ“N INTELIGENTE DE REFERENCIAS - SISTEMA ESCOLAR

CONSULTA DEL USUARIO: "{user_query}"

CONTEXTO CONVERSACIONAL DISPONIBLE:
{context_summary}

ðŸŽ¯ TU TAREA:
Analiza si la consulta del usuario hace referencia a algÃºn alumno especÃ­fico del contexto.

ðŸ§  REGLAS CRÃTICAS DE RAZONAMIENTO:
1. Si hay UNA SOLA persona en el contexto â†’ Referencia clara
2. Si hay MÃšLTIPLES personas CON OPERACIÃ“N DE FILTRO â†’ Referencia clara a la lista completa
3. Si hay MÃšLTIPLES personas SIN especificaciÃ³n ni operaciÃ³n â†’ AMBIGUO, no asumir
4. Si hay posiciÃ³n especÃ­fica ("el segundo") â†’ Referencia clara
5. Si hay pronombre vago ("su") con lista mÃºltiple SIN OPERACIÃ“N â†’ AMBIGUO

TIPOS DE REFERENCIAS VÃLIDAS:
- Pronominales CLARAS: "su informaciÃ³n" (cuando hay 1 alumno especÃ­fico)
- Posicionales ESPECÃFICAS: "el segundo", "el tercero", "el Ãºltimo"
- ImplÃ­citas CLARAS: "tambiÃ©n necesito" (cuando hay 1 alumno especÃ­fico)
- OPERACIONES DE FILTRO: "de ellos los que...", "de esos cuÃ¡ntos...", "los GarcÃ­a del turno..."

ðŸŽ¯ OPERACIONES DE FILTRO (SIEMPRE CLARAS):
- "de ellos dame los que esten en el turno matutino" â†’ FILTRO sobre lista completa
- "de esos cuÃ¡ntos son de primer grado" â†’ CONTEO sobre lista completa
- "los GarcÃ­a del turno vespertino" â†’ FILTRO sobre lista completa
- "cuÃ¡ntos de ellos tienen calificaciones" â†’ ANÃLISIS sobre lista completa

âŒ NO ASUMIR REFERENCIAS EN CASOS AMBIGUOS:
- "su informaciÃ³n" con lista de 20+ alumnos SIN OPERACIÃ“N â†’ AMBIGUO
- "Ã©l" con mÃºltiples candidatos SIN ESPECIFICACIÃ“N â†’ AMBIGUO
- Pronombres vagos sin contexto especÃ­fico â†’ AMBIGUO

FORMATO DE RESPUESTA JSON:
{{
    "tiene_referencia": true/false,
    "es_ambiguo": true/false,
    "alumno_referenciado": {{
        "id": nÃºmero_id,
        "nombre": "NOMBRE COMPLETO",
        "razonamiento": "explicaciÃ³n especÃ­fica"
    }},
    "motivo_ambiguedad": "explicaciÃ³n si es ambiguo"
}}

EJEMPLOS CORRECTOS:
- "su informaciÃ³n" + 1 alumno especÃ­fico â†’ tiene_referencia: true
- "su informaciÃ³n" + lista de 21 â†’ es_ambiguo: true
- "el segundo" + lista â†’ tiene_referencia: true (posiciÃ³n especÃ­fica)
- "de ellos los del turno matutino" + lista de 49 â†’ tiene_referencia: false (operaciÃ³n de filtro, NO referencia individual)
- "de esos cuÃ¡ntos son de primer grado" + lista â†’ tiene_referencia: false (operaciÃ³n de conteo, NO referencia individual)

RESPONDE SOLO CON EL JSON:
"""

            if self.gemini_client:
                response = self.gemini_client.send_prompt_sync(prompt)
                if response:
                    import json
                    try:
                        # ðŸ”§ LIMPIAR RESPUESTA: Remover bloques de cÃ³digo markdown
                        clean_response = response.strip()
                        if clean_response.startswith('```json'):
                            clean_response = clean_response[7:]  # Remover ```json
                        if clean_response.startswith('```'):
                            clean_response = clean_response[3:]   # Remover ```
                        if clean_response.endswith('```'):
                            clean_response = clean_response[:-3]  # Remover ``` final
                        clean_response = clean_response.strip()

                        result = json.loads(clean_response)

                        # Verificar si es ambiguo
                        if result.get("es_ambiguo"):
                            motivo = result.get("motivo_ambiguedad", "Referencia ambigua")
                            self.logger.info(f"ðŸ§  [LLM] Referencia AMBIGUA detectada: {motivo}")
                            return None  # No resolver, que pida aclaraciÃ³n

                        # Si tiene referencia clara
                        if result.get("tiene_referencia") and result.get("alumno_referenciado"):
                            alumno = result["alumno_referenciado"]
                            self.logger.info(f"ðŸ§  [LLM] Referencia CLARA detectada: {alumno.get('razonamiento')}")
                            return {
                                'id': alumno.get('id'),
                                'nombre': alumno.get('nombre'),
                                'posicion': 'resuelto por LLM'
                            }
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"ðŸ§  [LLM] Error parsing JSON: {e}")
                        self.logger.warning(f"ðŸ§  [LLM] Respuesta original: {response}")
                        self.logger.warning(f"ðŸ§  [LLM] Respuesta limpia: {clean_response}")

            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia con LLM: {e}")
            return None

    def _create_detailed_context_for_reference(self, conversation_stack: list) -> str:
        """
        ðŸ§  CONTEXTO DETALLADO PARA RESOLUCIÃ“N DE REFERENCIAS
        """
        try:
            if not conversation_stack:
                return "Sin contexto previo"

            context_parts = []

            for i, nivel in enumerate(reversed(conversation_stack), 1):
                query = nivel.get('query', 'N/A')
                data = nivel.get('data', [])
                row_count = nivel.get('row_count', 0)

                if data:
                    if row_count == 1:
                        alumno = data[0]
                        context_parts.append(f"""
NIVEL {i} (mÃ¡s reciente): "{query}"
â†’ Alumno especÃ­fico: {alumno.get('nombre')} (ID: {alumno.get('id')})
â†’ Grado: {alumno.get('grado')}Â° {alumno.get('grupo')}, Turno: {alumno.get('turno')}
â†’ CURP: {alumno.get('curp')}""")
                    elif row_count <= 10:
                        if isinstance(data, list):
                            nombres = [f"{j+1}. {d.get('nombre')} (ID: {d.get('id')})" for j, d in enumerate(data[:5])]
                            context_parts.append(f"""
NIVEL {i}: "{query}"
â†’ Lista de {row_count} alumnos:
{chr(10).join(nombres)}""")
                        else:
                            context_parts.append(f"""
NIVEL {i}: "{query}"
â†’ Datos estructurados: {row_count} elementos""")
                    else:
                        context_parts.append(f"""
NIVEL {i}: "{query}"
â†’ Lista grande de {row_count} alumnos (primeros 3):
1. {data[0].get('nombre')} (ID: {data[0].get('id')})
2. {data[1].get('nombre')} (ID: {data[1].get('id')})
3. {data[2].get('nombre')} (ID: {data[2].get('id')})
... y {row_count-3} mÃ¡s""")

            return "\n".join(context_parts) if context_parts else "Sin contexto Ãºtil"

        except Exception as e:
            self.logger.error(f"Error creando contexto detallado: {e}")
            return "Error en contexto"



    def _resolve_positional_reference(self, user_query: str, conversation_stack: list) -> dict:
        """Resuelve referencias posicionales como 'segundo', 'tercero'"""
        try:
            if not conversation_stack:
                return None

            # Obtener datos del Ãºltimo nivel
            ultimo_nivel = conversation_stack[-1]
            data = ultimo_nivel.get('data', [])

            if not data:
                return None

            # Determinar posiciÃ³n
            position_index = None
            query_lower = user_query.lower()

            if 'primer' in query_lower or 'primero' in query_lower:
                position_index = 0
            elif 'segundo' in query_lower or 'segunda' in query_lower:
                position_index = 1
            elif 'tercer' in query_lower or 'tercero' in query_lower:
                position_index = 2
            elif 'cuarto' in query_lower or 'cuarta' in query_lower:
                position_index = 3
            elif 'quinto' in query_lower or 'quinta' in query_lower:
                position_index = 4
            elif 'Ãºltimo' in query_lower or 'Ãºltima' in query_lower:
                position_index = len(data) - 1

            if position_index is not None and position_index < len(data):
                alumno = data[position_index]
                return {
                    'id': alumno.get('id') or alumno.get('alumno_id'),
                    'nombre': alumno.get('nombre'),
                    'posicion': f"posiciÃ³n {position_index + 1}"
                }

            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia posicional: {e}")
            return None

    def _resolve_pronominal_reference(self, conversation_stack: list) -> dict:
        """
        ðŸ§  RESOLUCIÃ“N INTELIGENTE DE REFERENCIAS PRONOMINALES
        MEJORA: Busca en TODOS los niveles con lÃ³gica inteligente
        """
        try:
            if not conversation_stack:
                return None

            # ðŸ§  BUSCAR EN TODOS LOS NIVELES CON LÃ“GICA INTELIGENTE
            for nivel in reversed(conversation_stack):
                data = nivel.get('data', [])
                query = nivel.get('query', '').lower()

                if not data:
                    continue

                # CASO 1: Un solo alumno (ideal)
                if len(data) == 1:
                    alumno = data[0]
                    # Verificar que no sea consulta de campo especÃ­fico
                    if not any(word in query for word in ['curp', 'matrÃ­cula', 'informaciÃ³n']):
                        return {
                            'id': alumno.get('id') or alumno.get('alumno_id'),
                            'nombre': alumno.get('nombre'),
                            'posicion': 'Ãºltimo mencionado especÃ­ficamente'
                        }

                # CASO 2: MÃºltiples alumnos - buscar referencia especÃ­fica
                elif len(data) > 1:
                    # Si hay nombre especÃ­fico en la query
                    for alumno in data:
                        nombre_completo = alumno.get('nombre', '').lower()
                        if any(part in query for part in nombre_completo.split()):
                            return {
                                'id': alumno.get('id') or alumno.get('alumno_id'),
                                'nombre': alumno.get('nombre'),
                                'posicion': 'referencia especÃ­fica por nombre'
                            }

                    # Si no hay referencia especÃ­fica, tomar el primero
                    alumno = data[0]
                    return {
                        'id': alumno.get('id') or alumno.get('alumno_id'),
                        'nombre': alumno.get('nombre'),
                        'posicion': 'primero de la lista anterior'
                    }

            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia pronominal: {e}")
            return None

    def _resolve_name_reference(self, nombre_detectado: str, conversation_stack: list) -> dict:
        """Resuelve referencias por nombre parcial como 'mario' â†’ 'MARIO LOPEZ GONZALEZ'"""
        try:
            if not conversation_stack or not nombre_detectado:
                return None

            nombre_lower = nombre_detectado.lower()
            self.logger.info(f"ðŸ” [MASTER] Buscando referencia por nombre: '{nombre_detectado}'")

            # Buscar en todos los niveles del contexto
            for nivel in reversed(conversation_stack):
                data = nivel.get('data', [])
                if not data:
                    continue

                # Buscar coincidencia por nombre parcial
                for alumno in data:
                    nombre_completo = alumno.get('nombre', '').lower()

                    # Verificar si el nombre detectado estÃ¡ contenido en el nombre completo
                    if nombre_lower in nombre_completo:
                        alumno_id = alumno.get('id') or alumno.get('alumno_id')
                        self.logger.info(f"âœ… [MASTER] COINCIDENCIA ENCONTRADA: '{nombre_detectado}' â†’ '{alumno.get('nombre')}' (ID: {alumno_id})")
                        return {
                            'id': alumno_id,
                            'nombre': alumno.get('nombre'),
                            'posicion': 'referencia por nombre'
                        }

            self.logger.warning(f"âŒ [MASTER] No se encontrÃ³ referencia para: '{nombre_detectado}'")
            return None

        except Exception as e:
            self.logger.error(f"Error resolviendo referencia por nombre: {e}")
            return None

    def _get_student_id_by_name(self, nombre_completo: str) -> Optional[int]:
        """
        ðŸš« MÃ‰TODO OBSOLETO - NO USAR
        RAZÃ“N: Master no debe buscar IDs, nombre completo es suficiente
        Student maneja el mapeo tÃ©cnico a la base de datos
        """
        try:
            if not nombre_completo:
                return None

            self.logger.info(f"ðŸ” [MASTER] Buscando ID para: '{nombre_completo}'")

            # Usar el servicio de alumnos para buscar
            from app.core.service_provider import ServiceProvider
            service_provider = ServiceProvider.get_instance()
            alumno_service = service_provider.alumno_service

            # Buscar por nombre exacto
            alumnos_encontrados = alumno_service.buscar_alumnos(nombre_completo)

            if not alumnos_encontrados:
                self.logger.warning(f"âŒ [MASTER] No se encontrÃ³ alumno con nombre: '{nombre_completo}'")
                return None

            # Buscar coincidencia exacta
            for alumno in alumnos_encontrados:
                alumno_dict = alumno.to_dict() if hasattr(alumno, 'to_dict') else alumno
                if alumno_dict.get('nombre', '').upper() == nombre_completo.upper():
                    alumno_id = alumno_dict.get('id')
                    self.logger.info(f"âœ… [MASTER] ID encontrado: '{nombre_completo}' â†’ ID: {alumno_id}")
                    return alumno_id

            # Si no hay coincidencia exacta, tomar el primero si es muy similar
            primer_alumno = alumnos_encontrados[0]
            alumno_dict = primer_alumno.to_dict() if hasattr(primer_alumno, 'to_dict') else primer_alumno
            alumno_id = alumno_dict.get('id')

            self.logger.info(f"âœ… [MASTER] Usando primer resultado similar: '{alumno_dict.get('nombre')}' â†’ ID: {alumno_id}")
            return alumno_id

        except Exception as e:
            self.logger.error(f"Error buscando ID por nombre: {e}")
            return None

    def _validate_intention_with_system_map(self, intention):
        """ðŸ›¡ï¸ VALIDAR INTENCIÃ“N CON SYSTEM MAP Y CORREGIR AUTOMÃTICAMENTE"""
        try:
            intention_type = intention.intention_type

            # ðŸŽ¯ LISTA DE INTENCIONES VÃLIDAS (SEGÃšN SYSTEM_MAP)
            valid_intentions = []
            for specialist, config in self.system_map.items():
                valid_intentions.extend(config["handles"])

            # âœ… VERIFICAR SI LA INTENCIÃ“N ES VÃLIDA
            if intention_type in valid_intentions:
                for specialist, config in self.system_map.items():
                    if intention_type in config["handles"]:
                        self.logger.info(f"âœ… [MASTER] IntenciÃ³n '{intention_type}' validada para {specialist}")
                        return intention

            # ðŸ”§ CORRECCIÃ“N AUTOMÃTICA DE INTENCIONES INCORRECTAS
            self.logger.warning(f"âš ï¸ [MASTER] IntenciÃ³n '{intention_type}' no encontrada en system_map")

            # Mapeo automÃ¡tico para intenciones comunes mal detectadas
            incorrect_mappings = {
                "estadistica": "consulta_alumnos",
                "busqueda": "consulta_alumnos",
                "constancia": "consulta_alumnos",
                "transformacion": "transformacion_pdf",  # Corregir transformacion â†’ transformacion_pdf
                "ayuda": "ayuda_sistema",
                "help": "ayuda_sistema"
            }

            if intention_type in incorrect_mappings:
                old_intention = intention_type
                intention.intention_type = incorrect_mappings[old_intention]
                self.logger.info(f"ðŸ”§ [MASTER] Auto-correcciÃ³n: '{old_intention}' â†’ '{intention.intention_type}'")
                return intention

            # âŒ ERROR SI NO SE PUEDE MAPEAR
            self.logger.error(f"âŒ [MASTER] IntenciÃ³n no reconocida: {intention_type}")
            self.logger.error(f"âŒ Intenciones vÃ¡lidas: {valid_intentions}")

            # Fallback a consulta_alumnos para mantener funcionalidad
            self.logger.info(f"ðŸ”§ [MASTER] Fallback: '{intention_type}' â†’ 'consulta_alumnos'")
            intention.intention_type = "consulta_alumnos"
            return intention

        except Exception as e:
            self.logger.error(f"âŒ Error validando intenciÃ³n: {e}")
            return intention

    def _delegate_to_specialist_direct(self, context: InterpretationContext, intention, current_pdf=None):
        """ðŸŽ¯ DELEGAR AL ESPECIALISTA CON CONTEXTO COMPLETO"""
        try:


            # Agregar informaciÃ³n de intenciÃ³n consolidada al contexto
            context.intention_info = {
                'intention_type': intention.intention_type,
                'sub_intention': intention.sub_intention,
                'confidence': intention.confidence,
                'reasoning': intention.reasoning,
                'detected_entities': intention.detected_entities,
                # ðŸ†• CATEGORIZACIÃ“N ESPECÃFICA CONSOLIDADA
                'categoria': intention.categoria,
                'sub_tipo': intention.sub_tipo,
                'complejidad': intention.complejidad,
                'requiere_contexto': intention.requiere_contexto,
                'flujo_optimo': intention.flujo_optimo
            }

            # ðŸ§  [MASTER] Delegando a Student con instrucciones claras

            # ðŸŽ¯ DELEGACIÃ“N CONSOLIDADA - Elimina duplicaciÃ³n masiva
            return self._execute_delegation_unified(intention, context, current_pdf=current_pdf)

        except Exception as e:
            self.logger.error(f"âŒ Error delegando al especialista: {e}")
            # ðŸ§¹ SIN FALLBACKS - Que falle claramente para debugging
            raise

    def _execute_delegation_unified(self, intention, context: InterpretationContext, current_pdf=None):
        """
        ðŸŽ¯ DELEGACIÃ“N UNIFICADA - Elimina duplicaciÃ³n masiva de cÃ³digo

        Consolida la lÃ³gica de delegaciÃ³n que estaba duplicada 4 veces.
        Mantiene 100% la funcionalidad original pero sin repeticiÃ³n.
        """
        try:
            intention_type = intention.intention_type

            # ðŸŽ¯ MAPEO DE INTENCIONES A ESPECIALISTAS
            specialist_map = {
                "consulta_alumnos": {
                    "interpreter": self.student_interpreter,
                    "name": "StudentQueryInterpreter",
                    "description": ""
                },
                "transformacion_pdf": {
                    "interpreter": self.student_interpreter,
                    "name": "StudentQueryInterpreter",
                    "description": " (transformaciÃ³n PDF)"
                },
                "ayuda_sistema": {
                    "interpreter": self.help_interpreter,
                    "name": "HelpInterpreter",
                    "description": ""
                },
                "conversacion_general": {
                    "interpreter": self.general_interpreter,
                    "name": "GeneralInterpreter",
                    "description": " (conversaciÃ³n natural)"
                }
            }

            # ðŸŽ¯ OBTENER ESPECIALISTA PARA LA INTENCIÃ“N
            specialist_config = specialist_map.get(intention_type)
            if not specialist_config:
                self.logger.error(f"âŒ [MASTER] IntenciÃ³n no reconocida: {intention_type}")
                raise ValueError(f"IntenciÃ³n no reconocida: {intention_type}")

            # ðŸŽ¯ LOGS UNIFICADOS (MISMA ESTRUCTURA QUE ANTES)
            specialist_name = specialist_config["name"]
            description = specialist_config["description"]
            self.logger.info(f"ðŸŽ¯ [MASTER] Dirigiendo a {specialist_name}{description}")
            self.logger.info(f"   â”œâ”€â”€ Sub-intenciÃ³n: {intention.sub_intention}")
            self.logger.info(f"   â””â”€â”€ Entidades: {len(intention.detected_entities)} detectadas")

            # ðŸŽ¯ EJECUTAR DELEGACIÃ“N
            specialist = specialist_config["interpreter"]
            # Verificar si el specialist acepta current_pdf
            if hasattr(specialist, 'interpret'):
                import inspect
                sig = inspect.signature(specialist.interpret)
                if 'current_pdf' in sig.parameters:
                    result = specialist.interpret(context, current_pdf=current_pdf)
                else:
                    result = specialist.interpret(context)
            else:
                result = specialist.interpret(context)
            self.logger.info(f"ðŸ“Š [MASTER] Resultado: {result.action if result else 'None'}")

            # ðŸŽ¯ MASTER COMO VOCERO: Generar respuesta final (IGUAL QUE ANTES)
            if result:
                final_result = self._generate_master_response(result, context.user_message)
                self.logger.info(f"ðŸ—£ï¸ [MASTER] Respuesta final generada como vocero")
                return final_result

            return result

        except Exception as e:
            self.logger.error(f"âŒ Error en delegaciÃ³n unificada: {e}")
            raise

    def _process_specialist_feedback(self, intention, result):
        """ðŸŽ¯ PROCESAR RETROALIMENTACIÃ“N DEL ESPECIALISTA"""
        try:
            if result:
                # Actualizar memoria de interacciones
                self.interaction_memory.update({
                    "last_specialist": self._get_specialist_for_intention(intention.intention_type),
                    "last_result_summary": f"AcciÃ³n: {result.action}",
                    "conversation_flow": f"{intention.intention_type} â†’ {result.action}",
                    "specialist_feedback": "Completado exitosamente",
                    "awaiting_continuation": getattr(result, 'awaiting_continuation', False),
                    "continuation_type": getattr(result, 'continuation_type', None)
                })

                self.logger.info(f"ðŸ”„ [MASTER] Memoria actualizada:")
                self.logger.info(f"   â”œâ”€â”€ Especialista: {self.interaction_memory['last_specialist']}")
                self.logger.info(f"   â”œâ”€â”€ Resultado: {self.interaction_memory['last_result_summary']}")
                self.logger.info(f"   â””â”€â”€ Flujo: {self.interaction_memory['conversation_flow']}")
            else:
                self.logger.warning(f"âš ï¸ [MASTER] No se recibiÃ³ resultado del especialista")

        except Exception as e:
            self.logger.error(f"âŒ Error procesando retroalimentaciÃ³n: {e}")

    def _handle_ambiguous_query(self, context: InterpretationContext, intention) -> Optional[InterpretationResult]:
        """
        ðŸ¤” MANEJA CONSULTAS AMBIGUAS - PIDE ACLARACIÃ“N AL USUARIO DE FORMA SIMPLE
        """
        try:
            self.logger.info("ðŸ¤” [MASTER] Consulta ambigua detectada - pidiendo aclaraciÃ³n simple")

            # Crear respuesta de aclaraciÃ³n simple
            from app.core.ai.interpretation.base_interpreter import InterpretationResult

            human_response = f"ðŸ¤” Tu consulta '{context.user_message}' no es lo suficientemente clara para mÃ­. Â¿PodrÃ­as ser mÃ¡s especÃ­fico sobre quÃ© informaciÃ³n necesitas?"

            return InterpretationResult(
                action="aclaracion_requerida",
                parameters={
                    "message": human_response,
                    "original_query": context.user_message,
                    "human_response": human_response
                },
                confidence=intention.confidence
            )

        except Exception as e:
            self.logger.error(f"âŒ Error manejando consulta ambigua: {e}")
            return None



    def _get_specialist_for_intention(self, intention_type: str) -> str:
        """ðŸŽ¯ OBTENER ESPECIALISTA PARA INTENCIÃ“N"""
        for specialist, config in self.system_map.items():
            if intention_type in config["handles"]:
                return specialist
        return "Unknown"

    def _should_ask_user_about_results(self, result: 'InterpretationResult', user_query: str) -> bool:
        """
        ðŸ§  MASTER ANALIZA RESULTADOS: Â¿DeberÃ­a preguntar al usuario?
        Decide si los resultados del Student requieren aclaraciÃ³n del usuario
        """
        try:
            if not result or not result.parameters:
                return False

            row_count = result.parameters.get('row_count', 0)
            action = result.action

            # ðŸš¨ CASOS DONDE EL MASTER DEBERÃA PREGUNTAR:

            # 1. Constancias con mÃºltiples candidatos
            if 'constancia' in user_query.lower() and row_count > 1:
                self.logger.info(f"ðŸ”„ [MASTER] Constancia con {row_count} candidatos - necesita selecciÃ³n")
                return True

            # 2. BÃºsquedas muy amplias (mÃ¡s de 50 resultados)
            if 'buscar' in user_query.lower() and row_count > 50:
                self.logger.info(f"ðŸ”„ [MASTER] BÃºsqueda muy amplia ({row_count} resultados) - ofrecer filtros")
                return True

            # 3. Sin resultados - ofrecer ayuda
            if row_count == 0:
                self.logger.info(f"ðŸ”„ [MASTER] Sin resultados - ofrecer alternativas")
                return True

            # Para bÃºsquedas normales como "buscar garcia" con 21 resultados: NO preguntar
            self.logger.info(f"ðŸ”„ [MASTER] Resultados normales ({row_count}) - mostrar directamente")
            return False

        except Exception as e:
            self.logger.error(f"Error analizando si preguntar al usuario: {e}")
            return False

    def _handle_results_analysis(self, context, intention, result: 'InterpretationResult') -> 'InterpretationResult':
        """
        ðŸ§  MASTER MANEJA ANÃLISIS DE RESULTADOS
        Procesa los resultados del Student y decide quÃ© preguntar al usuario
        """
        try:
            self.logger.info("ðŸ”„ [MASTER] Analizando resultados para comunicaciÃ³n")

            row_count = result.parameters.get('row_count', 0)
            user_query = context.user_message

            # Determinar tipo de pregunta basado en los resultados
            if 'constancia' in user_query.lower() and row_count > 1:
                return self._create_candidate_selection_question(result, context)
            elif 'buscar' in user_query.lower() and row_count > 50:
                return self._create_filter_suggestion_question(result, context)
            elif row_count == 0:
                return self._create_no_results_help_question(result, context)
            else:
                # No deberÃ­a llegar aquÃ­, pero por seguridad
                return result

        except Exception as e:
            self.logger.error(f"Error analizando resultados: {e}")
            return result

    def _create_candidate_selection_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta para seleccionar candidato para constancia"""
        try:
            data = result.parameters.get('data', [])
            candidates = []

            if isinstance(data, list):
                for item in data[:5]:  # MÃ¡ximo 5 candidatos
                    candidates.append({
                        'nombre': item.get('nombre', 'N/A'),
                        'grado': f"{item.get('grado', 'N/A')}Â°{item.get('grupo', '')}"
                    })
            elif isinstance(data, dict):
                # Si data es un diccionario, tratarlo como un solo candidato
                candidates.append({
                    'nombre': data.get('nombre', 'N/A'),
                    'grado': f"{data.get('grado', 'N/A')}Â°{data.get('grupo', '')}"
                })

            message = f"ðŸ” EncontrÃ© {len(data)} candidatos para la constancia. Â¿CuÃ¡l necesitas?\n\n"
            for i, candidate in enumerate(candidates, 1):
                message += f"**{i}.** {candidate['nombre']} ({candidate['grado']})\n"

            message += f"\nðŸ’¡ Responde con el nÃºmero de la opciÃ³n que necesitas."

            return InterpretationResult(
                action="solicitar_seleccion_constancia",
                parameters={
                    "message": message,
                    "candidates": candidates,
                    "original_query": context.user_message,
                    "waiting_for": "selection",
                    "original_data": data
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de selecciÃ³n: {e}")
            return result

    def _create_filter_suggestion_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta para sugerir filtros en bÃºsquedas amplias"""
        try:
            row_count = result.parameters.get('row_count', 0)

            message = f"ðŸ” EncontrÃ© {row_count} resultados. Â¿Buscabas a todos o necesitas filtrar por algo especÃ­fico como grado, grupo o turno?"

            return InterpretationResult(
                action="solicitar_filtros",
                parameters={
                    "message": message,
                    "original_query": context.user_message,
                    "result_count": row_count,
                    "waiting_for": "filter_specification",
                    "original_data": result.parameters.get('data', [])
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de filtros: {e}")
            return result

    def _create_no_results_help_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta de ayuda cuando no hay resultados"""
        try:
            message = f"ðŸ¤” No encontrÃ© resultados para '{context.user_message}'. Â¿Quieres que busque con otros criterios o necesitas ayuda?"

            return InterpretationResult(
                action="solicitar_ayuda_busqueda",
                parameters={
                    "message": message,
                    "original_query": context.user_message,
                    "waiting_for": "search_help"
                },
                confidence=0.8
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de ayuda: {e}")
            return result

    def _log_strategic_context(self):
        """ðŸ§  [MASTER] Contexto estratÃ©gico del sistema"""
        try:
            self.logger.info("ðŸ§  [MASTER] Sistema Master-Student inicializado")
            self.logger.info(f"ðŸ§  [MASTER] Especialistas disponibles: {list(self.system_map.keys())}")
            self.logger.info("ðŸ§  [MASTER] Listo para procesar consultas")



        except Exception as e:
            self.logger.error(f"âŒ Error mostrando contexto detallado: {e}")

    def _handle_expected_response(self, context: InterpretationContext, conversation_state: dict) -> Optional[InterpretationResult]:
        """
        Maneja respuestas esperadas basadas en el estado conversacional
        VersiÃ³n simplificada que usa el Context Manager
        """
        waiting_for = conversation_state.get('waiting_for')
        user_message = context.user_message.lower().strip()

        # ðŸ†• DETECTAR CONFIRMACIONES DESDE CONFIGURACIÃ“N CENTRALIZADA
        confirmations = Config.RESPONSES['confirmation_words']

        if waiting_for == "confirmacion_constancia_estudios" and user_message in confirmations:
            return self._handle_confirmation(context, conversation_state)

        # AquÃ­ se pueden agregar otros tipos de respuestas esperadas
        # elif waiting_for == "seleccion_alumno":
        #     return self._handle_selection(context, conversation_state)

        return None

    def _generate_master_response(self, student_result: 'InterpretationResult', user_query: str) -> 'InterpretationResult':
        """
        ðŸŽ¯ MASTER COMO VOCERO: Genera respuesta final basada en reporte del Student

        Args:
            student_result: Resultado tÃ©cnico del Student
            user_query: Consulta original del usuario

        Returns:
            InterpretationResult con respuesta final del Master
        """
        try:
            self.logger.info("ðŸ—£ï¸ [MASTER] Generando respuesta final como vocero...")

            # Extraer datos tÃ©cnicos del Student
            student_data = student_result.parameters
            action_used = student_result.action

            # ðŸ”§ DEBUG: Mostrar reporte recibido del Student
            self._debug_pause("ðŸ“¥ [MASTER] RECIBIENDO REPORTE DEL STUDENT", {
                "action_recibida": action_used,
                "datos_tecnicos": list(student_data.keys()),
                "row_count": student_data.get('row_count', 0),
                "requiere_respuesta_master": student_data.get('requires_master_response', False),
                "sql_ejecutado": student_data.get('sql_executed', '')[:50] + "..." if student_data.get('sql_executed') else "N/A"
            })

            # ðŸŽ¯ EXTRAER CRITERIOS DE BÃšSQUEDA DINÃMICAMENTE DESPUÃ‰S DE LA EJECUCIÃ“N
            search_criteria = self._extract_search_criteria_for_display(student_data)

            # ðŸŽ¯ AGREGAR CRITERIOS A STUDENT_DATA PARA LAS FUNCIONES DE RESPUESTA
            student_data["search_criteria"] = search_criteria

            # ðŸŽ¯ MASTER GENERA RESPUESTA FINAL USANDO PROMPT ESPECIALIZADO
            self._debug_pause("ðŸ§  [MASTER] INTERPRETANDO REPORTE Y GENERANDO RESPUESTA", {
                "tipo_consulta": self._detect_query_type(action_used, student_data, user_query),
                "criterios_busqueda": len(search_criteria),
                "datos_disponibles": student_data.get('row_count', 0),
                "prompt_especializado": "Generando respuesta contextual con LLM"
            })

            master_response = self._generate_master_response_with_llm(student_data, user_query, action_used)

            # ðŸ”§ CASOS ESPECIALES QUE REQUIEREN PROCESAMIENTO ADICIONAL
            if action_used == "seleccion_realizada":
                # Respuesta de selecciÃ³n - mostrar datos del elemento seleccionado
                elemento_seleccionado = student_data.get("elemento_seleccionado")
                posicion = student_data.get("posicion", "N/A")

                if elemento_seleccionado:
                    # Preparar datos para mostrar en la interfaz
                    nombre = elemento_seleccionado.get('nombre', 'N/A')
                    master_response = f"ðŸ‘¤ InformaciÃ³n del alumno en posiciÃ³n {posicion}: **{nombre}**"

                    # Agregar los datos del elemento seleccionado para que se muestren en la interfaz
                    student_data["data"] = [elemento_seleccionado]
                    student_data["row_count"] = 1
                    student_data["human_response"] = master_response
                else:
                    master_response = student_data.get("message", "SelecciÃ³n procesada exitosamente")

            elif action_used == "transformation_preview":
                # ðŸ”„ RESPUESTA ESPECÃFICA PARA TRANSFORMACIONES (mantener por ahora)
                transformation_info = student_data.get("transformation_info", {})
                if transformation_info:
                    tipo_constancia = (transformation_info.get("tipo_constancia") or
                                     transformation_info.get("tipo_transformacion") or
                                     student_data.get("tipo_constancia", "constancia"))
                    alumno_info = (transformation_info.get("alumno") or
                                 student_data.get("alumno", {}))
                    alumno_nombre = alumno_info.get("nombre", "el alumno")

                    master_response = (f"âœ… **TransformaciÃ³n completada exitosamente**\n\n"
                                     f"He convertido tu PDF a una constancia de **{tipo_constancia}** para **{alumno_nombre}**.\n\n"
                                     f"ðŸ“„ **En el panel derecho puedes:**\n\n"
                                     f"Ver la vista previa, comparar con el original, revisar datos extraÃ­dos y abrir en navegador para imprimir.\n\n"
                                     f"ðŸ’¡ Â¿Necesitas hacer algÃºn ajuste o tienes otra consulta?")

            # ðŸŽ¯ CREAR RESULTADO FINAL CON RESPUESTA DEL MASTER
            final_result = InterpretationResult(
                action=student_result.action,
                parameters={
                    **student_data,  # Mantener datos tÃ©cnicos del Student
                    "human_response": master_response,  # Respuesta final del Master
                    "master_generated": True,  # Flag para indicar que Master generÃ³ la respuesta
                    "student_action": action_used,  # AcciÃ³n original del Student
                    "search_criteria": search_criteria,  # ðŸ†• Criterios para mostrar en listado
                },
                confidence=student_result.confidence
            )

            self.logger.info(f"âœ… [MASTER] Respuesta final: '{master_response[:50]}...'")
            return final_result

        except Exception as e:
            self.logger.error(f"âŒ [MASTER] Error generando respuesta final: {e}")
            # Fallback: retornar resultado original del Student
            return student_result

    def _extract_search_criteria_for_display(self, student_data: dict) -> dict:
        """ðŸŽ¯ EXTRAE CRITERIOS DE BÃšSQUEDA DINÃMICAMENTE DEL SQL EJECUTADO"""
        try:
            # ðŸ§  [MASTER] Analizando SQL ejecutado para extraer criterios
            sql_query = student_data.get("sql_executed", "") or student_data.get("sql_query", "")
            search_description = ""
            relevant_fields = []

            self.logger.info(f"ðŸ§  [MASTER] SQL encontrado: '{sql_query[:50]}...'" if sql_query else "ðŸ§  [MASTER] No hay SQL disponible")

            if sql_query:
                # Extraer campos de WHERE clause dinÃ¡micamente
                import re

                # ðŸŽ¯ PATRONES COMPLETOS PARA TODOS LOS CRITERIOS POSIBLES
                where_patterns = [
                    # ðŸ“… FECHAS
                    (r'fecha_nacimiento\s+LIKE\s+[\'"]%(\d{4})%[\'"]', 'fecha_nacimiento', 'nacidos en {}'),
                    (r'fecha_nacimiento\s+BETWEEN\s+[\'"](\d{4}-\d{2}-\d{2})[\'"].*[\'"](\d{4}-\d{2}-\d{2})[\'"]', 'fecha_nacimiento', 'nacidos entre {} y {}'),
                    (r'fecha_nacimiento\s*=\s*[\'"]([^\'\"]+)[\'"]', 'fecha_nacimiento', 'nacidos el {}'),

                    # ðŸŽ“ DATOS ESCOLARES
                    (r'grado\s*=\s*[\'"](\w+)[\'"]', 'grado', '{}Â° grado'),
                    (r'grupo\s*=\s*[\'"](\w+)[\'"]', 'grupo', 'grupo {}'),
                    (r'turno\s*=\s*[\'"](\w+)[\'"]', 'turno', 'turno {}'),

                    # ðŸ‘¤ IDENTIFICADORES
                    (r'matricula\s*=\s*[\'"]([^\'\"]+)[\'"]', 'matricula', 'matrÃ­cula {}'),
                    (r'curp\s*=\s*[\'"]([^\'\"]+)[\'"]', 'curp', 'CURP {}'),
                    (r'nombre\s+LIKE\s+[\'"]%([^%\'\"]+)%[\'"]', 'nombre', 'con nombre que contiene "{}"'),
                    (r'nombre\s*=\s*[\'"]([^\'\"]+)[\'"]', 'nombre', 'llamado {}'),

                    # ðŸ“Š CALIFICACIONES
                    (r'calificaciones\s+IS\s+NOT\s+NULL', 'calificaciones_status', 'con calificaciones'),
                    (r'calificaciones\s+IS\s+NULL', 'calificaciones_status', 'sin calificaciones'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*>\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio mayor a {}'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*<\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio menor a {}'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*=\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio de {}'),

                    # ðŸ  DATOS PERSONALES
                    (r'telefono\s*=\s*[\'"]([^\'\"]+)[\'"]', 'telefono', 'con telÃ©fono {}'),
                    (r'direccion\s+LIKE\s+[\'"]%([^%\'\"]+)%[\'"]', 'direccion', 'que viven en {}'),
                    (r'email\s*=\s*[\'"]([^\'\"]+)[\'"]', 'email', 'con email {}'),

                    # ðŸ”¢ RANGOS NUMÃ‰RICOS
                    (r'edad\s*>\s*(\d+)', 'edad', 'mayores de {} aÃ±os'),
                    (r'edad\s*<\s*(\d+)', 'edad', 'menores de {} aÃ±os'),
                    (r'edad\s*=\s*(\d+)', 'edad', 'de {} aÃ±os'),
                ]

                for pattern, field, description_template in where_patterns:
                    matches = re.findall(pattern, sql_query, re.IGNORECASE)
                    if matches:
                        relevant_fields.append(field)
                        if isinstance(matches[0], tuple):
                            # MÃºltiples grupos (ej: BETWEEN)
                            search_description += description_template.format(*matches[0]) + " "
                        else:
                            # Un solo grupo
                            search_description += description_template.format(matches[0]) + " "

                self.logger.info(f"ðŸ§  [MASTER] Criterios extraÃ­dos: {len(relevant_fields)} campos")
                if search_description.strip():
                    self.logger.info(f"ðŸ§  [MASTER] DescripciÃ³n: {search_description.strip()}")

            # Si no hay SQL o no se encontraron patrones, usar fallback inteligente
            if not relevant_fields:
                # Analizar parÃ¡metros de la acciÃ³n como fallback
                action_params = student_data.get("action_params", {})
                criterio_principal = action_params.get("criterio_principal", {})
                campo = criterio_principal.get("campo", "")

                if campo:
                    relevant_fields.append(campo)
                    search_description = f"bÃºsqueda por {campo}"
                    self.logger.info(f"ðŸ§  [MASTER] Campo principal: {campo}")

            # Incluir campos bÃ¡sicos dinÃ¡micamente desde configuraciÃ³n
            from app.core.config import Config
            basic_fields = getattr(Config, 'BASIC_DISPLAY_FIELDS', ['nombre', 'curp'])
            all_fields = basic_fields + [field for field in relevant_fields if field not in basic_fields]

            return {
                "fields_to_show": all_fields,
                "search_description": search_description.strip(),
                "has_specific_criteria": len(relevant_fields) > 0
            }

        except Exception as e:
            self.logger.error(f"Error extrayendo criterios dinÃ¡micamente: {e}")
            return {
                "fields_to_show": ['nombre', 'curp', 'turno'],  # Fallback por defecto
                "search_description": "",
                "has_specific_criteria": False
            }




    def _generate_master_response_with_llm(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        ðŸ—£ï¸ MASTER GENERA RESPUESTA HUMANIZADA CON CONTEXTO CONVERSACIONAL

        MEJORA: Ahora incluye contexto conversacional para respuestas contextuales
        El Master usa su propio prompt especializado en comunicaciÃ³n para generar
        respuestas humanizadas basÃ¡ndose en los datos tÃ©cnicos del Student.
        """
        try:
            # ðŸŽ¯ OBTENER CONTEXTO CONVERSACIONAL COMPLETO (IGUAL QUE MASTER INICIAL)
            conversation_stack = getattr(self, 'current_conversation_stack', [])
            context_info = self._create_context_summary(conversation_stack)

            # Crear prompt con contexto completo
            master_prompt = self._create_master_response_prompt_with_context(
                student_data, user_query, action_used, context_info
            )

            # Llamar al LLM para generar respuesta humanizada
            response = self.gemini_client.send_prompt_sync(master_prompt)

            if response and response.strip():
                self.logger.info(f"âœ… Master generÃ³ respuesta contextual exitosamente")
                return response.strip()
            else:
                self.logger.warning(f"âŒ Master LLM no generÃ³ respuesta, usando fallback")
                return self._generate_fallback_response(student_data, action_used)

        except Exception as e:
            self.logger.error(f"Error generando respuesta con Master LLM: {e}")
            return self._generate_fallback_response(student_data, action_used)

    def _generate_fallback_response(self, student_data: dict, action_used: str) -> str:
        """Genera respuesta de fallback si el LLM del Master falla"""
        row_count = student_data.get("row_count", 0)

        if action_used in ["BUSCAR_UNIVERSAL", "GENERAR_LISTADO_COMPLETO"]:
            if row_count == 0:
                return "No encontrÃ© resultados para tu bÃºsqueda. Â¿PodrÃ­as intentar con otros criterios?"
            elif row_count == 1:
                return "EncontrÃ© un resultado que coincide con tu bÃºsqueda."
            else:
                return f"EncontrÃ© {row_count} resultados que coinciden con tu bÃºsqueda."
        elif action_used in ["CONTAR_ALUMNOS", "CONTAR_UNIVERSAL"]:
            # Extraer el valor real del conteo desde los datos
            data = student_data.get("data", [])
            if data and isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
                total_count = data[0].get("total", row_count)
                return f"El conteo se completÃ³ exitosamente: {total_count} elementos."
            else:
                return f"El conteo se completÃ³ exitosamente: {row_count} elementos."
        else:
            return "Consulta procesada exitosamente."

    def _create_master_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        ðŸŽ¯ CREA PROMPT ESPECIALIZADO DINÃMICO SEGÃšN TIPO DE CONSULTA

        Diferentes tipos de consulta requieren diferentes enfoques de respuesta.
        INCLUYE INTERPRETACIÃ“N INTELIGENTE DEL REPORTE DEL STUDENT.
        """
        # ðŸ§  INTERPRETACIÃ“N INTELIGENTE DEL REPORTE
        intelligent_interpretation = self._interpret_student_report_intelligently(student_data, user_query)

        # ðŸ›‘ PAUSA ESTRATÃ‰GICA #5: INTERPRETACIÃ“N INTELIGENTE DEL REPORTE
        import os
        if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
            print(f"\nðŸ›‘ [MASTER-BRAIN] PASO 5: INTERPRETACIÃ“N INTELIGENTE DEL REPORTE")
            print(f"    â”œâ”€â”€ ðŸ§  PREGUNTA: Â¿QuÃ© pasÃ³ y cÃ³mo respondo al usuario?")
            print(f"    â”œâ”€â”€ ðŸ“ Consulta original: '{user_query}'")
            print(f"    â”œâ”€â”€ âš¡ AcciÃ³n ejecutada por Student: {action_used}")
            print(f"    â”œâ”€â”€ ðŸ“Š Resultados obtenidos: {student_data.get('row_count', 0)} elementos")
            print(f"    â”œâ”€â”€ âœ… Ã‰xito de la operaciÃ³n: {student_data.get('success', True)}")
            print(f"    â”œâ”€â”€ ðŸ§  InterpretaciÃ³n del conocimiento:")
            print(f"    â”‚   {intelligent_interpretation}")
            print(f"    â”œâ”€â”€ âš¡ DECISIÃ“N: Generar respuesta humanizada para el usuario")
            print(f"    â””â”€â”€ Presiona ENTER para generar respuesta final...")
            input()

        # Detectar tipo de consulta
        query_type = self._detect_query_type(action_used, student_data, user_query)

        # Crear prompt especÃ­fico segÃºn el tipo
        base_prompt = ""
        if query_type == "search":
            base_prompt = self._create_search_response_prompt(student_data, user_query, action_used)
        elif query_type == "constancia":
            base_prompt = self._create_constancia_response_prompt(student_data, user_query, action_used)
        elif query_type == "transformation":
            base_prompt = self._create_transformation_response_prompt(student_data, user_query, action_used)
        elif query_type == "statistics":
            base_prompt = self._create_statistics_response_prompt(student_data, user_query, action_used)
        elif query_type == "help":
            base_prompt = self._create_help_response_prompt(student_data, user_query, action_used)
        else:
            base_prompt = self._create_generic_response_prompt(student_data, user_query, action_used)

        # ðŸ§  AGREGAR INTERPRETACIÃ“N INTELIGENTE AL PROMPT
        enhanced_prompt = f"""
{base_prompt}

ðŸ§  INTERPRETACIÃ“N INTELIGENTE DEL MASTER:
{intelligent_interpretation}

ðŸŽ¯ INSTRUCCIONES ADICIONALES:
- Usa la interpretaciÃ³n inteligente para mejorar tu respuesta
- Si hay sugerencias, incorpÃ³ralas naturalmente
- Si hay limitaciones, explÃ­calas de manera empÃ¡tica
- MantÃ©n un tono profesional pero amigable

RESPONDE ÃšNICAMENTE con la respuesta conversacional final mejorada.
"""

        return enhanced_prompt

    def _create_master_response_prompt_with_context(self, student_data: dict, user_query: str, action_used: str, context_info: str) -> str:
        """
        ðŸ—£ï¸ MASTER RESPUESTA CON CONTEXTO CONVERSACIONAL

        NUEVO: Prompt de respuesta que incluye contexto conversacional completo
        """
        # Detectar tipo de consulta
        query_type = self._detect_query_type(action_used, student_data, user_query)

        # Crear prompt base segÃºn el tipo
        base_prompt = self._create_master_response_prompt(student_data, user_query, action_used)

        # Agregar contexto conversacional al prompt
        contextual_prompt = f"""
ðŸ—£ï¸ MASTER COMO VOCERO - RESPUESTA CONTEXTUAL INTELIGENTE

CONTEXTO CONVERSACIONAL:
{context_info}

CONSULTA ORIGINAL: "{user_query}"
RESULTADO DEL STUDENT: {action_used} - {student_data.get('row_count', 0)} resultados

ðŸŽ¯ GENERAR RESPUESTA NATURAL Y CONTEXTUAL:
1. Reconocer contexto conversacional cuando sea relevante
2. Comunicar resultado de manera clara
3. Mantener personalidad consistente
4. Conectar con consultas anteriores cuando sea natural

EJEMPLOS:
- ContinuaciÃ³n: "Perfecto! BasÃ¡ndome en [contexto]..."
- Referencia resuelta: "He generado la constancia para [nombre resuelto]..."
- Filtro aplicado: "De los resultados anteriores, encontrÃ©..."

{base_prompt}

IMPORTANTE: Si hay contexto conversacional relevante, conÃ©ctalo naturalmente en tu respuesta.
"""

        return contextual_prompt

    def _detect_query_type(self, action_used: str, student_data: dict, user_query: str) -> str:
        """Detecta el tipo especÃ­fico de consulta para usar el prompt correcto"""
        # ðŸŽ¯ AYUDA DEL SISTEMA (SIMPLIFICADO A 2 ACCIONES)
        if action_used in ["AYUDA_CAPACIDADES", "AYUDA_TUTORIAL"] or student_data.get("query_category") == "ayuda_sistema":
            return "help"

        # Constancias
        if action_used in ["constancia_generada", "PREPARAR_DATOS_CONSTANCIA"] or "constancia" in user_query.lower():
            return "constancia"

        # Transformaciones
        if action_used in ["transformation_completed", "transformation_preview"] or "transform" in user_query.lower():
            return "transformation"

        # EstadÃ­sticas
        if action_used in ["CONTAR_ALUMNOS", "CONTAR_UNIVERSAL", "CALCULAR_ESTADISTICA"] or any(word in user_query.lower() for word in ["cuÃ¡ntos", "total", "estadÃ­stica", "conteo"]):
            return "statistics"

        # BÃºsquedas (por defecto)
        return "search"

    def _create_search_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para bÃºsquedas y listados"""
        row_count = student_data.get("row_count", 0)
        data = student_data.get("data", [])
        ambiguity_level = student_data.get("ambiguity_level", "low")

        # ðŸŽ¯ MANEJO INTELIGENTE DE "NO ENCONTRADO" (row_count = 0)
        if row_count == 0:
            return self._create_no_results_response_prompt(student_data, user_query, action_used)

        # ðŸ”§ MANEJO SEGURO DE DATOS - verificar que data sea una lista
        data_context = ""
        if data and isinstance(data, list) and len(data) > 0:
            if len(data) <= 3:
                data_context = "RESULTADOS ENCONTRADOS:\n"
                for i, item in enumerate(data, 1):
                    if isinstance(item, dict):
                        # ðŸŽ¯ MANEJO DINÃMICO DE CAMPOS - usar los campos que realmente existen
                        item_info = []
                        if "nombre" in item:
                            item_info.append(item["nombre"])
                        if "matricula" in item:
                            item_info.append(f"MatrÃ­cula: {item['matricula']}")
                        if "grado" in item and "grupo" in item:
                            item_info.append(f"{item['grado']}Â° {item['grupo']}")
                        elif "grado" in item:
                            item_info.append(f"Grado: {item['grado']}")
                        elif "grupo" in item:
                            item_info.append(f"Grupo: {item['grupo']}")
                        if "curp" in item:
                            item_info.append(f"CURP: {item['curp']}")

                        # Si no hay campos reconocidos, mostrar todos los disponibles
                        if not item_info:
                            item_info = [f"{k}: {v}" for k, v in item.items() if v is not None]

                        data_context += f"{i}. {' - '.join(item_info)}\n"
            else:
                data_context = f"PRIMEROS 3 DE {len(data)} RESULTADOS:\n"
                for i in range(min(3, len(data))):
                    item = data[i]
                    if isinstance(item, dict):
                        # ðŸŽ¯ MANEJO DINÃMICO DE CAMPOS - usar los campos que realmente existen
                        item_info = []
                        if "nombre" in item:
                            item_info.append(item["nombre"])
                        if "matricula" in item:
                            item_info.append(f"MatrÃ­cula: {item['matricula']}")
                        if "grado" in item and "grupo" in item:
                            item_info.append(f"{item['grado']}Â° {item['grupo']}")
                        elif "grado" in item:
                            item_info.append(f"Grado: {item['grado']}")
                        elif "grupo" in item:
                            item_info.append(f"Grupo: {item['grupo']}")
                        if "curp" in item:
                            item_info.append(f"CURP: {item['curp']}")

                        # Si no hay campos reconocidos, mostrar todos los disponibles
                        if not item_info:
                            item_info = [f"{k}: {v}" for k, v in item.items() if v is not None]

                        data_context += f"{i+1}. {' - '.join(item_info)}\n"

        # Detectar si hay contexto conversacional previo
        reflexion = student_data.get("auto_reflexion", {})
        datos_recordar = reflexion.get("datos_recordar", {})
        conversation_context = datos_recordar.get("context", "")
        query_anterior = datos_recordar.get("query", "")

        # ðŸŽ¯ MEJORAR DETECCIÃ“N DE CONTINUACIÃ“N
        # Verificar mÃºltiples fuentes para detectar continuaciÃ³n
        master_intention = student_data.get("master_intention", {})
        categoria_master = master_intention.get("categoria", "")

        # Es continuaciÃ³n si:
        # 1. Hay contexto conversacional previo, O
        # 2. El Master categorizÃ³ como "continuacion", O
        # 3. La consulta tiene palabras de referencia contextual
        palabras_continuacion = ["ellos", "esos", "esas", "de ellos", "de esas", "ahora", "tambiÃ©n"]
        tiene_referencia = any(palabra in user_query.lower() for palabra in palabras_continuacion)

        es_continuacion = bool(
            conversation_context or
            query_anterior or
            categoria_master == "continuacion" or
            tiene_referencia
        )

        return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ðŸ«

ðŸŽ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- RESULTADOS: {row_count} estudiantes encontrados
- AMBIGÃœEDAD: {ambiguity_level}
- ES CONTINUACIÃ“N: {es_continuacion}
- CONTEXTO PREVIO: {conversation_context}
- CONSULTA ANTERIOR: {query_anterior}

{data_context}

ðŸŽ­ TU PERSONALIDAD:
- Entusiasta y humano (usa emojis apropiados)
- Profesional pero cercano
- EmpÃ¡tico y comprensivo
- Proactivo en sugerencias

ðŸŽ¯ TU TAREA PARA BÃšSQUEDAS:
Generar una respuesta HUMANA y CONECTADA que:

1. ðŸŽ‰ SALUDA con entusiasmo apropiado
2. ðŸ“Š PRESENTA los resultados de manera atractiva
3. ðŸ” EXPLICA quÃ© buscaste de forma natural
4. ðŸ¤” MANEJA la ambigÃ¼edad con empatÃ­a
5. ðŸ’¡ SUGIERE prÃ³ximos pasos Ãºtiles
6. ðŸ”„ CONECTA con el contexto conversacional si existe

ðŸŽ¯ MANEJO DE AMBIGÃœEDAD CON EMPATÃA:
- HIGH (10+ resultados): "Â¡EncontrÃ© muchos estudiantes! ðŸ˜Š Como [apellido] es comÃºn, te muestro todos para que encuentres al que necesitas. Â¿PodrÃ­as ser mÃ¡s especÃ­fico con el nombre o grado?"
- MEDIUM (4-9 resultados): "Â¡Perfecto! ðŸ‘ EncontrÃ© varios estudiantes que coinciden. Â¿Necesitas informaciÃ³n especÃ­fica de alguno?"
- LOW (1-3 resultados): "Â¡Excelente! âœ… AquÃ­ tienes [lo que encontrÃ©]..."

ðŸ”„ CONTINUIDAD CONVERSACIONAL (MUY IMPORTANTE):
- Si ES_CONTINUACIÃ“N = True: NUNCA digas "Â¡Hola!" - usa "Â¡Perfecto! ðŸ‘", "Â¡Excelente! âœ…", "Siguiendo con tu bÃºsqueda anterior..."
- Si ES_CONTINUACIÃ“N = False: Puedes saludar con "Â¡Hola! ðŸ‘‹"
- SIEMPRE conecta con la consulta anterior cuando hay contexto
- Menciona especÃ­ficamente quÃ© filtros se aplicaron sobre los datos previos

âœ… ENFOQUE HUMANO:
- Resultados presentados con entusiasmo
- Criterios explicados naturalmente
- Sugerencias Ãºtiles y empÃ¡ticas
- Tono conversacional y amigable

ðŸ“ FORMATO HUMANO:
- Saludo apropiado con emoji
- MÃ¡ximo 3-4 lÃ­neas pero con personalidad
- Cierre que invite a continuar la conversaciÃ³n

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_no_results_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        ðŸŽ¯ PROMPT ESPECIALIZADO PARA CASOS DE "NO ENCONTRADO" (row_count = 0)

        Master interpreta inteligentemente cuando Student reporta 0 resultados
        y genera respuestas humanas con sugerencias Ãºtiles.
        """
        # Detectar tipo de bÃºsqueda para respuesta especÃ­fica
        search_criteria = student_data.get("search_criteria", "")

        # Detectar si es bÃºsqueda por CURP
        is_curp_search = "curp" in user_query.lower() or "curp" in search_criteria.lower()

        # Detectar si es bÃºsqueda por matrÃ­cula
        is_matricula_search = "matrÃ­cula" in user_query.lower() or "matricula" in user_query.lower()

        # Detectar si es bÃºsqueda por nombre
        is_name_search = any(word in user_query.lower() for word in ["buscar", "encontrar", "dame", "informaciÃ³n"]) and not is_curp_search and not is_matricula_search

        # Detectar contexto conversacional
        reflexion = student_data.get("auto_reflexion", {})
        datos_recordar = reflexion.get("datos_recordar", {})
        conversation_context = datos_recordar.get("context", "")
        es_continuacion = bool(conversation_context)

        return f"""
Eres el asistente empÃ¡tico y Ãºtil de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ðŸ«

ðŸŽ¯ SITUACIÃ“N CRÃTICA - NO SE ENCONTRARON RESULTADOS:
- CONSULTA: "{user_query}"
- CRITERIOS BUSCADOS: {search_criteria}
- RESULTADOS: 0 estudiantes encontrados
- ES CONTINUACIÃ“N: {es_continuacion}
- CONTEXTO PREVIO: {conversation_context}
- BÃšSQUEDA POR CURP: {is_curp_search}
- BÃšSQUEDA POR MATRÃCULA: {is_matricula_search}
- BÃšSQUEDA POR NOMBRE: {is_name_search}

ðŸŽ­ TU PERSONALIDAD EMPÃTICA:
- Comprensivo y Ãºtil (NO frustrante)
- Profesional pero humano
- Proactivo en soluciones
- Educativo sin ser condescendiente

ðŸŽ¯ TU TAREA ESPECÃFICA PARA "NO ENCONTRADO":
Generar una respuesta EMPÃTICA Y ÃšTIL que:

1. ðŸ¤” RECONOCE que no encontraste nada (sin culpar al usuario)
2. ðŸ’¡ EXPLICA posibles causas de manera educativa
3. ðŸ” SUGIERE alternativas especÃ­ficas y Ãºtiles
4. ðŸŽ¯ OFRECE prÃ³ximos pasos concretos
5. ðŸ”„ MANTIENE continuidad conversacional si existe

ðŸŽ¯ RESPUESTAS ESPECÃFICAS POR TIPO DE BÃšSQUEDA:

**Para BÃšSQUEDAS POR CURP:**
- "No encontrÃ© ningÃºn alumno con esa CURP en nuestra base de datos."
- "Las CURPs tienen exactamente 18 caracteres. Â¿PodrÃ­as verificar que estÃ© completa?"
- "TambiÃ©n puedes buscar por nombre si prefieres: 'buscar [nombre del alumno]'"

**Para BÃšSQUEDAS POR MATRÃCULA:**
- "No encontrÃ© esa matrÃ­cula en nuestros registros."
- "Â¿PodrÃ­as verificar el nÃºmero? TambiÃ©n puedes buscar por nombre del alumno."

**Para BÃšSQUEDAS POR NOMBRE:**
- "No encontrÃ© ningÃºn alumno con ese nombre."
- "Â¿PodrÃ­as intentar con el apellido? Por ejemplo: 'buscar GarcÃ­a'"
- "O puedes ser mÃ¡s especÃ­fico: 'buscar MarÃ­a GarcÃ­a de 3er grado'"

ðŸ”„ CONTINUIDAD CONVERSACIONAL:
- Si ES_CONTINUACIÃ“N = True: "Siguiendo con tu bÃºsqueda anterior, no encontrÃ©..."
- Si ES_CONTINUACIÃ“N = False: "No encontrÃ©..."
- SIEMPRE ofrecer alternativas basadas en el contexto

âœ… PATRONES DE RESPUESTA EMPÃTICA:
- "No encontrÃ© [lo que buscaste], pero puedes intentar..."
- "Hmm, no hay resultados para [criterio]. Â¿Te ayudo de otra forma?"
- "No localicÃ© [lo especÃ­fico]. Â¿Quieres que busque por [alternativa]?"

âŒ EVITA RESPUESTAS TÃ‰CNICAS O FRÃAS:
- "0 resultados encontrados"
- "La consulta no devolviÃ³ datos"
- "No hay coincidencias en la base de datos"

ðŸ“ FORMATO EMPÃTICO Y ÃšTIL:
- Reconocimiento empÃ¡tico del problema
- ExplicaciÃ³n breve y educativa
- 2-3 sugerencias concretas y especÃ­ficas
- InvitaciÃ³n amigable a continuar
- MÃ¡ximo 4-5 lÃ­neas con personalidad humana

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_constancia_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para constancias generadas"""
        # ðŸ”§ MANEJO SEGURO DE DATOS - puede ser lista o string
        data = student_data.get("data", [])
        if isinstance(data, list) and len(data) > 0:
            alumno_data = data[0]
        elif isinstance(data, dict):
            alumno_data = data
        else:
            alumno_data = {}

        # Obtener nombre del alumno de mÃºltiples fuentes posibles
        alumno_nombre = (
            alumno_data.get("nombre") or
            student_data.get("alumno", {}).get("nombre") if isinstance(student_data.get("alumno"), dict) else
            student_data.get("alumno") if isinstance(student_data.get("alumno"), str) else
            "el estudiante"
        )

        tipo_constancia = student_data.get("tipo_constancia", "constancia")

        # Detectar contexto conversacional
        reflexion = student_data.get("reflexion_conversacional", {})
        conversation_context = reflexion.get("datos_recordar", {}).get("context", "")

        return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ðŸ«

ðŸŽ¯ SITUACIÃ“N:
- CONSTANCIA GENERADA: {tipo_constancia}
- PARA ESTUDIANTE: {alumno_nombre}
- CONSULTA ORIGINAL: "{user_query}"
- CONTEXTO PREVIO: {conversation_context}

ðŸŽ­ TU PERSONALIDAD:
- Entusiasta y celebrativo (usa emojis de Ã©xito)
- Profesional pero cercano
- GuÃ­a claro y Ãºtil
- EmpÃ¡tico y comprensivo

ðŸŽ¯ TU TAREA PARA CONSTANCIAS:
Generar una respuesta HUMANA y CELEBRATIVA que:

1. ðŸŽ‰ CELEBRA el Ã©xito de la generaciÃ³n
2. ðŸ“± EXPLICA el panel derecho de manera amigable
3. ðŸŽ›ï¸ MENCIONA botones especÃ­ficos Ãºtiles
4. ðŸ’¡ GUÃA prÃ³ximos pasos claramente
5. ðŸ”„ CONECTA con contexto conversacional si existe

ðŸŽ›ï¸ FUNCIONALIDADES DEL PANEL (explica amigablemente):
- BotÃ³n superior izquierdo: "puedes abrir/cerrar el panel"
- Vista previa: "visor PDF con zoom para revisar tu constancia"
- "Ver datos del alumno": "revisa la informaciÃ³n extraÃ­da"
- "Quitar PDF": "si quieres subir otro documento"
- "Abrir navegador/imprimir": "para guardar o imprimir"
- IMPORTANTE: "Solo vista previa - para guardar usa el navegador"

ðŸ”„ CONTINUIDAD CONVERSACIONAL:
- Si hay contexto previo, reconÃ³celo: "Â¡Perfecto! Siguiendo con [contexto]..."
- Si es nueva constancia, celebra: "Â¡Excelente! ðŸŽ‰"
- Siempre invita a continuar: "Â¿Necesitas algo mÃ¡s?"

ðŸ“ FORMATO HUMANO Y CELEBRATIVO:
- ConfirmaciÃ³n entusiasta con emojis
- ExplicaciÃ³n clara pero amigable del panel
- MÃ¡ximo 4-5 lÃ­neas con personalidad
- Cierre que invite a continuar

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_transformation_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para transformaciones de PDF"""
        transformation_info = student_data.get("transformation_info", {})
        tipo_constancia = transformation_info.get("tipo_constancia", "constancia")
        alumno_info = transformation_info.get("alumno", {})
        alumno_nombre = alumno_info.get("nombre", "el alumno")

        return f"""
Eres el asistente de la escuela especializado en TRANSFORMACIÃ“N DE PDFs.

ðŸŽ¯ SITUACIÃ“N:
- TRANSFORMACIÃ“N COMPLETADA: PDF â†’ {tipo_constancia}
- PARA ESTUDIANTE: {alumno_nombre}
- CONSULTA ORIGINAL: "{user_query}"

ðŸŽ¯ TU TAREA ESPECÃFICA PARA TRANSFORMACIONES:
Generar una respuesta ENFOCADA EN COMPARACIÃ“N que:

1. âœ… CONFIRME que la transformaciÃ³n se completÃ³
2. ðŸ”„ EXPLIQUE las funciones de comparaciÃ³n
3. ðŸ“± MENCIONE botones especÃ­ficos de transformaciÃ³n
4. ðŸ’¡ GUÃE sobre cÃ³mo comparar y decidir

ðŸ”„ FUNCIONALIDADES ESPECÃFICAS DE TRANSFORMACIÃ“N:
- Todo lo del panel normal MÃS:
- "Ver PDF original": muestra el que subiste
- "Ver PDF transformado": muestra el resultado
- ComparaciÃ³n rÃ¡pida: alternar entre ambos
- Misma lÃ³gica: solo vista previa, guardar desde navegador

ðŸ“ FORMATO PARA TRANSFORMACIONES:
- ConfirmaciÃ³n de transformaciÃ³n exitosa
- ExplicaciÃ³n de comparaciÃ³n
- GuÃ­a para decidir prÃ³ximos pasos
- MÃ¡ximo 4-5 lÃ­neas

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_statistics_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para estadÃ­sticas y conteos"""
        row_count = student_data.get("row_count", 0)

        # ðŸ” DEBUG: Analizar datos recibidos del Student
        data = student_data.get("data", [])
        self.logger.info(f"ðŸ” [MASTER-STATS] Analizando datos del Student:")
        self.logger.info(f"    â”œâ”€â”€ row_count: {row_count}")
        self.logger.info(f"    â”œâ”€â”€ data type: {type(data)}")
        self.logger.info(f"    â”œâ”€â”€ data length: {len(data) if isinstance(data, list) else 'N/A'}")
        if isinstance(data, list) and len(data) > 0:
            self.logger.info(f"    â”œâ”€â”€ data[0]: {data[0]}")
            self.logger.info(f"    â””â”€â”€ data[0] keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'N/A'}")

        # ðŸŽ¯ DETECTAR TIPO DE ESTADÃSTICA BASADO EN ESTRUCTURA DE DATOS
        is_distribution = False
        total_sum = 0

        if data and isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
            # ðŸŽ¯ DETECTAR DISTRIBUCIONES: mÃºltiples registros con campo de agrupaciÃ³n + cantidad
            distribution_fields = ['grado', 'grupo', 'turno', 'ciclo_escolar']
            has_distribution_field = any(field in data[0] for field in distribution_fields)
            has_cantidad = 'cantidad' in data[0]

            if len(data) > 1 and has_distribution_field and has_cantidad:
                is_distribution = True
                total_sum = sum(item.get('cantidad', 0) for item in data)

                # Detectar tipo de distribuciÃ³n
                distribution_type = next((field for field in distribution_fields if field in data[0]), 'campo')
                self.logger.info(f"ðŸ” [MASTER-STATS] DISTRIBUCIÃ“N detectada: {len(data)} {distribution_type}s, {total_sum} alumnos total")
            else:
                # Conteo simple
                actual_count = data[0].get("total", row_count)
                self.logger.info(f"ðŸ” [MASTER-STATS] CONTEO SIMPLE: {actual_count}")
        else:
            actual_count = row_count
            self.logger.info(f"ðŸ” [MASTER-STATS] usando row_count como actual_count: {actual_count}")

        # Preparar datos para el prompt
        if is_distribution:
            # Para distribuciones, usar datos completos
            distribution_summary = f"{len(data)} grados con {total_sum} alumnos total"
            self.logger.info(f"ðŸ” [MASTER-STATS] Resumen distribuciÃ³n: {distribution_summary}")
        else:
            # Para conteos simples, usar valor individual
            total_alumnos = 211  # Valor conocido de la base de datos
            porcentaje = round((actual_count / total_alumnos) * 100, 1) if total_alumnos > 0 else 0

        if is_distribution:
            # ðŸŽ¯ DETECTAR TIPO DE DISTRIBUCIÃ“N (SIN INCLUIR DATOS EN EL PROMPT)
            distribution_type = next((field for field in ['grado', 'grupo', 'turno', 'ciclo_escolar'] if field in data[0]), 'campo')

            return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ðŸ«

ðŸŽ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- TIPO: DISTRIBUCIÃ“N por {distribution_type}s
- RESULTADOS: {len(data)} {distribution_type}s diferentes
- TOTAL ALUMNOS: {total_sum} estudiantes

ðŸŽ­ TU PERSONALIDAD:
- Entusiasta y humano (usa emojis apropiados)
- Profesional pero cercano
- EmpÃ¡tico y comprensivo
- Celebra los datos interesantes

ðŸŽ¯ TU TAREA PARA DISTRIBUCIONES:
Generar una respuesta HUMANA y ENTUSIASTA que:

1. ðŸŽ‰ SALUDA con entusiasmo apropiado
2. ðŸ“Š PRESENTA la distribuciÃ³n de manera atractiva
3. ðŸ”¢ DESTACA datos interesantes (total de estudiantes y categorÃ­as)
4. ðŸ’¡ INVITA a ver los detalles visuales abajo

âœ… PATRONES DE RESPUESTA HUMANA:
- "Â¡Perfecto! ðŸ“Š AquÃ­ tienes la distribuciÃ³n..."
- "Â¡Excelente consulta! ðŸ‘ Te muestro cÃ³mo se distribuyen..."
- "Â¡Genial! ðŸŽ¯ Los {total_sum} estudiantes se reparten en {len(data)} categorÃ­as..."
- "Â¡QuÃ© buena pregunta! ðŸ¤© AquÃ­ estÃ¡ la informaciÃ³n..."

âŒ EVITA LENGUAJE TÃ‰CNICO:
- "La distribuciÃ³n de alumnos por [campo] nos muestra..."
- "Los datos detallados se presentan a continuaciÃ³n"
- "Tenemos un total de X [categorÃ­as] distintos"

ðŸ“ FORMATO HUMANO Y ENTUSIASTA:
- Saludo entusiasta con emoji apropiado
- PresentaciÃ³n natural de los nÃºmeros clave
- InvitaciÃ³n amigable a explorar los detalles
- MÃ¡ximo 2-3 lÃ­neas con personalidad autÃ©ntica
- Adapta el lenguaje al tipo de distribuciÃ³n automÃ¡ticamente

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""
        else:
            return f"""
Eres el asistente de la escuela especializado en ESTADÃSTICAS Y CONTEOS.

ðŸŽ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- RESULTADO: {actual_count} alumnos
- PORCENTAJE: {porcentaje}% del total ({total_alumnos} alumnos)
- TIPO: Conteo simple

ðŸŽ¯ TU TAREA ESPECÃFICA PARA CONTEOS:
Generar una respuesta ENFOCADA EN NÃšMEROS que:

1. ðŸ“Š PRESENTE el resultado claramente
2. ðŸ” CONTEXTUALICE el nÃºmero (quÃ© significa)
3. ðŸ’¡ SUGIERA anÃ¡lisis relacionados
4. ðŸŽ¯ MANTENGA enfoque en datos

ðŸ“ FORMATO CONCISO:
- Resultado directo
- Contexto breve
- MÃ¡ximo 2-3 lÃ­neas

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_generic_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt genÃ©rico para casos no especÃ­ficos"""
        message = student_data.get("message", "Consulta procesada exitosamente")

        return f"""
Eres el asistente de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ".

ðŸŽ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- ACCIÃ“N: {action_used}
- MENSAJE: {message}

ðŸŽ¯ TU TAREA:
Generar una respuesta profesional y Ãºtil basada en la informaciÃ³n disponible.

ðŸ“ FORMATO:
- Respuesta clara y directa
- Tono profesional pero amigable
- MÃ¡ximo 2-3 lÃ­neas

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _create_help_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para respuestas de ayuda del sistema"""
        help_data = student_data.get("data", {})
        help_type = help_data.get("tipo", "ayuda_general")
        titulo = help_data.get("titulo", "Ayuda del Sistema")
        contenido = help_data.get("contenido", {})

        # ðŸŽ¯ EXTRAER INFORMACIÃ“N ESPECÃFICA DEL HELPINTERPRETER
        info_especifica = ""
        if help_type == "capacidades_sistema":
            # ðŸŽ¯ EXTRAER TODAS LAS CAPACIDADES DETALLADAS
            busquedas_apellido = contenido.get("busquedas_por_apellido", {})
            busquedas_nombre = contenido.get("busquedas_por_nombre_completo", {})
            busquedas_criterios = contenido.get("busquedas_por_criterios_academicos", {})
            constancias = contenido.get("constancias_pdf_completas", {})
            estadisticas = contenido.get("estadisticas_y_conteos", {})
            continuaciones = contenido.get("continuaciones_contextuales", {})
            filtros_calif = contenido.get("filtros_de_calificaciones", {})

            info_especifica = f"""
ðŸ“Š CAPACIDADES COMPLETAS DEL SISTEMA (PROBADAS):

ðŸ” **BÃšSQUEDAS POR APELLIDO**: {busquedas_apellido.get('descripcion', '')}
  Ejemplos: {', '.join(busquedas_apellido.get('ejemplos_reales', [])[:2])}

ðŸ‘¤ **BÃšSQUEDAS POR NOMBRE COMPLETO**: {busquedas_nombre.get('descripcion', '')}
  Ejemplos: {', '.join(busquedas_nombre.get('ejemplos_reales', [])[:2])}

ðŸŽ“ **BÃšSQUEDAS POR CRITERIOS ACADÃ‰MICOS**: {busquedas_criterios.get('descripcion', '')}
  Ejemplos: {', '.join(busquedas_criterios.get('ejemplos_reales', [])[:2])}

ðŸ“„ **CONSTANCIAS PDF**: {constancias.get('descripcion', '')}
  Ejemplos: {', '.join(constancias.get('ejemplos_reales', [])[:2])}

ðŸ“Š **ESTADÃSTICAS Y CONTEOS**: {estadisticas.get('descripcion', '')}
  Ejemplos: {', '.join(estadisticas.get('ejemplos_reales', [])[:2])}

ðŸ”„ **CONTINUACIONES CONTEXTUALES**: {continuaciones.get('descripcion', '')}
  Ejemplo: {continuaciones.get('ejemplos_reales', [''])[0] if continuaciones.get('ejemplos_reales') else ''}

ðŸ“ **FILTROS DE CALIFICACIONES**: {filtros_calif.get('descripcion', '')}
  Ejemplos: {', '.join(filtros_calif.get('ejemplos_reales', [])[:2])}
"""

        elif help_type == "tutorial_uso":
            pasos = contenido.get("pasos", [])
            consejos = contenido.get("consejos", [])

            pasos_info = ""
            for paso in pasos[:4]:  # MÃ¡ximo 4 pasos
                titulo = paso.get("titulo", "")
                descripcion = paso.get("descripcion", "")
                ejemplos = paso.get("ejemplos_reales", [])
                pasos_info += f"- {titulo}: {descripcion}\n  Ejemplos: {', '.join(ejemplos[:2])}\n"

            info_especifica = f"""
ðŸ“š TUTORIAL PASO A PASO - CASOS REALES PROBADOS:
{pasos_info}
ðŸ’¡ CONSEJOS IMPORTANTES:
{chr(10).join(consejos[:3])}
"""

        # ðŸ—‘ï¸ TIPOS ELIMINADOS - SOLO MANTENEMOS CAPACIDADES Y TUTORIAL

        return f"""
Eres el asistente amigable y experto de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" ðŸ«

ðŸŽ¯ SITUACIÃ“N:
- CONSULTA: "{user_query}"
- TIPO DE AYUDA: {help_type}
- ACCIÃ“N: {action_used}
- TÃTULO: {titulo}

{info_especifica}

ðŸŽ­ TU TAREA ESPECÃFICA:
Generar una respuesta ÃšTIL Y ESPECÃFICA que:

1. ðŸ‘‹ SALUDA de manera apropiada
2. ðŸ“š EXPLICA la informaciÃ³n REAL con ejemplos especÃ­ficos
3. ðŸ’¡ USA ÃšNICAMENTE los ejemplos concretos proporcionados arriba
4. ðŸŽ¯ INVITA a probar con ejemplos especÃ­ficos

âœ… PATRONES DE RESPUESTA SEGÃšN TIPO (SIMPLIFICADO):
**Para capacidades_sistema:**
- "Â¡Hola! ðŸ‘‹ Â¡Perfecto! Te explico quÃ© puedo hacer..."
- "Â¡Excelente pregunta! ðŸ¤” Estas son mis capacidades principales..."

**Para tutorial_uso:**
- "Â¡Hola! ðŸ‘‹ Â¡Perfecto! Te explico cÃ³mo usar el sistema paso a paso..."
- "Â¡Excelente! ðŸ¤” AquÃ­ tienes un tutorial con casos reales probados..."

ðŸ“ FORMATO ESPECÃFICO Y OBLIGATORIO:
- Saludo entusiasta apropiado
- ENUMERA TODAS las capacidades de la informaciÃ³n especÃ­fica arriba
- INCLUYE AL MENOS 1 EJEMPLO de cada tipo de bÃºsqueda/funcionalidad
- MENCIONA los tipos especÃ­ficos: apellido, nombre completo, criterios acadÃ©micos, etc.
- USA los nombres reales de los ejemplos (MARTINEZ TORRES, SOPHIA ROMERO GARCIA, etc.)
- Menciona que son casos PROBADOS y validados
- InvitaciÃ³n a probar con ejemplos concretos especÃ­ficos
- Tono conversacional y humano
- INCLUYE SALTOS DE LÃNEA entre cada capacidad para mejor legibilidad
- MÃ¡ximo 8-10 lÃ­neas para incluir TODOS los tipos

ðŸš¨ OBLIGATORIO - INCLUYE TODOS LOS TIPOS CON SALTOS DE LÃNEA:
- BÃšSQUEDAS POR APELLIDO: Al menos 1 ejemplo + SALTO DE LÃNEA
- BÃšSQUEDAS POR NOMBRE COMPLETO: Al menos 1 ejemplo + SALTO DE LÃNEA
- BÃšSQUEDAS POR CRITERIOS ACADÃ‰MICOS: Al menos 1 ejemplo + SALTO DE LÃNEA
- CONSTANCIAS PDF: Al menos 1 ejemplo + SALTO DE LÃNEA
- ESTADÃSTICAS: Al menos 1 ejemplo + SALTO DE LÃNEA
- CONTINUACIONES: Al menos 1 ejemplo + SALTO DE LÃNEA
- FILTROS DE CALIFICACIONES: Al menos 1 ejemplo + SALTO DE LÃNEA
- FORMATEA con nÃºmeros (1., 2., 3., etc.) y SALTO DE LÃNEA despuÃ©s de cada punto

RESPONDE ÃšNICAMENTE con la respuesta conversacional final.
"""

    def _detect_user_interaction_needed(self, action_used: str, student_data: dict) -> bool:
        """
        ðŸŽ¯ DETECTA SI EL STUDENT INDICA QUE NECESITA INTERACCIÃ“N CON EL USUARIO

        Analiza la respuesta del Student para determinar si requiere:
        - Aclaraciones
        - Confirmaciones
        - Selecciones
        - Especificaciones adicionales
        """
        # 1. ACCIONES QUE EXPLÃCITAMENTE REQUIEREN INTERACCIÃ“N
        interactive_actions = [
            "constancia_requiere_aclaracion",
            "seleccion_requerida",
            "confirmacion_requerida",
            "especificacion_requerida"
        ]

        if action_used in interactive_actions:
            return True

        # 2. VERIFICAR SI EL STUDENT INDICA ESPERA DE CONTINUACIÃ“N
        reflexion = student_data.get("reflexion_conversacional", {})
        if reflexion.get("espera_continuacion", False):
            continuation_type = reflexion.get("tipo_esperado", "")
            if continuation_type in ["confirmation", "specification", "selection"]:
                return True

        # 3. VERIFICAR PATRONES EN EL MENSAJE QUE INDICAN PREGUNTA
        message = student_data.get("human_response", "")
        question_patterns = [
            "Â¿cuÃ¡l necesitas?",
            "Â¿te refieres a",
            "necesito que especifiques",
            "Â¿quÃ© tipo de",
            "Â¿confirmas que",
            "Â¿estÃ¡s seguro"
        ]

        for pattern in question_patterns:
            if pattern.lower() in message.lower():
                return True

        return False

    def get_available_modules(self) -> dict:
        """Retorna informaciÃ³n sobre los mÃ³dulos disponibles"""
        return {
            "consulta_alumnos": {
                "disponible": True,
                "descripcion": "Consultas sobre informaciÃ³n de estudiantes",
                "ejemplos": ["cuÃ¡ntos alumnos hay", "buscar a Juan", "alumnos de 3er grado"]
            },
            "transformacion_pdf": {
                "disponible": True,
                "descripcion": "TransformaciÃ³n de constancias entre formatos",
                "ejemplos": ["convertir constancia", "cambiar formato PDF", "transformar a estudios"]
            },
            "ayuda_sistema": {
                "disponible": True,
                "descripcion": "Ayuda sobre el uso del sistema",
                "ejemplos": ["cÃ³mo usar el sistema", "quÃ© puedo hacer", "ayuda con consultas"]
            },
            "conversacion_general": {
                "disponible": False,
                "descripcion": "Chat general y conversaciÃ³n casual",
                "ejemplos": ["hola", "Â¿cÃ³mo estÃ¡s?"]
            }
        }

    # ðŸ§  MÃ‰TODOS DE CONOCIMIENTO PROFUNDO (FASE 2)

    def _validate_feasibility_with_knowledge(self, intention, user_query: str) -> dict:
        """
        ðŸ§  VALIDAR FACTIBILIDAD CON CONOCIMIENTO PROFUNDO
        Usa MasterKnowledge para evaluar si la consulta es factible
        """
        try:
            query_details = {"original_query": user_query}

            feasibility = self.knowledge.can_handle_request(
                intention.intention_type,
                intention.sub_intention,
                query_details
            )

            if feasibility["can_handle"]:
                self.logger.info(f"âœ… [MASTER-KNOWLEDGE] Consulta factible: {feasibility['explanation']}")
                if feasibility["limitations"]:
                    self.logger.info(f"âš ï¸ [MASTER-KNOWLEDGE] Limitaciones: {feasibility['limitations']}")
            else:
                self.logger.warning(f"âŒ [MASTER-KNOWLEDGE] Consulta no factible: {feasibility['explanation']}")
                self.logger.info(f"ðŸ’¡ [MASTER-KNOWLEDGE] Alternativas: {feasibility['alternatives']}")

            return feasibility

        except Exception as e:
            self.logger.error(f"Error validando factibilidad: {e}")
            # Fallback: asumir que es factible
            return {
                "can_handle": True,
                "confidence": 0.5,
                "limitations": [],
                "alternatives": [],
                "explanation": "ValidaciÃ³n de factibilidad fallÃ³, procediendo con precauciÃ³n"
            }

    def _create_limitation_response(self, feasibility: dict, user_query: str) -> 'InterpretationResult':
        """
        ðŸ’¡ CREAR RESPUESTA INTELIGENTE CUANDO ALGO NO ES FACTIBLE
        """
        try:
            from app.core.ai.interpretation.base_interpreter import InterpretationResult

            explanation = feasibility.get("explanation", "Esta funcionalidad no estÃ¡ disponible")
            alternatives = feasibility.get("alternatives", [])

            # Crear respuesta empÃ¡tica con alternativas
            response_parts = [
                f"ðŸ¤” {explanation}.",
                "",
                "ðŸ’¡ **Pero puedo ayudarte con estas alternativas:**"
            ]

            for i, alternative in enumerate(alternatives, 1):
                response_parts.append(f"{i}. {alternative}")

            if not alternatives:
                response_parts.extend([
                    "",
                    "ðŸ“‹ **Capacidades disponibles:**",
                    "â€¢ Buscar informaciÃ³n de alumnos",
                    "â€¢ Generar estadÃ­sticas bÃ¡sicas",
                    "â€¢ Crear constancias oficiales",
                    "â€¢ Transformar documentos PDF"
                ])

            response_parts.append("\nÂ¿Te gustarÃ­a probar alguna de estas opciones? ðŸ˜Š")

            response_text = "\n".join(response_parts)

            self.logger.info(f"ðŸ’¡ [MASTER-KNOWLEDGE] Respuesta de limitaciÃ³n generada")

            return InterpretationResult(
                action="limitation_explanation",
                parameters={
                    "response": response_text,
                    "explanation": explanation,
                    "alternatives": alternatives,
                    "user_query": user_query,
                    "human_response": response_text,
                    "success": True
                },
                confidence=1.0,
                reasoning=f"Consulta no factible: {explanation}"
            )

        except Exception as e:
            self.logger.error(f"Error creando respuesta de limitaciÃ³n: {e}")
            return None

    def _debug_pause(self, title: str, data: dict):
        """MÃ©todo de debug para mostrar informaciÃ³n en --debug-pauses"""
        import os
        if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
            print(f"\nðŸ›‘ {title}")
            for key, value in data.items():
                if isinstance(value, list) and len(value) > 3:
                    print(f"    â”œâ”€â”€ {key}: {value[:3]}... ({len(value)} total)")
                elif isinstance(value, str) and len(value) > 50:
                    print(f"    â”œâ”€â”€ {key}: {value[:50]}...")
                else:
                    print(f"    â”œâ”€â”€ {key}: {value}")
            print(f"    â””â”€â”€ Presiona ENTER para continuar...")
            input()

    def _interpret_student_report_intelligently(self, student_data: dict, original_query: str) -> str:
        """
        ðŸ” INTERPRETAR REPORTES DEL STUDENT CON CONOCIMIENTO PROFUNDO
        Usa MasterKnowledge para analizar quÃ© pasÃ³ y sugerir mejoras
        """
        try:
            interpretation = self.knowledge.interpret_student_report(student_data, original_query)

            user_explanation = interpretation.get("user_explanation", "")
            suggestions = interpretation.get("suggestions", [])

            self.logger.info(f"ðŸ” [MASTER-KNOWLEDGE] InterpretaciÃ³n: {interpretation.get('interpretation', '')}")

            if suggestions:
                self.logger.info(f"ðŸ’¡ [MASTER-KNOWLEDGE] Sugerencias: {suggestions}")

            return user_explanation

        except Exception as e:
            self.logger.error(f"Error interpretando reporte del Student: {e}")
            return "OperaciÃ³n completada."
