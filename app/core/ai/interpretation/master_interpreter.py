"""
Int√©rprete maestro - Coordina todos los m√≥dulos de interpretaci√≥n
"""
from typing import Optional
from app.core.ai.interpretation.base_interpreter import InterpretationContext, InterpretationResult
from app.core.ai.interpretation.intention_detector import IntentionDetector
from app.core.ai.interpretation.student_query_interpreter import StudentQueryInterpreter
from app.core.logging import get_logger
from app.core.config import Config

class MasterInterpreter:
    """
    üéØ INT√âRPRETE MAESTRO - L√çDER INTELIGENTE DEL SISTEMA

    RESPONSABILIDADES:
    - Detectar intenciones con contexto estrat√©gico completo
    - Dirigir al especialista correcto con informaci√≥n precisa
    - Mantener memoria de interacciones para retroalimentaci√≥n
    - Comunicaci√≥n bidireccional con especialistas
    """

    def __init__(self, gemini_client):
        self.gemini_client = gemini_client
        self.logger = get_logger(__name__)

        # üéØ CONTEXTO ESTRAT√âGICO DEL SISTEMA (SEG√öN INTENCIONES_ACCIONES_DEFINITIVAS.md)
        self.system_map = {
            "StudentQueryInterpreter": {
                "handles": ["consulta_alumnos"],
                "sub_intentions": ["busqueda_simple", "busqueda_compleja", "estadisticas", "generar_constancia", "transformacion_pdf"],
                "capabilities": "Consultas de BD, documentos, an√°lisis de 211 alumnos",
                "description": "Especialista en datos de alumnos y generaci√≥n de documentos"
            },
            "HelpInterpreter": {
                "handles": ["ayuda_sistema"],
                "sub_intentions": ["pregunta_capacidades", "pregunta_tecnica"],
                "capabilities": "Ayuda y soporte t√©cnico del sistema",
                "description": "Especialista en ayuda y explicaciones del sistema"
            }
        }

        # üí≠ MEMORIA DE INTERACCIONES (RETROALIMENTACI√ìN)
        self.interaction_memory = {
            "last_specialist": None,
            "last_result_summary": None,
            "conversation_flow": None,
            "specialist_feedback": None,
            "awaiting_continuation": False,
            "continuation_type": None
        }

        # üîß INICIALIZAR COMPONENTES
        self.intention_detector = IntentionDetector(gemini_client)

        # üéØ LOGS DE DEPURACI√ìN FORZADOS - CONTEXTO ESTRAT√âGICO COMPLETO
        self.logger.info("üéØ [MASTER] INICIALIZADO CON CONTEXTO ESTRAT√âGICO")
        self.logger.info(f"   ‚îú‚îÄ‚îÄ Especialistas disponibles: {len(self.system_map)}")
        self.logger.info(f"   ‚îú‚îÄ‚îÄ StudentQueryInterpreter: {self.system_map['StudentQueryInterpreter']['capabilities']}")
        self.logger.info(f"   ‚îî‚îÄ‚îÄ HelpInterpreter: {self.system_map['HelpInterpreter']['capabilities']}")

        # üîç DEBUG DETALLADO DEL CONTEXTO ESTRAT√âGICO
        # Puedes cambiar esto a False para desactivar logs detallados
        self.debug_detailed_context = True
        if self.debug_detailed_context:
            self._log_detailed_strategic_context()

        # üéØ INICIALIZAR ESPECIALISTAS (DESPU√âS DE MOSTRAR CONTEXTO MASTER)
        self.logger.info("üéØ [MASTER] Inicializando especialistas...")
        from app.core.config import Config
        db_path = Config.DB_PATH
        self.student_interpreter = StudentQueryInterpreter(db_path, gemini_client)

        from app.core.ai.interpretation.help_interpreter import HelpInterpreter
        self.help_interpreter = HelpInterpreter(gemini_client)
        self.logger.info("‚úÖ [MASTER] Especialistas inicializados correctamente")

    def interpret(self, context: InterpretationContext, conversation_stack=None) -> Optional[InterpretationResult]:
        """
        üéØ INTERPRETACI√ìN MAESTRO CON CONTEXTO ESTRAT√âGICO COMPLETO

        FLUJO MEJORADO:
        1. An√°lisis con contexto estrat√©gico completo
        2. Detecci√≥n de intenci√≥n con memoria de interacciones
        3. Delegaci√≥n inteligente al especialista correcto
        4. Comunicaci√≥n bidireccional y retroalimentaci√≥n
        """
        try:
            # üéØ LOGS DE DEPURACI√ìN FORZADOS
            self.logger.info("üéØ [MASTER] INICIANDO INTERPRETACI√ìN CON CONTEXTO ESTRAT√âGICO")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ Consulta: '{context.user_message}'")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ Conversation_stack: {len(conversation_stack) if conversation_stack else 0} niveles")
            self.logger.info(f"   ‚îî‚îÄ‚îÄ Memoria anterior: {self.interaction_memory}")

            # üéØ PROCESAMIENTO CON CONTEXTO CONVERSACIONAL ACTIVADO
            context.conversation_stack = conversation_stack or []
            if context.conversation_stack:
                self.logger.info(f"üéØ [MASTER] Procesando con contexto - {len(context.conversation_stack)} niveles disponibles")
            else:
                self.logger.info("üéØ [MASTER] Procesando consulta individual")

            # PASO 1: DETECTAR INTENCI√ìN DIRECTA
            intention = self._detect_intention_direct(context.user_message)

            # üéØ LOGS DE DEPURACI√ìN DE INTENCI√ìN CONSOLIDADA
            self.logger.info(f"üéØ [MASTER] INTENCI√ìN CONSOLIDADA DETECTADA:")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ Tipo: {intention.intention_type}")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ Sub-intenci√≥n: {intention.sub_intention}")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ Confianza: {intention.confidence}")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ üÜï Categor√≠a: {intention.categoria}")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ üÜï Sub-tipo: {intention.sub_tipo}")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ üÜï Complejidad: {intention.complejidad}")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ üÜï Requiere contexto: {intention.requiere_contexto}")
            self.logger.info(f"   ‚îú‚îÄ‚îÄ üÜï Flujo √≥ptimo: {intention.flujo_optimo}")
            self.logger.info(f"   ‚îî‚îÄ‚îÄ Razonamiento: {intention.reasoning}")

            # PASO 3: VALIDAR INTENCI√ìN CON SISTEMA MAP
            validated_intention = self._validate_intention_with_system_map(intention)
            if validated_intention != intention:
                self.logger.info(f"üîß [MASTER] Intenci√≥n corregida por system_map")

            # PASO 3: VERIFICAR AMBIG√úEDADES ANTES DE DELEGAR
            ambiguity_check = self._check_for_ambiguities(context.user_message, validated_intention)

            # üõë PAUSA ESTRAT√âGICA #1: MASTER RAZONAMIENTO INICIAL COMPLETO
            import os
            if os.environ.get('DEBUG_PAUSES', 'false').lower() == 'true':
                print(f"\nüõë [MASTER] AN√ÅLISIS INICIAL:")
                print(f"    ‚îú‚îÄ‚îÄ üìù Consulta: '{context.user_message}'")
                print(f"    ‚îú‚îÄ‚îÄ üß† Intenci√≥n detectada: {intention.intention_type}/{intention.sub_intention}")
                print(f"    ‚îú‚îÄ‚îÄ üìä Confianza: {intention.confidence}")
                print(f"    ‚îú‚îÄ‚îÄ üéØ Entidades extra√≠das: {list(intention.detected_entities.keys())}")
                for key, value in intention.detected_entities.items():
                    if isinstance(value, list) and len(value) > 2:
                        print(f"    ‚îÇ   ‚îú‚îÄ‚îÄ {key}: {value[:2]}... (+{len(value)-2} m√°s)")
                    else:
                        print(f"    ‚îÇ   ‚îú‚îÄ‚îÄ {key}: {value}")
                print(f"    ‚îú‚îÄ‚îÄ üí≠ Razonamiento: {intention.reasoning[:100]}...")
                print(f"    ‚îú‚îÄ‚îÄ üîç Necesita clarificaci√≥n: {ambiguity_check.get('needs_clarification', False)}")
                if ambiguity_check.get('needs_clarification'):
                    print(f"    ‚îÇ   ‚îú‚îÄ‚îÄ T√©rminos ambiguos: {ambiguity_check.get('ambiguous_terms', [])}")
                    print(f"    ‚îÇ   ‚îî‚îÄ‚îÄ Opciones disponibles: {len(ambiguity_check.get('options', []))}")
                print(f"    ‚îî‚îÄ‚îÄ Presiona ENTER para delegar a Student...")
                input()

            if ambiguity_check.get("needs_clarification"):
                # COMUNICACI√ìN BIDIRECCIONAL: Preguntar al usuario
                return self._handle_ambiguous_query(context, validated_intention, ambiguity_check)

            # PASO 4: DIRIGIR AL ESPECIALISTA DIRECTAMENTE (sin ambig√ºedades)
            result = self._delegate_to_specialist_direct(context, validated_intention)

            # PASO 5: ANALIZAR RESULTADOS Y DECIDIR SI NECESITA COMUNICACI√ìN BIDIRECCIONAL
            if result and self._should_ask_user_about_results(result, context.user_message):
                return self._handle_results_analysis(context, validated_intention, result)

            # PASO 6: PROCESAR RETROALIMENTACI√ìN DEL ESPECIALISTA
            self._process_specialist_feedback(validated_intention, result)

            return result

        except Exception as e:
            self.logger.error(f"‚ùå [MASTER] Error en interpretaci√≥n: {e}")
            return None

    def _detect_intention_direct(self, user_message: str):
        """üéØ DETECTAR INTENCI√ìN DIRECTA SIN CONTEXTO"""
        try:
            return self.intention_detector.detect_intention(user_message)
        except Exception as e:
            self.logger.error(f"‚ùå Error detectando intenci√≥n: {e}")
            # Fallback b√°sico
            from app.core.ai.interpretation.intention_detector import IntentionResult
            return IntentionResult(
                intention_type="consulta_alumnos",
                sub_intention="busqueda_simple",
                confidence=0.5,
                reasoning="Fallback por error en detecci√≥n",
                detected_entities={}
            )

    def _validate_intention_with_system_map(self, intention):
        """üéØ VALIDAR INTENCI√ìN CON SYSTEM MAP"""
        try:
            intention_type = intention.intention_type

            # Verificar si la intenci√≥n es manejada por alg√∫n especialista
            for specialist, config in self.system_map.items():
                if intention_type in config["handles"]:
                    self.logger.info(f"‚úÖ [MASTER] Intenci√≥n '{intention_type}' validada para {specialist}")
                    return intention

            # Si no se encuentra, log de advertencia pero continuar
            self.logger.warning(f"‚ö†Ô∏è [MASTER] Intenci√≥n '{intention_type}' no encontrada en system_map")
            return intention

        except Exception as e:
            self.logger.error(f"‚ùå Error validando intenci√≥n: {e}")
            return intention

    def _delegate_to_specialist_direct(self, context: InterpretationContext, intention):
        """üéØ DELEGAR AL ESPECIALISTA CON CONTEXTO COMPLETO"""
        try:


            # Agregar informaci√≥n de intenci√≥n consolidada al contexto
            context.intention_info = {
                'intention_type': intention.intention_type,
                'sub_intention': intention.sub_intention,
                'confidence': intention.confidence,
                'reasoning': intention.reasoning,
                'detected_entities': intention.detected_entities,
                # üÜï CATEGORIZACI√ìN ESPEC√çFICA CONSOLIDADA
                'categoria': intention.categoria,
                'sub_tipo': intention.sub_tipo,
                'complejidad': intention.complejidad,
                'requiere_contexto': intention.requiere_contexto,
                'flujo_optimo': intention.flujo_optimo
            }

            # üéØ DEBUG ESTRAT√âGICO: LO QUE MASTER ENV√çA A STUDENT (CONSOLIDADO)
            self.logger.info("=" * 60)
            self.logger.info("üéØ [DEBUG] MASTER ‚Üí STUDENT COMMUNICATION (CONSOLIDADO):")
            self.logger.info("=" * 60)
            self.logger.info(f"üì§ CONSULTA ORIGINAL: '{context.user_message}'")
            self.logger.info(f"üì§ INTENCI√ìN DETECTADA: {intention.intention_type}/{intention.sub_intention}")
            self.logger.info(f"üì§ CONFIANZA: {intention.confidence}")
            self.logger.info(f"üì§ üÜï CATEGOR√çA: {intention.categoria}")
            self.logger.info(f"üì§ üÜï SUB-TIPO: {intention.sub_tipo}")
            self.logger.info(f"üì§ üÜï COMPLEJIDAD: {intention.complejidad}")
            self.logger.info(f"üì§ üÜï FLUJO √ìPTIMO: {intention.flujo_optimo}")
            self.logger.info(f"üì§ ENTIDADES DETECTADAS: {len(intention.detected_entities)} elementos")
            for key, value in intention.detected_entities.items():
                self.logger.info(f"     ‚îú‚îÄ‚îÄ {key}: {value}")
            self.logger.info(f"üì§ RAZONAMIENTO MASTER: {intention.reasoning}")
            self.logger.info("=" * 60)

            # Dirigir seg√∫n la intenci√≥n
            if intention.intention_type == "consulta_alumnos":
                self.logger.info(f"üéØ [MASTER] Dirigiendo a StudentQueryInterpreter")
                self.logger.info(f"   ‚îú‚îÄ‚îÄ Sub-intenci√≥n: {intention.sub_intention}")
                self.logger.info(f"   ‚îî‚îÄ‚îÄ Entidades: {len(intention.detected_entities)} detectadas")



                result = self.student_interpreter.interpret(context)
                self.logger.info(f"üìä [MASTER] Resultado: {result.action if result else 'None'}")



                # üéØ MASTER COMO VOCERO: Generar respuesta final
                if result:
                    final_result = self._generate_master_response(result, context.user_message)
                    self.logger.info(f"üó£Ô∏è [MASTER] Respuesta final generada como vocero")
                    return final_result

                return result

            elif intention.intention_type == "generar_constancia":
                self.logger.info("üéØ [MASTER] Dirigiendo a StudentQueryInterpreter (constancia)")
                self.logger.info(f"   ‚îú‚îÄ‚îÄ Sub-intenci√≥n: {intention.sub_intention}")
                self.logger.info(f"   ‚îî‚îÄ‚îÄ Entidades: {len(intention.detected_entities)} detectadas")

                result = self.student_interpreter.interpret(context)
                self.logger.info(f"üìä [MASTER] Resultado: {result.action if result else 'None'}")

                # üéØ MASTER COMO VOCERO: Generar respuesta final
                if result:
                    final_result = self._generate_master_response(result, context.user_message)
                    self.logger.info(f"üó£Ô∏è [MASTER] Respuesta final generada como vocero")
                    return final_result

                return result

            elif intention.intention_type == "transformacion_pdf":
                self.logger.info("üéØ [MASTER] Dirigiendo a StudentQueryInterpreter (transformaci√≥n PDF)")
                self.logger.info(f"   ‚îú‚îÄ‚îÄ Sub-intenci√≥n: {intention.sub_intention}")
                self.logger.info(f"   ‚îî‚îÄ‚îÄ Entidades: {len(intention.detected_entities)} detectadas")

                result = self.student_interpreter.interpret(context)
                self.logger.info(f"üìä [MASTER] Resultado: {result.action if result else 'None'}")

                # üéØ MASTER COMO VOCERO: Generar respuesta final
                if result:
                    final_result = self._generate_master_response(result, context.user_message)
                    self.logger.info(f"üó£Ô∏è [MASTER] Respuesta final generada como vocero")
                    return final_result

                return result

            elif intention.intention_type == "ayuda_sistema":
                self.logger.info("üéØ [MASTER] Dirigiendo a HelpInterpreter")
                result = self.help_interpreter.interpret(context)
                self.logger.info(f"üìä [MASTER] Resultado: {result.action if result else 'None'}")

                # üéØ MASTER COMO VOCERO: Generar respuesta final
                if result:
                    final_result = self._generate_master_response(result, context.user_message)
                    self.logger.info(f"üó£Ô∏è [MASTER] Respuesta final generada como vocero")
                    return final_result

                return result

            else:
                # üßπ SIN FALLBACKS - Que falle claramente para debugging
                self.logger.error(f"‚ùå [MASTER] Intenci√≥n no reconocida: {intention.intention_type}")
                raise ValueError(f"Intenci√≥n no reconocida: {intention.intention_type}")

        except Exception as e:
            self.logger.error(f"‚ùå Error delegando al especialista: {e}")
            # üßπ SIN FALLBACKS - Que falle claramente para debugging
            raise

    def _process_specialist_feedback(self, intention, result):
        """üéØ PROCESAR RETROALIMENTACI√ìN DEL ESPECIALISTA"""
        try:
            if result:
                # Actualizar memoria de interacciones
                self.interaction_memory.update({
                    "last_specialist": self._get_specialist_for_intention(intention.intention_type),
                    "last_result_summary": f"Acci√≥n: {result.action}",
                    "conversation_flow": f"{intention.intention_type} ‚Üí {result.action}",
                    "specialist_feedback": "Completado exitosamente",
                    "awaiting_continuation": getattr(result, 'awaiting_continuation', False),
                    "continuation_type": getattr(result, 'continuation_type', None)
                })

                self.logger.info(f"üîÑ [MASTER] Memoria actualizada:")
                self.logger.info(f"   ‚îú‚îÄ‚îÄ Especialista: {self.interaction_memory['last_specialist']}")
                self.logger.info(f"   ‚îú‚îÄ‚îÄ Resultado: {self.interaction_memory['last_result_summary']}")
                self.logger.info(f"   ‚îî‚îÄ‚îÄ Flujo: {self.interaction_memory['conversation_flow']}")
            else:
                self.logger.warning(f"‚ö†Ô∏è [MASTER] No se recibi√≥ resultado del especialista")

        except Exception as e:
            self.logger.error(f"‚ùå Error procesando retroalimentaci√≥n: {e}")

    def _check_for_ambiguities(self, user_message: str, intention) -> dict:
        """
        üß† RAZONAMIENTO INTERNO DEL MASTER: Verificar ambig√ºedades
        Analiza si la consulta es ambigua ANTES de enviar al Student
        """
        try:
            self.logger.info(f"üß† [MASTER] Verificando ambig√ºedades en: '{user_message}'")

            # Crear prompt de an√°lisis de ambig√ºedades
            ambiguity_prompt = self._create_ambiguity_analysis_prompt(user_message, intention)

            # Enviar al LLM para an√°lisis
            response = self.gemini_client.send_prompt_sync(ambiguity_prompt)

            if response:
                ambiguity_data = self._parse_ambiguity_response(response)
                self.logger.info(f"üß† [MASTER] An√°lisis de ambig√ºedad: {ambiguity_data}")
                return ambiguity_data
            else:
                self.logger.warning("‚ö†Ô∏è [MASTER] No se pudo analizar ambig√ºedad")
                return {"needs_clarification": False}

        except Exception as e:
            self.logger.error(f"‚ùå Error verificando ambig√ºedades: {e}")
            return {"needs_clarification": False}

    def _handle_ambiguous_query(self, context, intention, ambiguity_check) -> 'InterpretationResult':
        """
        üîÑ COMUNICACI√ìN BIDIRECCIONAL: Manejar consulta ambigua
        Pregunta al usuario para aclarar la ambig√ºedad
        """
        try:
            self.logger.info(f"üîÑ [MASTER] Manejando consulta ambigua")

            # Generar pregunta de aclaraci√≥n
            clarification_question = self._generate_clarification_question(ambiguity_check)

            # Crear resultado que solicita aclaraci√≥n al usuario
            from app.core.ai.interpretation import InterpretationResult
            return InterpretationResult(
                action="solicitar_aclaracion",
                parameters={
                    "message": clarification_question,
                    "ambiguity_options": ambiguity_check.get("options", []),
                    "original_query": context.user_message,
                    "waiting_for": "clarification",
                    "ambiguity_type": ambiguity_check.get("type", "general")
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"‚ùå Error manejando consulta ambigua: {e}")
            # Fallback: continuar sin aclaraci√≥n
            return self._delegate_to_specialist_direct(context, intention)

    def _get_specialist_for_intention(self, intention_type: str) -> str:
        """üéØ OBTENER ESPECIALISTA PARA INTENCI√ìN"""
        for specialist, config in self.system_map.items():
            if intention_type in config["handles"]:
                return specialist
        return "Unknown"

    def _create_ambiguity_analysis_prompt(self, user_message: str, intention) -> str:
        """Crea prompt para analizar ambig√ºedades en la consulta"""
        return f"""
Analiza si esta consulta del usuario es REALMENTE ambigua y requiere aclaraci√≥n:

CONSULTA: "{user_message}"

INTENCI√ìN DETECTADA: {intention.intention_type}
ENTIDADES DETECTADAS: {intention.detected_entities}

CONTEXTO DEL SISTEMA:
- Sistema de gesti√≥n escolar
- Campos disponibles: grado (1-6), grupo (A,B,C), turno (MATUTINO/VESPERTINO)

REGLAS IMPORTANTES:
1. **S√â PERMISIVO**: Solo marca como ambiguo si es REALMENTE confuso
2. **CONSULTAS COMUNES SON CLARAS**: "buscar garcia" = buscar por apellido Garc√≠a (CLARO)
3. **SOLO AMBIGUO SI HAY M√öLTIPLES INTERPRETACIONES V√ÅLIDAS**: "segundo" puede ser grado 2 o grupo 2
4. **NOMBRES/APELLIDOS SON CLAROS**: Garc√≠a, L√≥pez, Juan, etc. son b√∫squedas directas

EJEMPLOS:
- "buscar garcia" ‚Üí NO ambiguo (buscar apellido Garc√≠a)
- "buscar juan" ‚Üí NO ambiguo (buscar nombre Juan)
- "alumnos de segundo" ‚Üí S√ç ambiguo (¬øgrado 2 o grupo 2?)
- "constancia para juan" ‚Üí PUEDE ser ambiguo si hay m√∫ltiples Juan

ANALIZA:
1. ¬øLa consulta tiene UNA interpretaci√≥n obvia y natural?
2. ¬øHay t√©rminos que REALMENTE podr√≠an confundir al usuario?
3. ¬øEs necesario preguntar o se puede proceder directamente?

RESPONDE EN JSON:
{{
    "needs_clarification": true/false,
    "ambiguous_terms": ["t√©rmino1", "t√©rmino2"],
    "type": "field_ambiguity|value_ambiguity|context_ambiguity",
    "options": [
        {{"label": "Opci√≥n 1", "value": "valor1", "description": "Descripci√≥n"}},
        {{"label": "Opci√≥n 2", "value": "valor2", "description": "Descripci√≥n"}}
    ],
    "reasoning": "Explicaci√≥n del an√°lisis"
}}
"""

    def _parse_ambiguity_response(self, response: str) -> dict:
        """Parsea la respuesta del an√°lisis de ambig√ºedad"""
        try:
            import json
            import re

            # Buscar JSON en la respuesta
            json_patterns = [
                r'```json\s*(.*?)\s*```',
                r'```\s*(.*?)\s*```',
                r'(\{.*?\})'
            ]

            for pattern in json_patterns:
                matches = re.findall(pattern, response, re.DOTALL)
                if matches:
                    return json.loads(matches[0])

            # Intentar parseo directo
            return json.loads(response.strip())

        except Exception as e:
            self.logger.error(f"Error parseando respuesta de ambig√ºedad: {e}")
            return {"needs_clarification": False}

    def _generate_clarification_question(self, ambiguity_check: dict) -> str:
        """Genera pregunta de aclaraci√≥n para el usuario"""
        try:
            ambiguous_terms = ambiguity_check.get("ambiguous_terms", [])
            options = ambiguity_check.get("options", [])

            if not options:
                return "Tu consulta es un poco ambigua. ¬øPodr√≠as ser m√°s espec√≠fico?"

            question = f"ü§î Tu consulta tiene algunas ambig√ºedades. "

            if len(options) == 2:
                question += f"¬øTe refieres a:\n\n"
                for i, option in enumerate(options, 1):
                    question += f"**{i}.** {option['label']} - {option['description']}\n"
            else:
                question += f"¬øPodr√≠as especificar qu√© necesitas?\n\n"
                for i, option in enumerate(options, 1):
                    question += f"**{i}.** {option['label']}\n"

            question += f"\nüí° Responde con el n√∫mero de la opci√≥n que necesitas."

            return question

        except Exception as e:
            self.logger.error(f"Error generando pregunta de aclaraci√≥n: {e}")
            return "Tu consulta necesita m√°s detalles. ¬øPodr√≠as ser m√°s espec√≠fico?"

    def _should_ask_user_about_results(self, result: 'InterpretationResult', user_query: str) -> bool:
        """
        üß† MASTER ANALIZA RESULTADOS: ¬øDeber√≠a preguntar al usuario?
        Decide si los resultados del Student requieren aclaraci√≥n del usuario
        """
        try:
            if not result or not result.parameters:
                return False

            row_count = result.parameters.get('row_count', 0)
            action = result.action

            # üö® CASOS DONDE EL MASTER DEBER√çA PREGUNTAR:

            # 1. Constancias con m√∫ltiples candidatos
            if 'constancia' in user_query.lower() and row_count > 1:
                self.logger.info(f"üîÑ [MASTER] Constancia con {row_count} candidatos - necesita selecci√≥n")
                return True

            # 2. B√∫squedas muy amplias (m√°s de 50 resultados)
            if 'buscar' in user_query.lower() and row_count > 50:
                self.logger.info(f"üîÑ [MASTER] B√∫squeda muy amplia ({row_count} resultados) - ofrecer filtros")
                return True

            # 3. Sin resultados - ofrecer ayuda
            if row_count == 0:
                self.logger.info(f"üîÑ [MASTER] Sin resultados - ofrecer alternativas")
                return True

            # Para b√∫squedas normales como "buscar garcia" con 21 resultados: NO preguntar
            self.logger.info(f"üîÑ [MASTER] Resultados normales ({row_count}) - mostrar directamente")
            return False

        except Exception as e:
            self.logger.error(f"Error analizando si preguntar al usuario: {e}")
            return False

    def _handle_results_analysis(self, context, intention, result: 'InterpretationResult') -> 'InterpretationResult':
        """
        üß† MASTER MANEJA AN√ÅLISIS DE RESULTADOS
        Procesa los resultados del Student y decide qu√© preguntar al usuario
        """
        try:
            self.logger.info("üîÑ [MASTER] Analizando resultados para comunicaci√≥n")

            row_count = result.parameters.get('row_count', 0)
            user_query = context.user_message

            # Determinar tipo de pregunta basado en los resultados
            if 'constancia' in user_query.lower() and row_count > 1:
                return self._create_candidate_selection_question(result, context)
            elif 'buscar' in user_query.lower() and row_count > 50:
                return self._create_filter_suggestion_question(result, context)
            elif row_count == 0:
                return self._create_no_results_help_question(result, context)
            else:
                # No deber√≠a llegar aqu√≠, pero por seguridad
                return result

        except Exception as e:
            self.logger.error(f"Error analizando resultados: {e}")
            return result

    def _create_candidate_selection_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta para seleccionar candidato para constancia"""
        try:
            data = result.parameters.get('data', [])
            candidates = []

            for item in data[:5]:  # M√°ximo 5 candidatos
                candidates.append({
                    'nombre': item.get('nombre', 'N/A'),
                    'grado': f"{item.get('grado', 'N/A')}¬∞{item.get('grupo', '')}"
                })

            message = f"üîç Encontr√© {len(data)} candidatos para la constancia. ¬øCu√°l necesitas?\n\n"
            for i, candidate in enumerate(candidates, 1):
                message += f"**{i}.** {candidate['nombre']} ({candidate['grado']})\n"

            message += f"\nüí° Responde con el n√∫mero de la opci√≥n que necesitas."

            return InterpretationResult(
                action="solicitar_seleccion_constancia",
                parameters={
                    "message": message,
                    "candidates": candidates,
                    "original_query": context.user_message,
                    "waiting_for": "selection",
                    "original_data": data
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de selecci√≥n: {e}")
            return result

    def _create_filter_suggestion_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta para sugerir filtros en b√∫squedas amplias"""
        try:
            row_count = result.parameters.get('row_count', 0)

            message = f"üîç Encontr√© {row_count} resultados. ¬øBuscabas a todos o necesitas filtrar por algo espec√≠fico como grado, grupo o turno?"

            return InterpretationResult(
                action="solicitar_filtros",
                parameters={
                    "message": message,
                    "original_query": context.user_message,
                    "result_count": row_count,
                    "waiting_for": "filter_specification",
                    "original_data": result.parameters.get('data', [])
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de filtros: {e}")
            return result

    def _create_no_results_help_question(self, result: 'InterpretationResult', context) -> 'InterpretationResult':
        """Crea pregunta de ayuda cuando no hay resultados"""
        try:
            message = f"ü§î No encontr√© resultados para '{context.user_message}'. ¬øQuieres que busque con otros criterios o necesitas ayuda?"

            return InterpretationResult(
                action="solicitar_ayuda_busqueda",
                parameters={
                    "message": message,
                    "original_query": context.user_message,
                    "waiting_for": "search_help"
                },
                confidence=0.8
            )

        except Exception as e:
            self.logger.error(f"Error creando pregunta de ayuda: {e}")
            return result

    # üóëÔ∏è M√âTODOS OBSOLETOS ELIMINADOS: COMUNICACI√ìN BIDIRECCIONAL LEGACY
    # RAZ√ìN: Student ya no env√≠a feedback bidireccional - Master analiza resultados directamente
    #
    # M√©todos eliminados:
    # - _needs_bidirectional_communication() ‚Üí Reemplazado por _should_ask_user_about_results()
    # - _handle_student_feedback() ‚Üí Ya no es necesario, Student no env√≠a feedback
    # - _handle_multiple_results_feedback() ‚Üí Integrado en _handle_results_analysis()
    # - _handle_multiple_candidates_feedback() ‚Üí Integrado en _create_candidate_selection_question()
    # - _handle_ambiguity_feedback() ‚Üí Manejado en an√°lisis previo
    # - _handle_validation_feedback() ‚Üí Integrado en _handle_results_analysis()
    # - _handle_generic_feedback() ‚Üí Ya no es necesario

    # üóëÔ∏è M√âTODOS OBSOLETOS ELIMINADOS: FEEDBACK HANDLERS LEGACY
    # RAZ√ìN: Student ya no env√≠a feedback bidireccional - Master analiza resultados directamente
    #
    # M√©todos eliminados:
    # - _handle_multiple_results_feedback() ‚Üí Integrado en _create_filter_suggestion_question()
    # - _handle_multiple_candidates_feedback() ‚Üí Integrado en _create_candidate_selection_question()
    # - _handle_ambiguity_feedback() ‚Üí Manejado en an√°lisis previo con _check_for_ambiguities()
    # - _handle_validation_feedback() ‚Üí Integrado en _create_no_results_help_question()
    # - _handle_generic_feedback() ‚Üí Ya no es necesario

    def _log_detailed_strategic_context(self):
        """üîç MOSTRAR CONTEXTO ESTRAT√âGICO COMPLETO EN LOGS"""
        try:
            self.logger.info("=" * 80)
            self.logger.info("üîç [MASTER] CONTEXTO ESTRAT√âGICO DETALLADO:")
            self.logger.info("=" * 80)

            # 1. SYSTEM MAP COMPLETO
            self.logger.info("üìã 1. SYSTEM MAP (Especialistas y Capacidades):")
            for specialist, config in self.system_map.items():
                self.logger.info(f"   üéØ {specialist}:")
                self.logger.info(f"      ‚îú‚îÄ‚îÄ Rol: {config.get('description', 'No definido')}")
                self.logger.info(f"      ‚îú‚îÄ‚îÄ Maneja: {config.get('handles', [])}")
                self.logger.info(f"      ‚îú‚îÄ‚îÄ Sub-intenciones: {config.get('sub_intentions', [])}")
                self.logger.info(f"      ‚îî‚îÄ‚îÄ Capacidades: {config.get('capabilities', 'No definidas')}")

            # 2. MEMORIA DE INTERACCIONES
            self.logger.info("")
            self.logger.info("üí≠ 2. MEMORIA DE INTERACCIONES:")
            for key, value in self.interaction_memory.items():
                self.logger.info(f"      ‚îú‚îÄ‚îÄ {key}: {value}")

            # 3. CONTEXTO DEL SISTEMA ESCOLAR
            sistema_context = {
                "tipo": "Direcci√≥n de Escuela Primaria PROF. MAXIMO GAMIZ FERNANDEZ",
                "estudiantes_total": 211,
                "areas_disponibles": ["Consultas de Alumnos", "Ayuda T√©cnica"]
            }
            self.logger.info("")
            self.logger.info("üè´ 3. CONTEXTO DEL SISTEMA ESCOLAR:")
            for key, value in sistema_context.items():
                self.logger.info(f"      ‚îú‚îÄ‚îÄ {key}: {value}")

            # 4. TIPOS DE CONSULTAS ESPERADAS
            tipos_consultas = {
                "busquedas": "buscar Garc√≠a, alumnos de 2do A, estudiantes matutinos",
                "estadisticas": "cu√°ntos alumnos hay, promedio general, distribuciones",
                "documentos": "constancia para Juan, certificado de calificaciones",
                "transformaciones": "convertir constancia, cambiar formato",
                "ayuda": "qu√© puedes hacer, c√≥mo buscar alumnos"
            }
            self.logger.info("")
            self.logger.info("üìù 4. TIPOS DE CONSULTAS ESPERADAS:")
            for tipo, ejemplos in tipos_consultas.items():
                self.logger.info(f"      ‚îú‚îÄ‚îÄ {tipo}: {ejemplos}")

            # 5. ESTADO DE INICIALIZACI√ìN (COMPONENTES B√ÅSICOS)
            self.logger.info("")
            self.logger.info("‚úÖ 5. ESTADO DE INICIALIZACI√ìN:")
            self.logger.info(f"      ‚îú‚îÄ‚îÄ Gemini Client: {'‚úÖ Conectado' if self.gemini_client else '‚ùå No disponible'}")
            self.logger.info(f"      ‚îú‚îÄ‚îÄ Intention Detector: {'‚úÖ Inicializado' if self.intention_detector else '‚ùå No disponible'}")
            self.logger.info(f"      ‚îú‚îÄ‚îÄ Student Interpreter: ‚è≥ Se inicializar√° despu√©s")
            self.logger.info(f"      ‚îî‚îÄ‚îÄ Help Interpreter: ‚è≥ Se inicializar√° despu√©s")

            self.logger.info("=" * 80)
            self.logger.info("üéØ [MASTER] CONTEXTO ESTRAT√âGICO CARGADO Y VERIFICADO")
            self.logger.info("=" * 80)

        except Exception as e:
            self.logger.error(f"‚ùå Error mostrando contexto detallado: {e}")

    def _handle_expected_response(self, context: InterpretationContext, conversation_state: dict) -> Optional[InterpretationResult]:
        """
        Maneja respuestas esperadas basadas en el estado conversacional
        Versi√≥n simplificada que usa el Context Manager
        """
        waiting_for = conversation_state.get('waiting_for')
        user_message = context.user_message.lower().strip()

        # üÜï DETECTAR CONFIRMACIONES DESDE CONFIGURACI√ìN CENTRALIZADA
        confirmations = Config.RESPONSES['confirmation_words']

        if waiting_for == "confirmacion_constancia_estudios" and user_message in confirmations:
            return self._handle_confirmation(context, conversation_state)

        # Aqu√≠ se pueden agregar otros tipos de respuestas esperadas
        # elif waiting_for == "seleccion_alumno":
        #     return self._handle_selection(context, conversation_state)

        return None

    def _generate_master_response(self, student_result: 'InterpretationResult', user_query: str) -> 'InterpretationResult':
        """
        üéØ MASTER COMO VOCERO: Genera respuesta final basada en reporte del Student

        Args:
            student_result: Resultado t√©cnico del Student
            user_query: Consulta original del usuario

        Returns:
            InterpretationResult con respuesta final del Master
        """
        try:
            self.logger.info("üó£Ô∏è [MASTER] Generando respuesta final como vocero...")

            # Extraer datos t√©cnicos del Student
            student_data = student_result.parameters
            action_used = student_result.action

            # üéØ EXTRAER CRITERIOS DE B√öSQUEDA DIN√ÅMICAMENTE DESPU√âS DE LA EJECUCI√ìN
            search_criteria = self._extract_search_criteria_for_display(student_data)

            # üéØ AGREGAR CRITERIOS A STUDENT_DATA PARA LAS FUNCIONES DE RESPUESTA
            student_data["search_criteria"] = search_criteria

            # üéØ MASTER GENERA RESPUESTA FINAL USANDO PROMPT ESPECIALIZADO
            master_response = self._generate_master_response_with_llm(student_data, user_query, action_used)

            # üîß CASOS ESPECIALES QUE REQUIEREN PROCESAMIENTO ADICIONAL
            if action_used == "seleccion_realizada":
                # Respuesta de selecci√≥n - mostrar datos del elemento seleccionado
                elemento_seleccionado = student_data.get("elemento_seleccionado")
                posicion = student_data.get("posicion", "N/A")

                if elemento_seleccionado:
                    # Preparar datos para mostrar en la interfaz
                    nombre = elemento_seleccionado.get('nombre', 'N/A')
                    master_response = f"üë§ Informaci√≥n del alumno en posici√≥n {posicion}: **{nombre}**"

                    # Agregar los datos del elemento seleccionado para que se muestren en la interfaz
                    student_data["data"] = [elemento_seleccionado]
                    student_data["row_count"] = 1
                    student_data["human_response"] = master_response
                else:
                    master_response = student_data.get("message", "Selecci√≥n procesada exitosamente")

            elif action_used == "transformation_preview":
                # üîÑ RESPUESTA ESPEC√çFICA PARA TRANSFORMACIONES (mantener por ahora)
                transformation_info = student_data.get("transformation_info", {})
                if transformation_info:
                    tipo_constancia = (transformation_info.get("tipo_constancia") or
                                     transformation_info.get("tipo_transformacion") or
                                     student_data.get("tipo_constancia", "constancia"))
                    alumno_info = (transformation_info.get("alumno") or
                                 student_data.get("alumno", {}))
                    alumno_nombre = alumno_info.get("nombre", "el alumno")

                    master_response = (f"‚úÖ **Transformaci√≥n completada exitosamente**\n\n"
                                     f"He convertido tu PDF a una constancia de **{tipo_constancia}** para **{alumno_nombre}**.\n\n"
                                     f"üìÑ **En el panel derecho puedes:**\n\n"
                                     f"Ver la vista previa, comparar con el original, revisar datos extra√≠dos y abrir en navegador para imprimir.\n\n"
                                     f"üí° ¬øNecesitas hacer alg√∫n ajuste o tienes otra consulta?")

            # üéØ CREAR RESULTADO FINAL CON RESPUESTA DEL MASTER
            final_result = InterpretationResult(
                action=student_result.action,
                parameters={
                    **student_data,  # Mantener datos t√©cnicos del Student
                    "human_response": master_response,  # Respuesta final del Master
                    "master_generated": True,  # Flag para indicar que Master gener√≥ la respuesta
                    "student_action": action_used,  # Acci√≥n original del Student
                    "search_criteria": search_criteria,  # üÜï Criterios para mostrar en listado
                },
                confidence=student_result.confidence
            )

            self.logger.info(f"‚úÖ [MASTER] Respuesta final: '{master_response[:50]}...'")
            return final_result

        except Exception as e:
            self.logger.error(f"‚ùå [MASTER] Error generando respuesta final: {e}")
            # Fallback: retornar resultado original del Student
            return student_result

    def _extract_search_criteria_for_display(self, student_data: dict) -> dict:
        """üéØ EXTRAE CRITERIOS DE B√öSQUEDA DIN√ÅMICAMENTE DEL SQL EJECUTADO"""
        try:
            # üîç DEBUG: Ver qu√© datos llegan
            self.logger.info(f"üîç [DEBUG] student_data keys: {list(student_data.keys())}")

            # üöÄ ENFOQUE DIN√ÅMICO: Analizar el SQL real que se ejecut√≥
            sql_query = student_data.get("sql_executed", "") or student_data.get("sql_query", "")
            search_description = ""
            relevant_fields = []

            self.logger.info(f"üîç [DEBUG] sql_query encontrado: '{sql_query}'")
            self.logger.info(f"üîç [DEBUG] ¬øsql_executed existe? {'sql_executed' in student_data}")
            self.logger.info(f"üîç [DEBUG] ¬øsql_query existe? {'sql_query' in student_data}")

            if sql_query:
                # Extraer campos de WHERE clause din√°micamente
                import re

                # üéØ PATRONES COMPLETOS PARA TODOS LOS CRITERIOS POSIBLES
                where_patterns = [
                    # üìÖ FECHAS
                    (r'fecha_nacimiento\s+LIKE\s+[\'"]%(\d{4})%[\'"]', 'fecha_nacimiento', 'nacidos en {}'),
                    (r'fecha_nacimiento\s+BETWEEN\s+[\'"](\d{4}-\d{2}-\d{2})[\'"].*[\'"](\d{4}-\d{2}-\d{2})[\'"]', 'fecha_nacimiento', 'nacidos entre {} y {}'),
                    (r'fecha_nacimiento\s*=\s*[\'"]([^\'\"]+)[\'"]', 'fecha_nacimiento', 'nacidos el {}'),

                    # üéì DATOS ESCOLARES
                    (r'grado\s*=\s*[\'"](\w+)[\'"]', 'grado', '{}¬∞ grado'),
                    (r'grupo\s*=\s*[\'"](\w+)[\'"]', 'grupo', 'grupo {}'),
                    (r'turno\s*=\s*[\'"](\w+)[\'"]', 'turno', 'turno {}'),

                    # üë§ IDENTIFICADORES
                    (r'matricula\s*=\s*[\'"]([^\'\"]+)[\'"]', 'matricula', 'matr√≠cula {}'),
                    (r'curp\s*=\s*[\'"]([^\'\"]+)[\'"]', 'curp', 'CURP {}'),
                    (r'nombre\s+LIKE\s+[\'"]%([^%\'\"]+)%[\'"]', 'nombre', 'con nombre que contiene "{}"'),
                    (r'nombre\s*=\s*[\'"]([^\'\"]+)[\'"]', 'nombre', 'llamado {}'),

                    # üìä CALIFICACIONES
                    (r'calificaciones\s+IS\s+NOT\s+NULL', 'calificaciones_status', 'con calificaciones'),
                    (r'calificaciones\s+IS\s+NULL', 'calificaciones_status', 'sin calificaciones'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*>\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio mayor a {}'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*<\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio menor a {}'),
                    (r'JSON_EXTRACT\([^,]+,\s*[\'"][^\'\"]*promedio[^\'\"]*[\'\"]\)\s*=\s*(\d+(?:\.\d+)?)', 'promedio', 'con promedio de {}'),

                    # üè† DATOS PERSONALES
                    (r'telefono\s*=\s*[\'"]([^\'\"]+)[\'"]', 'telefono', 'con tel√©fono {}'),
                    (r'direccion\s+LIKE\s+[\'"]%([^%\'\"]+)%[\'"]', 'direccion', 'que viven en {}'),
                    (r'email\s*=\s*[\'"]([^\'\"]+)[\'"]', 'email', 'con email {}'),

                    # üî¢ RANGOS NUM√âRICOS
                    (r'edad\s*>\s*(\d+)', 'edad', 'mayores de {} a√±os'),
                    (r'edad\s*<\s*(\d+)', 'edad', 'menores de {} a√±os'),
                    (r'edad\s*=\s*(\d+)', 'edad', 'de {} a√±os'),
                ]

                for pattern, field, description_template in where_patterns:
                    matches = re.findall(pattern, sql_query, re.IGNORECASE)
                    if matches:
                        relevant_fields.append(field)
                        if isinstance(matches[0], tuple):
                            # M√∫ltiples grupos (ej: BETWEEN)
                            search_description += description_template.format(*matches[0]) + " "
                        else:
                            # Un solo grupo
                            search_description += description_template.format(matches[0]) + " "

                self.logger.info(f"üîç [DYNAMIC] SQL analizado: {sql_query[:100]}...")
                self.logger.info(f"üîç [DYNAMIC] Campos extra√≠dos: {relevant_fields}")
                self.logger.info(f"üîç [DYNAMIC] Descripci√≥n: {search_description.strip()}")

            # Si no hay SQL o no se encontraron patrones, usar fallback inteligente
            if not relevant_fields:
                # Analizar par√°metros de la acci√≥n como fallback
                action_params = student_data.get("action_params", {})
                criterio_principal = action_params.get("criterio_principal", {})
                campo = criterio_principal.get("campo", "")

                if campo:
                    relevant_fields.append(campo)
                    search_description = f"b√∫squeda por {campo}"
                    self.logger.info(f"üîç [FALLBACK] Campo del criterio principal: {campo}")

            # Siempre incluir campos b√°sicos
            basic_fields = ['nombre', 'curp']
            all_fields = basic_fields + [field for field in relevant_fields if field not in basic_fields]

            return {
                "fields_to_show": all_fields,
                "search_description": search_description.strip(),
                "has_specific_criteria": len(relevant_fields) > 0
            }

        except Exception as e:
            self.logger.error(f"Error extrayendo criterios din√°micamente: {e}")
            return {
                "fields_to_show": ['nombre', 'curp', 'turno'],  # Fallback por defecto
                "search_description": "",
                "has_specific_criteria": False
            }

    # üóëÔ∏è M√âTODO ELIMINADO: _generate_count_response
    # RAZ√ìN: Ahora el Student genera respuestas din√°micas directamente con el prompt mejorado

    # üóëÔ∏è M√âTODO ELIMINADO: _generate_count_response_from_filters
    # RAZ√ìN: Ahora el Student genera respuestas din√°micas directamente con el prompt mejorado

    # üóëÔ∏è M√âTODO ELIMINADO: _generate_search_response
    # RAZ√ìN: Ahora el Student genera respuestas din√°micas directamente con el prompt mejorado



    def _generate_master_response_with_llm(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        üéØ GENERA RESPUESTA FINAL USANDO LLM ESPECIALIZADO DEL MASTER

        El Master usa su propio prompt especializado en comunicaci√≥n para generar
        respuestas humanizadas bas√°ndose en los datos t√©cnicos del Student.
        """
        try:
            # Crear prompt especializado para respuesta del Master
            master_prompt = self._create_master_response_prompt(student_data, user_query, action_used)

            # Llamar al LLM para generar respuesta humanizada
            response = self.gemini_client.send_prompt_sync(master_prompt)

            if response and response.strip():
                self.logger.info(f"‚úÖ Master gener√≥ respuesta humanizada exitosamente")
                return response.strip()
            else:
                self.logger.warning(f"‚ùå Master LLM no gener√≥ respuesta, usando fallback")
                return self._generate_fallback_response(student_data, action_used)

        except Exception as e:
            self.logger.error(f"Error generando respuesta con Master LLM: {e}")
            return self._generate_fallback_response(student_data, action_used)

    def _generate_fallback_response(self, student_data: dict, action_used: str) -> str:
        """Genera respuesta de fallback si el LLM del Master falla"""
        row_count = student_data.get("row_count", 0)

        if action_used in ["BUSCAR_UNIVERSAL", "GENERAR_LISTADO_COMPLETO"]:
            if row_count == 0:
                return "No encontr√© resultados para tu b√∫squeda. ¬øPodr√≠as intentar con otros criterios?"
            elif row_count == 1:
                return "Encontr√© un resultado que coincide con tu b√∫squeda."
            else:
                return f"Encontr√© {row_count} resultados que coinciden con tu b√∫squeda."
        elif action_used in ["CONTAR_ALUMNOS", "CONTAR_UNIVERSAL"]:
            return f"El conteo se complet√≥ exitosamente: {row_count} elementos."
        else:
            return "Consulta procesada exitosamente."

    def _create_master_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """
        üéØ CREA PROMPT ESPECIALIZADO DIN√ÅMICO SEG√öN TIPO DE CONSULTA

        Diferentes tipos de consulta requieren diferentes enfoques de respuesta.
        """
        # Detectar tipo de consulta
        query_type = self._detect_query_type(action_used, student_data, user_query)

        # Crear prompt espec√≠fico seg√∫n el tipo
        if query_type == "search":
            return self._create_search_response_prompt(student_data, user_query, action_used)
        elif query_type == "constancia":
            return self._create_constancia_response_prompt(student_data, user_query, action_used)
        elif query_type == "transformation":
            return self._create_transformation_response_prompt(student_data, user_query, action_used)
        elif query_type == "statistics":
            return self._create_statistics_response_prompt(student_data, user_query, action_used)
        else:
            return self._create_generic_response_prompt(student_data, user_query, action_used)

    def _detect_query_type(self, action_used: str, student_data: dict, user_query: str) -> str:
        """Detecta el tipo espec√≠fico de consulta para usar el prompt correcto"""
        # Constancias
        if action_used in ["constancia_generada", "PREPARAR_DATOS_CONSTANCIA"] or "constancia" in user_query.lower():
            return "constancia"

        # Transformaciones
        if action_used in ["transformation_completed", "transformation_preview"] or "transform" in user_query.lower():
            return "transformation"

        # Estad√≠sticas
        if action_used in ["CONTAR_ALUMNOS", "CONTAR_UNIVERSAL", "CALCULAR_ESTADISTICA"] or any(word in user_query.lower() for word in ["cu√°ntos", "total", "estad√≠stica", "conteo"]):
            return "statistics"

        # B√∫squedas (por defecto)
        return "search"

    def _create_search_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para b√∫squedas y listados"""
        row_count = student_data.get("row_count", 0)
        data = student_data.get("data", [])
        ambiguity_level = student_data.get("ambiguity_level", "low")

        # üîß MANEJO SEGURO DE DATOS - verificar que data sea una lista
        data_context = ""
        if data and isinstance(data, list) and len(data) > 0:
            if len(data) <= 3:
                data_context = "RESULTADOS ENCONTRADOS:\n"
                for i, item in enumerate(data, 1):
                    if isinstance(item, dict):
                        nombre = item.get("nombre", "N/A")
                        grado = item.get("grado", "N/A")
                        grupo = item.get("grupo", "N/A")
                        data_context += f"{i}. {nombre} - {grado}¬∞ {grupo}\n"
            else:
                data_context = f"PRIMEROS 3 DE {len(data)} RESULTADOS:\n"
                for i in range(min(3, len(data))):
                    item = data[i]
                    if isinstance(item, dict):
                        nombre = item.get("nombre", "N/A")
                        grado = item.get("grado", "N/A")
                        grupo = item.get("grupo", "N/A")
                        data_context += f"{i+1}. {nombre} - {grado}¬∞ {grupo}\n"

        # Detectar si hay contexto conversacional previo
        reflexion = student_data.get("auto_reflexion", {})
        datos_recordar = reflexion.get("datos_recordar", {})
        conversation_context = datos_recordar.get("context", "")
        query_anterior = datos_recordar.get("query", "")

        # Determinar si es continuaci√≥n
        es_continuacion = bool(conversation_context and query_anterior)

        return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" üè´

üéØ SITUACI√ìN:
- CONSULTA: "{user_query}"
- RESULTADOS: {row_count} estudiantes encontrados
- AMBIG√úEDAD: {ambiguity_level}
- ES CONTINUACI√ìN: {es_continuacion}
- CONTEXTO PREVIO: {conversation_context}
- CONSULTA ANTERIOR: {query_anterior}

{data_context}

üé≠ TU PERSONALIDAD:
- Entusiasta y humano (usa emojis apropiados)
- Profesional pero cercano
- Emp√°tico y comprensivo
- Proactivo en sugerencias

üéØ TU TAREA PARA B√öSQUEDAS:
Generar una respuesta HUMANA y CONECTADA que:

1. üéâ SALUDA con entusiasmo apropiado
2. üìä PRESENTA los resultados de manera atractiva
3. üîç EXPLICA qu√© buscaste de forma natural
4. ü§î MANEJA la ambig√ºedad con empat√≠a
5. üí° SUGIERE pr√≥ximos pasos √∫tiles
6. üîÑ CONECTA con el contexto conversacional si existe

üéØ MANEJO DE AMBIG√úEDAD CON EMPAT√çA:
- HIGH (10+ resultados): "¬°Encontr√© muchos estudiantes! üòä Como [apellido] es com√∫n, te muestro todos para que encuentres al que necesitas. ¬øPodr√≠as ser m√°s espec√≠fico con el nombre o grado?"
- MEDIUM (4-9 resultados): "¬°Perfecto! üëç Encontr√© varios estudiantes que coinciden. ¬øNecesitas informaci√≥n espec√≠fica de alguno?"
- LOW (1-3 resultados): "¬°Excelente! ‚úÖ Aqu√≠ tienes [lo que encontr√©]..."

üîÑ CONTINUIDAD CONVERSACIONAL (MUY IMPORTANTE):
- Si ES_CONTINUACI√ìN = True: NUNCA digas "¬°Hola!" - usa "¬°Perfecto! üëç", "¬°Excelente! ‚úÖ", "Siguiendo con tu b√∫squeda anterior..."
- Si ES_CONTINUACI√ìN = False: Puedes saludar con "¬°Hola! üëã"
- SIEMPRE conecta con la consulta anterior cuando hay contexto
- Menciona espec√≠ficamente qu√© filtros se aplicaron sobre los datos previos

‚úÖ ENFOQUE HUMANO:
- Resultados presentados con entusiasmo
- Criterios explicados naturalmente
- Sugerencias √∫tiles y emp√°ticas
- Tono conversacional y amigable

üìù FORMATO HUMANO:
- Saludo apropiado con emoji
- M√°ximo 3-4 l√≠neas pero con personalidad
- Cierre que invite a continuar la conversaci√≥n

RESPONDE √öNICAMENTE con la respuesta conversacional final.
"""

    def _create_constancia_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para constancias generadas"""
        # üîß MANEJO SEGURO DE DATOS - puede ser lista o string
        data = student_data.get("data", [])
        if isinstance(data, list) and len(data) > 0:
            alumno_data = data[0]
        elif isinstance(data, dict):
            alumno_data = data
        else:
            alumno_data = {}

        # Obtener nombre del alumno de m√∫ltiples fuentes posibles
        alumno_nombre = (
            alumno_data.get("nombre") or
            student_data.get("alumno", {}).get("nombre") if isinstance(student_data.get("alumno"), dict) else
            student_data.get("alumno") if isinstance(student_data.get("alumno"), str) else
            "el estudiante"
        )

        tipo_constancia = student_data.get("tipo_constancia", "constancia")

        # Detectar contexto conversacional
        reflexion = student_data.get("reflexion_conversacional", {})
        conversation_context = reflexion.get("datos_recordar", {}).get("context", "")

        return f"""
Eres el asistente amigable y entusiasta de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ" üè´

üéØ SITUACI√ìN:
- CONSTANCIA GENERADA: {tipo_constancia}
- PARA ESTUDIANTE: {alumno_nombre}
- CONSULTA ORIGINAL: "{user_query}"
- CONTEXTO PREVIO: {conversation_context}

üé≠ TU PERSONALIDAD:
- Entusiasta y celebrativo (usa emojis de √©xito)
- Profesional pero cercano
- Gu√≠a claro y √∫til
- Emp√°tico y comprensivo

üéØ TU TAREA PARA CONSTANCIAS:
Generar una respuesta HUMANA y CELEBRATIVA que:

1. üéâ CELEBRA el √©xito de la generaci√≥n
2. üì± EXPLICA el panel derecho de manera amigable
3. üéõÔ∏è MENCIONA botones espec√≠ficos √∫tiles
4. üí° GU√çA pr√≥ximos pasos claramente
5. üîÑ CONECTA con contexto conversacional si existe

üéõÔ∏è FUNCIONALIDADES DEL PANEL (explica amigablemente):
- Bot√≥n superior izquierdo: "puedes abrir/cerrar el panel"
- Vista previa: "visor PDF con zoom para revisar tu constancia"
- "Ver datos del alumno": "revisa la informaci√≥n extra√≠da"
- "Quitar PDF": "si quieres subir otro documento"
- "Abrir navegador/imprimir": "para guardar o imprimir"
- IMPORTANTE: "Solo vista previa - para guardar usa el navegador"

üîÑ CONTINUIDAD CONVERSACIONAL:
- Si hay contexto previo, recon√≥celo: "¬°Perfecto! Siguiendo con [contexto]..."
- Si es nueva constancia, celebra: "¬°Excelente! üéâ"
- Siempre invita a continuar: "¬øNecesitas algo m√°s?"

üìù FORMATO HUMANO Y CELEBRATIVO:
- Confirmaci√≥n entusiasta con emojis
- Explicaci√≥n clara pero amigable del panel
- M√°ximo 4-5 l√≠neas con personalidad
- Cierre que invite a continuar

RESPONDE √öNICAMENTE con la respuesta conversacional final.
"""

    def _create_transformation_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para transformaciones de PDF"""
        transformation_info = student_data.get("transformation_info", {})
        tipo_constancia = transformation_info.get("tipo_constancia", "constancia")
        alumno_info = transformation_info.get("alumno", {})
        alumno_nombre = alumno_info.get("nombre", "el alumno")

        return f"""
Eres el asistente de la escuela especializado en TRANSFORMACI√ìN DE PDFs.

üéØ SITUACI√ìN:
- TRANSFORMACI√ìN COMPLETADA: PDF ‚Üí {tipo_constancia}
- PARA ESTUDIANTE: {alumno_nombre}
- CONSULTA ORIGINAL: "{user_query}"

üéØ TU TAREA ESPEC√çFICA PARA TRANSFORMACIONES:
Generar una respuesta ENFOCADA EN COMPARACI√ìN que:

1. ‚úÖ CONFIRME que la transformaci√≥n se complet√≥
2. üîÑ EXPLIQUE las funciones de comparaci√≥n
3. üì± MENCIONE botones espec√≠ficos de transformaci√≥n
4. üí° GU√çE sobre c√≥mo comparar y decidir

üîÑ FUNCIONALIDADES ESPEC√çFICAS DE TRANSFORMACI√ìN:
- Todo lo del panel normal M√ÅS:
- "Ver PDF original": muestra el que subiste
- "Ver PDF transformado": muestra el resultado
- Comparaci√≥n r√°pida: alternar entre ambos
- Misma l√≥gica: solo vista previa, guardar desde navegador

üìù FORMATO PARA TRANSFORMACIONES:
- Confirmaci√≥n de transformaci√≥n exitosa
- Explicaci√≥n de comparaci√≥n
- Gu√≠a para decidir pr√≥ximos pasos
- M√°ximo 4-5 l√≠neas

RESPONDE √öNICAMENTE con la respuesta conversacional final.
"""

    def _create_statistics_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt especializado para estad√≠sticas y conteos"""
        row_count = student_data.get("row_count", 0)

        return f"""
Eres el asistente de la escuela especializado en ESTAD√çSTICAS Y CONTEOS.

üéØ SITUACI√ìN:
- CONSULTA: "{user_query}"
- RESULTADO: {row_count}
- TIPO: Estad√≠stica/Conteo

üéØ TU TAREA ESPEC√çFICA PARA ESTAD√çSTICAS:
Generar una respuesta ENFOCADA EN N√öMEROS que:

1. üìä PRESENTE el resultado claramente
2. üîç CONTEXTUALICE el n√∫mero (qu√© significa)
3. üí° SUGIERA an√°lisis relacionados
4. üéØ MANTENGA enfoque en datos

üö´ NO MENCIONES:
- Panel PDF
- Constancias
- Transformaciones

‚úÖ S√ç ENF√ìCATE EN:
- Resultado num√©rico claro
- Contexto del conteo
- Sugerencias de an√°lisis adicional

üìù FORMATO CONCISO:
- Resultado directo
- Contexto breve
- M√°ximo 2-3 l√≠neas

RESPONDE √öNICAMENTE con la respuesta conversacional final.
"""

    def _create_generic_response_prompt(self, student_data: dict, user_query: str, action_used: str) -> str:
        """Prompt gen√©rico para casos no espec√≠ficos"""
        message = student_data.get("message", "Consulta procesada exitosamente")

        return f"""
Eres el asistente de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ".

üéØ SITUACI√ìN:
- CONSULTA: "{user_query}"
- ACCI√ìN: {action_used}
- MENSAJE: {message}

üéØ TU TAREA:
Generar una respuesta profesional y √∫til basada en la informaci√≥n disponible.

üìù FORMATO:
- Respuesta clara y directa
- Tono profesional pero amigable
- M√°ximo 2-3 l√≠neas

RESPONDE √öNICAMENTE con la respuesta conversacional final.
"""

    def _detect_user_interaction_needed(self, action_used: str, student_data: dict) -> bool:
        """
        üéØ DETECTA SI EL STUDENT INDICA QUE NECESITA INTERACCI√ìN CON EL USUARIO

        Analiza la respuesta del Student para determinar si requiere:
        - Aclaraciones
        - Confirmaciones
        - Selecciones
        - Especificaciones adicionales
        """
        # 1. ACCIONES QUE EXPL√çCITAMENTE REQUIEREN INTERACCI√ìN
        interactive_actions = [
            "constancia_requiere_aclaracion",
            "seleccion_requerida",
            "confirmacion_requerida",
            "especificacion_requerida"
        ]

        if action_used in interactive_actions:
            return True

        # 2. VERIFICAR SI EL STUDENT INDICA ESPERA DE CONTINUACI√ìN
        reflexion = student_data.get("reflexion_conversacional", {})
        if reflexion.get("espera_continuacion", False):
            continuation_type = reflexion.get("tipo_esperado", "")
            if continuation_type in ["confirmation", "specification", "selection"]:
                return True

        # 3. VERIFICAR PATRONES EN EL MENSAJE QUE INDICAN PREGUNTA
        message = student_data.get("human_response", "")
        question_patterns = [
            "¬øcu√°l necesitas?",
            "¬øte refieres a",
            "necesito que especifiques",
            "¬øqu√© tipo de",
            "¬øconfirmas que",
            "¬øest√°s seguro"
        ]

        for pattern in question_patterns:
            if pattern.lower() in message.lower():
                return True

        return False

    def _handle_user_interaction_request(self, action_used: str, student_data: dict, base_response: str) -> str:
        """
        üéØ MANEJA SOLICITUDES DE INTERACCI√ìN CON EL USUARIO

        Mejora la respuesta del Student cuando necesita interacci√≥n,
        agregando contexto y opciones claras para el usuario.
        """
        reflexion = student_data.get("reflexion_conversacional", {})
        continuation_type = reflexion.get("tipo_esperado", "")

        # MEJORAR RESPUESTA SEG√öN EL TIPO DE INTERACCI√ìN NECESARIA
        if continuation_type == "confirmation":
            return f"{base_response}\n\nüí° **Responde 's√≠' o 'no' para continuar.**"

        elif continuation_type == "specification":
            return f"{base_response}\n\nüí° **Por favor especifica los detalles que necesitas.**"

        elif continuation_type == "selection":
            data = student_data.get("data", [])
            if data and len(data) > 1:
                return f"{base_response}\n\nüí° **Puedes referenciar por posici√≥n (ej: 'el segundo', 'n√∫mero 3') o por nombre.**"
            else:
                return f"{base_response}\n\nüí° **Por favor especifica cu√°l necesitas.**"

        else:
            # Caso gen√©rico - agregar instrucci√≥n de ayuda
            return f"{base_response}\n\nüí° **¬øNecesitas ayuda con algo espec√≠fico?**"

    def _handle_confirmation(self, context: InterpretationContext, conversation_state: dict) -> Optional[InterpretationResult]:
        """
        Maneja confirmaciones del usuario para diferentes tipos de acciones pendientes
        """
        waiting_for = conversation_state.get('waiting_for')
        context_data = conversation_state.get('context_data', {})

        if waiting_for == "confirmacion_constancia_estudios":
            # Usuario confirm√≥ generar constancia de estudios como alternativa
            alumno_nombre = context_data.get('alumno')
            if alumno_nombre:
                self.logger.info(f"Confirmaci√≥n recibida: generar constancia de estudios para {alumno_nombre}")

                # Crear consulta para generar constancia de estudios
                constancia_query = f"generar constancia de estudios para {alumno_nombre}"
                new_context = InterpretationContext(user_message=constancia_query)
                return self.student_interpreter.interpret(new_context)

        # Aqu√≠ se pueden agregar otros tipos de confirmaci√≥n:
        # elif waiting_for == "confirmacion_eliminar_alumno":
        #     return self._handle_delete_confirmation(context_data)
        # elif waiting_for == "confirmacion_actualizar_datos":
        #     return self._handle_update_confirmation(context_data)

        return None

    def _handle_clarification(self, context: InterpretationContext, conversation_state: dict) -> Optional[InterpretationResult]:
        """Maneja solicitudes de aclaraci√≥n del usuario"""
        return self.student_interpreter.interpret(context)

    def _handle_selection(self, context: InterpretationContext, conversation_state: dict) -> Optional[InterpretationResult]:
        """Maneja selecciones del usuario cuando hay m√∫ltiples opciones"""
        return self.student_interpreter.interpret(context)

    def _handle_related_question(self, context: InterpretationContext, conversation_state: dict) -> Optional[InterpretationResult]:
        """Maneja preguntas relacionadas al contexto actual de la conversaci√≥n"""
        if self.help_interpreter:
            return self.help_interpreter.interpret(context)
        return self.student_interpreter.interpret(context)

    def get_available_modules(self) -> dict:
        """Retorna informaci√≥n sobre los m√≥dulos disponibles"""
        return {
            "consulta_alumnos": {
                "disponible": True,
                "descripcion": "Consultas sobre informaci√≥n de estudiantes",
                "ejemplos": ["cu√°ntos alumnos hay", "buscar a Juan", "alumnos de 3er grado"]
            },
            "transformacion_pdf": {
                "disponible": True,
                "descripcion": "Transformaci√≥n de constancias entre formatos",
                "ejemplos": ["convertir constancia", "cambiar formato PDF", "transformar a estudios"]
            },
            "ayuda_sistema": {
                "disponible": True,
                "descripcion": "Ayuda sobre el uso del sistema",
                "ejemplos": ["c√≥mo usar el sistema", "qu√© puedo hacer", "ayuda con consultas"]
            },
            "conversacion_general": {
                "disponible": False,
                "descripcion": "Chat general y conversaci√≥n casual",
                "ejemplos": ["hola", "¬øc√≥mo est√°s?"]
            }
        }
