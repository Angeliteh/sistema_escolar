"""
Interpretador especializado en consultas de alumnos/estudiantes usando LLM
Implementa la filosofÃ­a de SETs especializados con auto-reflexiÃ³n integrada.

ARQUITECTURA MODULAR COMPLETADA:
- Clases especializadas implementadas y funcionando
- CÃ³digo limpio y mantenible
- Responsabilidades separadas por componente
"""
import json
import re
from typing import Dict, Any, Optional, List, Tuple

from .base_interpreter import BaseInterpreter, InterpretationContext, InterpretationResult
from .database_analyzer import DatabaseAnalyzer
from .sql_executor import SQLExecutor
from .response_parser import ResponseParser
from ..prompts.student_query_prompt_manager import StudentQueryPromptManager
from app.core.logging import get_logger

# âœ… CLASES ESPECIALIZADAS (ARQUITECTURA MODULAR COMPLETADA)
from .student_query.continuation_detector import ContinuationDetector
from .student_query.student_identifier import StudentIdentifier
from .student_query.constancia_processor import ConstanciaProcessor
from .student_query.data_normalizer import DataNormalizer
from .student_query.response_generator import ResponseGenerator
from .utils.json_parser import JSONParser

class StudentQueryInterpreter(BaseInterpreter):
    """Interpretador especializado en consultas de alumnos/estudiantes usando LLM"""

    def __init__(self, db_path: str, gemini_client=None):
        super().__init__("SQL_Interpreter", priority=10)  # Alta prioridad

        # ðŸ†• LOGGING CENTRALIZADO
        self.logger = get_logger(__name__)

        self.db_path = db_path
        self.database_analyzer = DatabaseAnalyzer(db_path)
        self.sql_executor = SQLExecutor(db_path)
        self.response_parser = ResponseParser()
        self.gemini_client = gemini_client

        # Analizar BD una vez al inicializar
        self.database_analyzer.analyze_database()

        # PromptManager centralizado para contexto escolar unificado
        self.prompt_manager = StudentQueryPromptManager(self.database_analyzer)
        self.logger.debug("StudentQueryPromptManager inicializado")

        # Cache del contexto SQL
        self._sql_context = None

        # âœ… INICIALIZAR CLASES ESPECIALIZADAS (ARQUITECTURA MODULAR COMPLETADA)
        self.continuation_detector = ContinuationDetector(gemini_client)
        self.student_identifier = StudentIdentifier()
        self.constancia_processor = ConstanciaProcessor(gemini_client)
        self.data_normalizer = DataNormalizer()
        self.response_generator = ResponseGenerator(gemini_client, self.prompt_manager)
        self.json_parser = JSONParser()

        self.logger.debug("âœ… Clases especializadas inicializadas (Arquitectura modular completada)")

    def _get_supported_actions(self):
        """MÃ©todo requerido por BaseInterpreter - mantenido por compatibilidad"""
        return ["consulta_sql_exitosa", "consulta_sql_fallida"]

    def can_handle(self, context: InterpretationContext) -> bool:
        """MÃ©todo abstracto requerido - siempre True porque se usa desde MasterInterpreter"""
        return True  # El MasterInterpreter ya decidiÃ³ que somos el intÃ©rprete correcto

    def interpret(self, context: InterpretationContext) -> Optional[InterpretationResult]:
        """Interpreta la consulta usando el enfoque optimizado de 3 prompts + sistema conversacional"""
        try:
            self.logger.info(f"ðŸŽ¯ StudentQueryInterpreter INICIADO - Consulta: '{context.user_message}'")
            if hasattr(context, 'conversation_history') and context.conversation_history is not None:
                self.logger.debug(f"Contexto conversacional disponible: {len(context.conversation_history)} mensajes")
            else:
                self.logger.debug("NO hay contexto conversacional disponible")
                # ðŸ†• INICIALIZAR CONVERSATION_HISTORY SI ES NONE
                if not hasattr(context, 'conversation_history') or context.conversation_history is None:
                    context.conversation_history = []

            # ðŸ†• PASO 0: VERIFICAR INFORMACIÃ“N DE INTENCIÃ“N DEL MASTER
            intention_info = getattr(context, 'intention_info', {})
            sub_intention = intention_info.get('sub_intention', '')
            detected_entities = intention_info.get('detected_entities', {})

            # InformaciÃ³n ya mostrada en logs arquitectÃ³nicos del Master

            # ðŸš€ FLUJO DIRECTO BASADO EN SUB-INTENCIÃ“N (PERO VERIFICANDO CONTEXTO PRIMERO)
            if sub_intention == "generar_constancia":
                self.logger.info("ðŸŽ¯ SUB-INTENCIÃ“N: GENERAR CONSTANCIA")

                # ðŸ”§ VERIFICAR PILA CONVERSACIONAL PRIMERO (INCLUSO PARA CONSTANCIAS)
                # ðŸ†• ASEGURAR QUE CONVERSATION_STACK EXISTE
                if not hasattr(context, 'conversation_stack') or context.conversation_stack is None:
                    context.conversation_stack = []

                if hasattr(context, 'conversation_stack') and context.conversation_stack:
                    self.logger.info(f"ðŸ“š VERIFICANDO PILA CONVERSACIONAL: {len(context.conversation_stack)} niveles")

                    # Detectar si es continuaciÃ³n usando LLM
                    continuation_info = self._detect_continuation_query(context.user_message, context.conversation_stack)

                    if continuation_info and continuation_info.get('es_continuacion', False):
                        self.logger.info(f"âœ… CONSTANCIA ES CONTINUACIÃ“N: {continuation_info.get('tipo_continuacion', 'unknown')}")
                        # Procesar usando la pila conversacional
                        return self._process_continuation(context.user_message, continuation_info, context.conversation_stack)
                    else:
                        self.logger.info(f"âŒ CONSTANCIA NO ES CONTINUACIÃ“N: {continuation_info}")

                # Si no es continuaciÃ³n, procesar como constancia nueva
                self.logger.info("âœ… PROCESANDO CONSTANCIA NUEVA...")
                simple_context = {
                    'pdf_panel': getattr(context, 'pdf_panel', None),
                    'user_message': context.user_message,
                    'detected_entities': detected_entities  # â† NUEVO: Pasar entidades detectadas
                }
                result = self._process_constancia_request(context.user_message, simple_context)
                self.logger.info(f"ðŸŽ¯ Resultado constancia: {result.action if result else 'None'}")
                return result

            elif sub_intention == "transformar_pdf":
                self.logger.info("âœ… SUB-INTENCIÃ“N: TRANSFORMAR PDF - Procesando directamente...")
                simple_context = {
                    'pdf_panel': getattr(context, 'pdf_panel', None),
                    'user_message': context.user_message,
                    'detected_entities': detected_entities
                }
                result = self._process_constancia_request(context.user_message, simple_context)
                self.logger.info(f"ðŸŽ¯ Resultado transformaciÃ³n: {result.action if result else 'None'}")
                return result

            # PASO 1: Verificar si hay pila conversacional y detectar continuaciÃ³n
            # ðŸ†• INICIALIZAR CONVERSATION_STACK SI ES NONE
            if not hasattr(context, 'conversation_stack') or context.conversation_stack is None:
                context.conversation_stack = []

            if hasattr(context, 'conversation_stack') and context.conversation_stack:
                self.logger.info(f"ðŸ“š PILA CONVERSACIONAL DISPONIBLE: {len(context.conversation_stack)} niveles")

                # Mostrar contenido de la pila de forma estructurada
                for i, level in enumerate(context.conversation_stack, 1):
                    self.logger.info(f"   ðŸ“‹ Nivel {i}: '{level.get('query', 'N/A')[:30]}...' - {level.get('row_count', 0)} elementos")

                # Detectar si es continuaciÃ³n usando LLM
                continuation_info = self._detect_continuation_query(context.user_message, context.conversation_stack)

                if continuation_info and continuation_info.get('es_continuacion', False):
                    self.logger.info(f"âœ… CONTINUACIÃ“N DETECTADA: {continuation_info.get('tipo_continuacion', 'unknown')}")
                    # Procesar usando la pila conversacional (NO generar SQL)
                    return self._process_continuation(context.user_message, continuation_info, context.conversation_stack)
                else:
                    self.logger.info(f"âŒ NO ES CONTINUACIÃ“N: {continuation_info}")
            else:
                self.logger.info("âŒ NO HAY PILA CONVERSACIONAL disponible")

            # ðŸ”„ [STUDENT] INICIANDO FLUJO DE 4 PROMPTS
            self.logger.info("ðŸ”„ [STUDENT] Iniciando flujo de 4 prompts")

            # PROMPT 1: Detectar intenciÃ³n especÃ­fica (USANDO PROMPT MANAGER CENTRALIZADO CON CONTEXTO)
            self.logger.info("   â”œâ”€â”€ PROMPT 1: AnÃ¡lisis de intenciÃ³n especÃ­fica...")

            # ðŸ†• PREPARAR CONTEXTO CONVERSACIONAL PARA PROMPT 1
            conversation_context = ""
            # ðŸ†• ASEGURAR QUE CONVERSATION_STACK EXISTE ANTES DE USARLO
            if not hasattr(context, 'conversation_stack') or context.conversation_stack is None:
                context.conversation_stack = []

            if hasattr(context, 'conversation_stack') and context.conversation_stack:
                conversation_context = self._format_conversation_stack_for_llm(context.conversation_stack)
                self.logger.info(f"   â”œâ”€â”€ Contexto conversacional disponible: {len(context.conversation_stack)} niveles")

            is_student_query = self._detect_student_query_intention_centralized(context.user_message, conversation_context)
            if not is_student_query:
                self.logger.info("   â””â”€â”€ âŒ No es consulta de alumnos")
                return None
            self.logger.info("   â””â”€â”€ âœ… Consulta de alumnos confirmada")

            # ðŸ†• DETECCIÃ“N LIMPIA: Solo usar entidades del Master (SIN fallbacks)
            if (detected_entities and
                detected_entities.get('tipo_constancia') and
                detected_entities.get('tipo_constancia') != 'null'):

                self.logger.info("âœ… CONSTANCIA DETECTADA POR MASTER - Procesando directamente")
                # ðŸ†• PASAR CONTEXTO COMPLETO CON INTENTION_INFO
                result = self._process_constancia_request(context.user_message, context)
                self.logger.info(f"ðŸ“Š [STUDENT] Resultado constancia: {result.action if result else 'None'}")
                return result

            # PROMPT 2: Generar estrategia + SQL (USANDO PROMPT MANAGER CENTRALIZADO)
            self.logger.info("   â”œâ”€â”€ PROMPT 2: GeneraciÃ³n SQL inteligente...")
            sql_query = self._generate_sql_with_strategy_centralized(context.user_message)

            if not sql_query:
                self.logger.info("   â””â”€â”€ âŒ No se pudo generar SQL")
                return None
            self.logger.info("   â””â”€â”€ âœ… SQL generado exitosamente")

            # Ejecutar consulta SQL
            result = self.sql_executor.execute_query(sql_query)

            if result.success:
                # PROMPT 3: Validar + Generar respuesta + Auto-reflexiÃ³n
                self.logger.info("   â”œâ”€â”€ PROMPT 3: ValidaciÃ³n + respuesta + auto-reflexiÃ³n...")
                response_with_reflection = self._validate_and_generate_response(
                    context.user_message, sql_query, result.data, result.row_count
                )

                if not response_with_reflection:
                    self.logger.info("   â””â”€â”€ âŒ ValidaciÃ³n fallÃ³")
                    return InterpretationResult(
                        action="consulta_sql_fallida",
                        parameters={
                            "error": "La consulta SQL no resolviÃ³ correctamente la solicitud",
                            "sql_query": sql_query
                        },
                        confidence=0.2
                    )
                self.logger.info("   â””â”€â”€ âœ… ValidaciÃ³n y respuesta completadas")

                # Extraer respuesta y reflexiÃ³n
                human_response = response_with_reflection.get("respuesta_usuario", "Respuesta procesada")
                reflexion = response_with_reflection.get("reflexion_conversacional", {})

                # PROMPT 4: Filtrado inteligente (implÃ­cito en validate_and_generate_response)
                self.logger.info("   â””â”€â”€ PROMPT 4: Filtrado inteligente aplicado âœ…")

                # Preparar parÃ¡metros con informaciÃ³n de auto-reflexiÃ³n
                parameters = {
                    "sql_query": result.query_executed,
                    "data": result.data,
                    "row_count": result.row_count,
                    "message": human_response,
                    "human_response": human_response,
                    "auto_reflexion": reflexion
                }

                self.logger.info(f"ðŸ“Š [STUDENT] Flujo completado: {result.row_count} resultados encontrados")
                return InterpretationResult(
                    action="consulta_sql_exitosa",
                    parameters=parameters,
                    confidence=0.9
                )
            else:
                self.logger.info("   â””â”€â”€ âŒ EjecuciÃ³n SQL fallÃ³")
                return InterpretationResult(
                    action="consulta_sql_fallida",
                    parameters={
                        "error": result.message,
                        "sql_query": sql_query
                    },
                    confidence=0.3
                )

        except Exception as e:
            self.logger.error(f"âŒ Error en StudentQueryInterpreter: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

            return InterpretationResult(
                action="consulta_sql_fallida",
                parameters={
                    "error": f"Error interno: {str(e)}",
                    "message": "âŒ Error procesando tu consulta. Intenta reformularla.",
                    "exception": str(e)
                },
                confidence=0.1
            )



    def _detect_continuation_query(self, user_query: str, conversation_stack: list) -> Optional[Dict[str, Any]]:
        """
        DETECTOR DE CONTINUACIÃ“N: Usa LLM para determinar si la consulta se refiere a la pila conversacional
        âœ… MIGRADO: Usa ContinuationDetector centralizado
        """
        try:
            result = self.continuation_detector.detect_continuation(user_query, conversation_stack)

            if result:
                self.logger.debug(f"âœ… ContinuaciÃ³n detectada exitosamente con ContinuationDetector: {result.get('tipo_continuacion', 'unknown')}")
                return result
            else:
                self.logger.warning(f"âŒ ContinuationDetector no detectÃ³ continuaciÃ³n")
                return {"es_continuacion": False, "tipo_continuacion": "none"}

        except Exception as e:
            self.logger.error(f"Error usando ContinuationDetector: {e}")
            return {"es_continuacion": False, "tipo_continuacion": "none"}

    def _process_continuation(self, user_query: str, continuation_info: Dict[str, Any], conversation_stack: list) -> Optional[InterpretationResult]:
        """
        PROCESADOR DE CONTINUACIÃ“N: Procesa la consulta usando datos de la pila (NO genera SQL)
        """
        try:
            tipo_continuacion = continuation_info.get('tipo_continuacion', 'none')
            nivel_referenciado = continuation_info.get('nivel_referenciado')
            elemento_referenciado = continuation_info.get('elemento_referenciado')

            self.logger.debug(f"Procesando continuaciÃ³n tipo: {tipo_continuacion}")

            if tipo_continuacion == "selection":
                return self._process_selection_continuation(user_query, elemento_referenciado, conversation_stack)
            elif tipo_continuacion == "action":
                return self._process_action_continuation(user_query, nivel_referenciado, conversation_stack)
            elif tipo_continuacion == "confirmation":
                return self._process_confirmation_continuation(user_query, conversation_stack)
            elif tipo_continuacion == "specification":
                return self._process_specification_continuation(user_query, conversation_stack)
            else:
                self.logger.warning(f"Tipo de continuaciÃ³n no reconocido: {tipo_continuacion}")
                return None

        except Exception as e:
            self.logger.error(f"Error procesando continuaciÃ³n: {e}")
            return None

    def _format_conversation_stack_for_llm(self, conversation_stack: list) -> str:
        """Formatea la pila conversacional para el LLM"""
        if not conversation_stack:
            return "PILA VACÃA"

        context = ""
        for i, level in enumerate(conversation_stack, 1):
            context += f"""
NIVEL {i}:
- Consulta: "{level.get('query', 'N/A')}"
- Datos disponibles: {level.get('row_count', 0)} elementos
- Esperando: {level.get('awaiting', 'N/A')}
- Timestamp: {level.get('timestamp', 'N/A')}
"""
            # Mostrar algunos datos de ejemplo si hay
            if level.get('data') and len(level.get('data', [])) > 0:
                context += f"- Primeros elementos: {level['data'][:3]}\n"

        return context

    def _parse_json_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Parsea respuesta JSON del LLM
        âœ… MIGRADO: Usa JSONParser centralizado
        """
        try:
            result = self.json_parser.parse_llm_response(response)

            if result:
                self.logger.debug(f"âœ… JSON parseado exitosamente con JSONParser")
                return result
            else:
                self.logger.warning(f"âŒ JSONParser no pudo parsear respuesta: {response[:100]}...")
                return None

        except Exception as e:
            self.logger.error(f"Error usando JSONParser: {e}")
            return None

    def _process_selection_continuation(self, user_query: str, elemento_referenciado: int, conversation_stack: list) -> Optional[InterpretationResult]:
        """Procesa continuaciÃ³n de tipo SELECCIÃ“N (ej: 'del quinto', 'nÃºmero 3')"""
        try:
            # Obtener el Ãºltimo nivel con datos de lista
            ultimo_nivel = None
            for level in reversed(conversation_stack):
                if level.get('data') and len(level.get('data', [])) > 0:
                    ultimo_nivel = level
                    break

            if not ultimo_nivel:
                return InterpretationResult(
                    action="continuacion_error",
                    parameters={
                        "error": "No hay datos de lista disponibles en la pila conversacional",
                        "message": "No encuentro una lista previa para hacer la selecciÃ³n. Â¿PodrÃ­as hacer una nueva consulta?"
                    },
                    confidence=0.3
                )

            # Verificar que el elemento existe
            datos = ultimo_nivel.get('data', [])
            if not elemento_referenciado or elemento_referenciado < 1 or elemento_referenciado > len(datos):
                return InterpretationResult(
                    action="continuacion_error",
                    parameters={
                        "error": f"Elemento {elemento_referenciado} no existe en la lista",
                        "message": f"La lista tiene {len(datos)} elementos. Â¿PodrÃ­as especificar un nÃºmero entre 1 y {len(datos)}?"
                    },
                    confidence=0.3
                )

            # Obtener el elemento seleccionado (Ã­ndice base 1)
            elemento_seleccionado = datos[elemento_referenciado - 1]

            # ðŸŽ¯ VERIFICAR SI ES SOLICITUD DE CONSTANCIA (IGUAL QUE EN ACTION)
            constancia_keywords = ["constancia", "certificado", "genera", "generar", "crear", "documento"]
            user_lower = user_query.lower()
            is_constancia_request = any(keyword in user_lower for keyword in constancia_keywords)

            if is_constancia_request:
                self.logger.info("ðŸŽ¯ SELECCIÃ“N + CONSTANCIA - Procesando DIRECTAMENTE...")

                # Extraer tipo de constancia del query
                tipo_constancia = "estudio"  # Por defecto
                if "calificaciones" in user_lower:
                    tipo_constancia = "calificaciones"
                elif "traslado" in user_lower:
                    tipo_constancia = "traslado"
                elif "estudios" in user_lower or "estudio" in user_lower:
                    tipo_constancia = "estudio"

                self.logger.info(f"   - Tipo detectado: {tipo_constancia}")
                self.logger.info(f"   - Alumno seleccionado: {elemento_seleccionado.get('nombre', 'N/A')}")

                # ðŸ”§ OBTENER DATOS COMPLETOS DEL ALUMNO (incluyendo ID)
                alumno_completo = self._get_complete_student_data(elemento_seleccionado)

                if not alumno_completo:
                    return InterpretationResult(
                        action="constancia_error",
                        parameters={
                            "message": f"âŒ No se pudieron obtener los datos completos de {elemento_seleccionado.get('nombre', 'N/A')}",
                            "error": "incomplete_student_data"
                        },
                        confidence=0.3
                    )

                # ðŸš€ GENERAR CONSTANCIA DIRECTAMENTE (SIN SQL)
                self.logger.info("ðŸš€ GENERANDO CONSTANCIA DIRECTAMENTE DESDE SELECCIÃ“N")
                return self._generate_constancia_for_student(alumno_completo, tipo_constancia, user_query)

            # Si NO es constancia, usar respuesta unificada normal
            response_message = self._generate_unified_continuation_response(
                user_query, "selection", ultimo_nivel, conversation_stack
            )

            return InterpretationResult(
                action="seleccion_realizada",
                parameters={
                    "message": response_message,
                    "elemento_seleccionado": elemento_seleccionado,
                    "posicion": elemento_referenciado,
                    "consulta_original": ultimo_nivel.get('query', ''),
                    "tipo": "seleccion"
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error en selecciÃ³n: {e}")
            return None

    def _process_action_continuation(self, user_query: str, nivel_referenciado: int, conversation_stack: list) -> Optional[InterpretationResult]:
        """Procesa continuaciÃ³n de tipo ACCIÃ“N (ej: 'constancia para Ã©l', 'CURP de ese')"""
        try:
            # NOTA: nivel_referenciado no se usa actualmente, se mantiene por compatibilidad
            # Obtener el Ãºltimo alumno seleccionado o el Ãºltimo nivel con datos
            ultimo_alumno = None
            ultimo_nivel = None

            for level in reversed(conversation_stack):
                if level.get('data'):
                    # Si hay un solo elemento, es una selecciÃ³n previa
                    if len(level.get('data', [])) == 1:
                        ultimo_alumno = level['data'][0]
                        ultimo_nivel = level
                        break
                    # Si hay mÃºltiples elementos, buscar si hay selecciÃ³n
                    elif len(level.get('data', [])) > 1:
                        ultimo_nivel = level
                        # Por ahora, tomar el primero como referencia
                        ultimo_alumno = level['data'][0]
                        break

            if not ultimo_alumno:
                return InterpretationResult(
                    action="continuacion_error",
                    parameters={
                        "error": "No hay alumno seleccionado previamente",
                        "message": "No encuentro un alumno seleccionado previamente. Â¿PodrÃ­as especificar de quÃ© alumno necesitas informaciÃ³n?"
                    },
                    confidence=0.3
                )

            # ðŸ§  DETECTAR SI NECESITA NUEVA CONSULTA SQL
            needs_sql = self._detect_if_needs_sql_query(user_query, ultimo_nivel)

            if needs_sql:
                self.logger.debug("AcciÃ³n requiere nueva consulta SQL")
                # Generar nueva consulta SQL basada en datos previos
                new_sql = self._generate_sql_for_action_continuation(user_query, ultimo_nivel)

                if new_sql:
                    self.logger.debug(f"SQL generado para continuaciÃ³n: {new_sql[:100]}...")

                    # Ejecutar la nueva consulta
                    result = self.sql_executor.execute_query(new_sql)

                    if result.success:
                        # Usar filtro inteligente en los nuevos datos
                        filtered_data, filter_decision = self._intelligent_final_filter(user_query, result.data, new_sql)

                        # Generar respuesta con datos reales
                        final_response = self._validate_and_generate_response(
                            user_query, new_sql, filtered_data, len(filtered_data)
                        )

                        if final_response:
                            return InterpretationResult(
                                action="consulta_sql_exitosa",
                                parameters={
                                    "sql_query": result.query_executed,
                                    "data": filtered_data,
                                    "row_count": len(filtered_data),
                                    "message": final_response.get("respuesta_usuario", "InformaciÃ³n obtenida"),
                                    "human_response": final_response.get("respuesta_usuario", "InformaciÃ³n obtenida"),
                                    "auto_reflexion": final_response.get("reflexion_conversacional", {}),
                                    "tipo": "accion_con_sql"
                                },
                                confidence=0.95
                            )

            # ðŸŽ¯ VERIFICAR SI ES SOLICITUD DE CONSTANCIA
            constancia_keywords = ["constancia", "certificado", "genera", "generar", "crear", "documento"]
            user_lower = user_query.lower()
            is_constancia_request = any(keyword in user_lower for keyword in constancia_keywords)

            if is_constancia_request:
                self.logger.info("ðŸŽ¯ ACCIÃ“N DE CONTINUACIÃ“N ES CONSTANCIA - Procesando DIRECTAMENTE...")

                # Extraer tipo de constancia del query
                tipo_constancia = "estudio"  # Por defecto
                if "calificaciones" in user_lower:
                    tipo_constancia = "calificaciones"
                elif "traslado" in user_lower:
                    tipo_constancia = "traslado"
                elif "estudios" in user_lower or "estudio" in user_lower:
                    tipo_constancia = "estudio"

                self.logger.info(f"   - Tipo detectado: {tipo_constancia}")

                # ðŸ”§ IDENTIFICAR ALUMNO CORRECTO USANDO CONTEXTO
                alumno_seleccionado = self._identify_student_from_context(user_query, conversation_stack)

                if not alumno_seleccionado:
                    self.logger.warning("âŒ No se pudo identificar alumno desde el contexto")
                    alumno_seleccionado = ultimo_alumno  # Fallback al Ãºltimo alumno

                self.logger.info(f"   - Alumno identificado: {alumno_seleccionado.get('nombre', 'N/A')}")

                # ðŸ”§ OBTENER DATOS COMPLETOS DEL ALUMNO (incluyendo ID)
                alumno_completo = self._get_complete_student_data(alumno_seleccionado)

                if not alumno_completo:
                    return InterpretationResult(
                        action="constancia_error",
                        parameters={
                            "message": f"âŒ No se pudieron obtener los datos completos de {alumno_seleccionado.get('nombre', 'N/A')}",
                            "error": "incomplete_student_data"
                        },
                        confidence=0.3
                    )

                # ðŸš€ GENERAR CONSTANCIA DIRECTAMENTE (SIN SQL)
                self.logger.info("ðŸš€ GENERANDO CONSTANCIA DIRECTAMENTE DESDE CONTEXTO")
                return self._generate_constancia_for_student(alumno_completo, tipo_constancia, user_query)

            # Si no necesita SQL o fallÃ³, usar respuesta LLM unificada
            self.logger.debug("AcciÃ³n NO requiere SQL, usando respuesta LLM unificada")
            response_message = self._generate_unified_continuation_response(
                user_query, "action", ultimo_nivel, conversation_stack
            )

            return InterpretationResult(
                action="accion_realizada",
                parameters={
                    "message": response_message,
                    "alumno": ultimo_alumno,
                    "accion_solicitada": user_query,
                    "tipo": "accion"
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error en acciÃ³n: {e}")
            return None

    def _identify_student_from_context(self, user_query: str, conversation_stack: list) -> Optional[Dict[str, Any]]:
        """
        Identifica al alumno correcto usando el contexto conversacional y referencias en la consulta
        ðŸ†• MIGRADO: Usa StudentIdentifier centralizado (RefactorizaciÃ³n completada)
        """
        try:
            result = self.student_identifier.identify_student_from_context(
                user_query, conversation_stack
            )

            if result:
                self.logger.debug(f"âœ… Alumno identificado exitosamente con StudentIdentifier: {result.get('nombre', 'N/A')}")
                return result
            else:
                self.logger.warning(f"âŒ StudentIdentifier no pudo identificar alumno")
                return None

        except Exception as e:
            self.logger.error(f"Error usando StudentIdentifier: {e}")
            return None



    def _normalize_student_data_structure(self, item: Dict) -> Optional[Dict]:
        """
        Normaliza diferentes estructuras de datos de alumnos a un formato estÃ¡ndar
        âœ… MIGRADO: Usa DataNormalizer centralizado
        """
        try:
            result = self.data_normalizer.normalize_student_data(item)

            if result:
                self.logger.debug(f"âœ… Datos normalizados exitosamente con DataNormalizer: {result.get('nombre', 'N/A')}")
                return result
            else:
                self.logger.warning(f"âŒ DataNormalizer no pudo normalizar estructura: {list(item.keys())}")
                return None

        except Exception as e:
            self.logger.error(f"Error usando DataNormalizer: {e}")
            return None

    def _get_complete_student_data(self, alumno_parcial: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Obtiene los datos completos del alumno desde la base de datos"""
        try:
            # Si ya tiene ID, verificar que tenga todos los datos necesarios
            if alumno_parcial.get('id'):
                self.logger.info(f"âœ… Alumno ya tiene ID: {alumno_parcial.get('id')}")
                return alumno_parcial

            # Si no tiene ID, buscar por nombre
            nombre_alumno = alumno_parcial.get('nombre', '')
            if not nombre_alumno:
                self.logger.warning("âŒ No se puede buscar alumno sin nombre")
                return None

            self.logger.info(f"ðŸ” Buscando datos completos para: {nombre_alumno}")

            from app.core.service_provider import ServiceProvider
            service_provider = ServiceProvider.get_instance()
            alumno_service = service_provider.alumno_service

            # Buscar alumno por nombre exacto
            alumnos_encontrados = alumno_service.buscar_alumnos(nombre_alumno)

            if not alumnos_encontrados:
                self.logger.warning(f"âŒ No se encontrÃ³ alumno con nombre: {nombre_alumno}")
                return None

            # Si hay mÃºltiples coincidencias, buscar coincidencia exacta
            for alumno in alumnos_encontrados:
                alumno_dict = alumno.to_dict() if hasattr(alumno, 'to_dict') else alumno
                if alumno_dict.get('nombre', '').upper() == nombre_alumno.upper():
                    self.logger.info(f"âœ… Datos completos obtenidos para: {alumno_dict.get('nombre')} (ID: {alumno_dict.get('id')})")
                    return alumno_dict

            # Si no hay coincidencia exacta, tomar el primero
            primer_alumno = alumnos_encontrados[0]
            alumno_dict = primer_alumno.to_dict() if hasattr(primer_alumno, 'to_dict') else primer_alumno

            self.logger.info(f"âœ… Usando primer resultado: {alumno_dict.get('nombre')} (ID: {alumno_dict.get('id')})")
            return alumno_dict

        except Exception as e:
            self.logger.error(f"Error obteniendo datos completos del alumno: {e}")
            return None

    def _generate_constancia_for_student(self, alumno: Dict, tipo_constancia: str, user_query: str) -> InterpretationResult:
        """
        Genera constancia directamente para un alumno especÃ­fico
        âœ… MIGRADO: Usa ConstanciaProcessor centralizado
        """
        try:
            result = self.constancia_processor.process_constancia_request(
                alumno, tipo_constancia, user_query
            )

            if result:
                self.logger.debug(f"âœ… Constancia procesada exitosamente con ConstanciaProcessor: {result.action}")
                return result
            else:
                self.logger.warning(f"âŒ ConstanciaProcessor no pudo procesar constancia")
                return InterpretationResult(
                    action="constancia_error",
                    parameters={
                        "message": f"âŒ Error procesando constancia para {alumno.get('nombre', 'N/A')}",
                        "error": "processor_failed"
                    },
                    confidence=0.3
                )

        except Exception as e:
            self.logger.error(f"Error usando ConstanciaProcessor: {e}")
            return InterpretationResult(
                action="constancia_error",
                parameters={
                    "message": f"âŒ Error interno procesando constancia para {alumno.get('nombre', 'N/A')}",
                    "error": "internal_error"
                },
                confidence=0.1
            )



    def _generate_unified_continuation_response(self, user_query: str, continuation_type: str,
                                              ultimo_nivel: Dict, conversation_stack: list) -> str:
        """
        âœ… IMPLEMENTADO: Respuesta unificada para continuaciones usando PromptManager

        REEMPLAZA:
        - _generate_action_response()
        - _generate_selection_response()

        TIPOS:
        - action: "constancia para Ã©l", "CURP de ese"
        - selection: "del segundo", "nÃºmero 5"
        - confirmation: "sÃ­", "correcto"
        """
        try:
            # ðŸ†• USAR PROMPT MANAGER para respuesta unificada
            continuation_prompt = self.prompt_manager.get_unified_continuation_prompt(
                user_query, continuation_type, ultimo_nivel, conversation_stack
            )

            response = self.gemini_client.send_prompt_sync(continuation_prompt)
            if response:
                return response.strip()
            else:
                # Respuesta de fallback
                return f"âœ… Procesando {continuation_type} para: {user_query}"

        except Exception as e:
            self.logger.error(f"Error en respuesta unificada: {e}")
            return f"âœ… Procesando solicitud: {user_query}"

    def _detect_if_needs_sql_query(self, user_query: str, ultimo_nivel: Dict) -> bool:
        """Detecta si la consulta de continuaciÃ³n necesita ejecutar SQL en lugar de solo usar LLM"""
        try:
            user_lower = user_query.lower()

            # ðŸŽ¯ PRIORIDAD 1: Si es solicitud de CONSTANCIA, NO necesita SQL
            constancia_keywords = ["constancia", "certificado", "genera", "generar", "crear", "documento"]
            is_constancia_request = any(keyword in user_lower for keyword in constancia_keywords)

            if is_constancia_request:
                self.logger.debug(f"Es solicitud de constancia, NO necesita SQL: '{user_query}'")
                return False

            # Palabras clave que indican necesidad de consulta SQL
            sql_indicators = [
                "nombre", "nombres", "curp", "matrÃ­cula", "matricula", "grado", "grupo", "turno",
                "fecha", "direcciÃ³n", "direccion", "telÃ©fono", "telefono", "padre", "madre",
                "calificaciones", "promedio", "datos", "informaciÃ³n", "informacion",
                "dame", "muestra", "dime", "cuÃ¡l", "cual", "quiÃ©n", "quien"
            ]

            # Si la consulta contiene indicadores de datos especÃ­ficos
            if any(indicator in user_lower for indicator in sql_indicators):
                self.logger.debug(f"Detectado indicador SQL en: '{user_query}'")

                # Verificar si los datos previos son incompletos para responder
                previous_data = ultimo_nivel.get('data', [])

                if previous_data:
                    # Si los datos previos solo tienen fechas pero se piden nombres
                    if "nombre" in user_lower and all('nombre' not in str(item) for item in previous_data):
                        self.logger.debug("Datos previos no tienen nombres, necesita SQL")
                        return True

                    # Si se pide informaciÃ³n especÃ­fica que no estÃ¡ en los datos previos
                    if any(field in user_lower for field in ["curp", "matrÃ­cula", "grado", "grupo", "direcciÃ³n"]):
                        # Verificar si esa informaciÃ³n ya estÃ¡ disponible
                        has_requested_info = any(
                            any(field in str(item).lower() for item in previous_data)
                            for field in ["curp", "matrÃ­cula", "grado", "grupo", "direcciÃ³n"]
                            if field in user_lower
                        )
                        if not has_requested_info:
                            self.logger.debug("InformaciÃ³n especÃ­fica no disponible, necesita SQL")
                            return True

                return True

            return False

        except Exception as e:
            self.logger.error(f"Error detectando necesidad SQL: {e}")
            return False

    def _generate_sql_for_action_continuation(self, user_query: str, ultimo_nivel: Dict) -> Optional[str]:
        """
        Genera SQL para continuaciÃ³n basÃ¡ndose en datos previos
        âœ… USA PromptManager centralizado
        """
        try:
            previous_data = ultimo_nivel.get('data', [])
            previous_query = ultimo_nivel.get('query', '')

            if not previous_data:
                return None

            # ðŸ†• USAR PROMPT MANAGER en lugar de prompt hardcodeado
            continuation_sql_prompt = self.prompt_manager.get_sql_continuation_prompt(
                user_query, previous_data, previous_query, self._get_sql_context()
            )

            response = self.gemini_client.send_prompt_sync(continuation_sql_prompt)

            if response:
                # Limpiar y extraer SQL
                sql_query = response.strip()

                # Remover markdown si existe
                if "```sql" in sql_query:
                    sql_query = sql_query.split("```sql")[1].split("```")[0].strip()
                elif "```" in sql_query:
                    sql_query = sql_query.split("```")[1].strip()

                self.logger.debug(f"SQL de continuaciÃ³n generado: {sql_query}")
                return sql_query

            return None

        except Exception as e:
            self.logger.error(f"Error generando SQL de continuaciÃ³n: {e}")
            return None

    def _process_confirmation_continuation(self, user_query: str, conversation_stack: list) -> Optional[InterpretationResult]:
        """
        CONTINUACIÃ“N INTELIGENTE: El LLM razona quÃ© acciÃ³n ejecutar automÃ¡ticamente
        """
        try:
            # Obtener el Ãºltimo nivel que espera confirmaciÃ³n
            ultimo_nivel = None
            for level in reversed(conversation_stack):
                if level.get('awaiting') in ['confirmation', 'action', 'specification']:
                    ultimo_nivel = level
                    break

            if not ultimo_nivel:
                return InterpretationResult(
                    action="continuacion_error",
                    parameters={
                        "error": "No hay acciÃ³n pendiente de confirmaciÃ³n",
                        "message": "No encuentro ninguna acciÃ³n pendiente de confirmaciÃ³n. Â¿En quÃ© puedo ayudarte?"
                    },
                    confidence=0.3
                )

            # ðŸ†• VERIFICAR SI ES CONFIRMACIÃ“N DE CONSTANCIA REAL (no solicitud directa)
            # CORREGIDO: Solo procesar como confirmaciÃ³n si realmente es una confirmaciÃ³n simple
            is_real_constancia_confirmation = (
                ultimo_nivel.get("estado") == "vista_previa_generada" and
                self._is_simple_confirmation(user_query)  # Solo "sÃ­", "ok", "confirmar", etc.
            )

            if is_real_constancia_confirmation:
                self.logger.info(f"ðŸŽ¯ CONFIRMACIÃ“N SIMPLE DE CONSTANCIA DETECTADA")
                self.logger.info(f"   - Estado: {ultimo_nivel.get('estado', 'N/A')}")
                self.logger.info(f"   - Awaiting: {ultimo_nivel.get('awaiting', 'N/A')}")
                self.logger.info(f"   - Tipo constancia: {ultimo_nivel.get('tipo_constancia', 'N/A')}")

                confirmation_type = self._detect_confirmation_type(user_query, ultimo_nivel)
                self.logger.info(f"   - Tipo confirmaciÃ³n: {confirmation_type}")

                return self._process_constancia_confirmation(user_query, confirmation_type, ultimo_nivel)

            # ðŸ§  PROMPT INTELIGENTE: LLM decide quÃ© hacer automÃ¡ticamente
            intelligent_continuation_prompt = f"""
Eres el asistente inteligente de la escuela "PROF. MAXIMO GAMIZ FERNANDEZ".

CONTEXTO CONVERSACIONAL:
- Consulta anterior del usuario: "{ultimo_nivel.get('query', 'N/A')}"
- Mi respuesta anterior: "{ultimo_nivel.get('message', 'N/A')[:200]}..."
- Tipo de continuaciÃ³n esperada: {ultimo_nivel.get('awaiting', 'N/A')}
- Datos disponibles: {ultimo_nivel.get('row_count', 0)} elementos

NUEVA CONSULTA DEL USUARIO: "{user_query}"

ðŸ§  ANÃLISIS INTELIGENTE:
El usuario estÃ¡ confirmando/continuando con la acciÃ³n anterior. Debo:

1. RAZONAR sobre quÃ© acciÃ³n especÃ­fica ejecutar
2. GENERAR automÃ¡ticamente la consulta SQL necesaria
3. ACTUAR sin preguntar mÃ¡s

EJEMPLOS DE RAZONAMIENTO:
- Si era "estadÃ­sticas" â†’ Generar estadÃ­sticas detalladas por grado/turno/calificaciones
- Si era "lista de alumnos" â†’ Proporcionar mÃ¡s detalles o servicios
- Si era "bÃºsqueda" â†’ Ofrecer acciones especÃ­ficas para los resultados

ESTRUCTURA DE LA BASE DE DATOS:
{self._get_sql_context()}

INSTRUCCIONES:
1. Analiza quÃ© quiere el usuario basado en el contexto
2. Genera la consulta SQL apropiada para completar la acciÃ³n
3. NO preguntes mÃ¡s, EJECUTA la acciÃ³n

RESPONDE CON:
{{
    "accion_a_ejecutar": "generar_sql|proporcionar_info|ofrecer_servicios",
    "sql_query": "SELECT ... (si aplica)",
    "razonamiento": "Por quÃ© esta es la acciÃ³n correcta",
    "mensaje_usuario": "Respuesta natural para el usuario"
}}
"""

            # Enviar al LLM para decisiÃ³n inteligente
            response = self.gemini_client.send_prompt_sync(intelligent_continuation_prompt)
            continuation_decision = self._parse_json_response(response)

            if not continuation_decision:
                # Fallback: respuesta bÃ¡sica
                return InterpretationResult(
                    action="confirmacion_realizada",
                    parameters={
                        "message": f"Perfecto, continuando con la acciÃ³n basada en: '{ultimo_nivel.get('query', 'consulta anterior')}'",
                        "accion_confirmada": ultimo_nivel.get('query', ''),
                        "tipo": "confirmacion"
                    },
                    confidence=0.6
                )

            # Ejecutar la acciÃ³n decidida por el LLM
            accion = continuation_decision.get('accion_a_ejecutar', 'proporcionar_info')

            if accion == 'generar_sql' and continuation_decision.get('sql_query'):
                # ðŸš€ EJECUTAR SQL AUTOMÃTICAMENTE
                sql_query = continuation_decision.get('sql_query')
                self.logger.debug(f"LLM decidiÃ³ ejecutar SQL: {sql_query[:100]}...")

                result = self.sql_executor.execute_query(sql_query)

                if result.success:
                    # Generar respuesta final con los nuevos datos
                    final_response = self._validate_and_generate_response(
                        f"{ultimo_nivel.get('query', '')} (continuaciÃ³n: {user_query})",
                        sql_query, result.data, result.row_count
                    )

                    if final_response:
                        return InterpretationResult(
                            action="consulta_sql_exitosa",
                            parameters={
                                "sql_query": result.query_executed,
                                "data": result.data,
                                "row_count": result.row_count,
                                "message": final_response.get("respuesta_usuario", continuation_decision.get('mensaje_usuario', 'AcciÃ³n completada')),
                                "human_response": final_response.get("respuesta_usuario", continuation_decision.get('mensaje_usuario', 'AcciÃ³n completada')),
                                "auto_reflexion": final_response.get("reflexion_conversacional", {}),
                                "tipo": "continuacion_inteligente"
                            },
                            confidence=0.9
                        )

            # Si no es SQL o fallÃ³, usar respuesta del LLM
            return InterpretationResult(
                action="continuacion_inteligente",
                parameters={
                    "message": continuation_decision.get('mensaje_usuario', 'AcciÃ³n completada'),
                    "razonamiento": continuation_decision.get('razonamiento', ''),
                    "accion_confirmada": ultimo_nivel.get('query', ''),
                    "tipo": "continuacion"
                },
                confidence=0.8
            )

        except Exception as e:
            self.logger.error(f"Error en continuaciÃ³n inteligente: {e}")
            return None

    def _process_specification_continuation(self, user_query: str, conversation_stack: list) -> Optional[InterpretationResult]:
        """Procesa continuaciÃ³n de tipo ESPECIFICACIÃ“N (ej: 'de quÃ© tipo', 'con foto')"""
        try:
            # Obtener el Ãºltimo nivel que espera especificaciÃ³n
            ultimo_nivel = None
            for level in reversed(conversation_stack):
                if level.get('awaiting') == 'specification':
                    ultimo_nivel = level
                    break

            if not ultimo_nivel:
                return InterpretationResult(
                    action="continuacion_error",
                    parameters={
                        "error": "No hay especificaciÃ³n pendiente",
                        "message": "No encuentro ninguna especificaciÃ³n pendiente. Â¿QuÃ© informaciÃ³n especÃ­fica necesitas?"
                    },
                    confidence=0.3
                )

            # Generar respuesta de especificaciÃ³n
            response_message = f"Entendido, procesando especificaciÃ³n '{user_query}' para: '{ultimo_nivel.get('query', 'consulta anterior')}'"

            return InterpretationResult(
                action="especificacion_realizada",
                parameters={
                    "message": response_message,
                    "especificacion": user_query,
                    "consulta_original": ultimo_nivel.get('query', ''),
                    "tipo": "especificacion"
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error en especificaciÃ³n: {e}")
            return None

    def _get_sql_context(self) -> str:
        """Obtiene el contexto SQL (con cache)"""
        if self._sql_context is None:
            self._sql_context = self.database_analyzer.generate_sql_context()
        return self._sql_context

    def _detect_student_query_intention_centralized(self, user_query: str, conversation_context: str = "") -> bool:
        """
        PROMPT 1 CENTRALIZADO: Detecta si la consulta es sobre alumnos/estudiantes
        ðŸ†• MEJORADO: Ahora incluye contexto conversacional
        REEMPLAZA: _detect_student_query_intention() (mÃ©todo eliminado)
        """
        try:
            # ðŸŽ¯ USAR PROMPT MANAGER CENTRALIZADO CON CONTEXTO CONVERSACIONAL
            intention_prompt = self.prompt_manager.get_student_query_intention_prompt(user_query, conversation_context)

            # Enviar al LLM
            response = self.gemini_client.send_prompt_sync(intention_prompt)

            if response:
                # Parsear respuesta
                intention_result = self._parse_intention_response(response)

                if intention_result:
                    is_student_query = intention_result.get('es_consulta_alumnos', False)
                    query_type = intention_result.get('tipo_detectado', 'otro')
                    requires_context = intention_result.get('requiere_contexto', False)
                    continuation_type = intention_result.get('tipo_continuacion', 'nueva_consulta')

                    # Guardar informaciÃ³n adicional para uso posterior
                    self._current_query_type = query_type
                    self._requires_context = requires_context
                    self._continuation_type = continuation_type

                    self.logger.info(f"ðŸ§  ANÃLISIS CONVERSACIONAL:")
                    self.logger.info(f"   - Tipo detectado: {query_type}")
                    self.logger.info(f"   - Requiere contexto: {requires_context}")
                    self.logger.info(f"   - Tipo continuaciÃ³n: {continuation_type}")

                    return is_student_query
                else:
                    return False
            else:
                return False

        except Exception as e:
            self.logger.error(f"Error detectando intenciÃ³n centralizada: {e}")
            return False

    def _parse_intention_response(self, intention_response: str) -> Optional[Dict[str, Any]]:
        """Parsea la respuesta de detecciÃ³n de intenciÃ³n"""
        try:
            # Limpiar la respuesta
            clean_response = intention_response.strip()

            # Buscar JSON en la respuesta
            json_patterns = [
                r'```json\s*(.*?)\s*```',
                r'```\s*(.*?)\s*```',
                r'(\{.*?\})'
            ]

            for pattern in json_patterns:
                matches = re.findall(pattern, clean_response, re.DOTALL)
                if matches:
                    try:
                        import json
                        intention = json.loads(matches[0])
                        return intention
                    except json.JSONDecodeError:
                        continue

            # Si no encuentra JSON, intentar parsear directamente
            try:
                import json
                intention = json.loads(clean_response)
                return intention
            except json.JSONDecodeError:
                return None

        except Exception as e:
            return None

    def _generate_sql_with_strategy_centralized(self, user_query: str) -> Optional[str]:
        """
        PROMPT 2 CENTRALIZADO: Genera estrategia + SQL en un solo paso
        REEMPLAZA: _generate_sql_with_strategy() (mÃ©todo eliminado)
        """
        try:
            # ðŸŽ¯ USAR PROMPT MANAGER CENTRALIZADO
            combined_prompt = self.prompt_manager.get_sql_generation_prompt(user_query)

            # Enviar al LLM
            response = self.gemini_client.send_prompt_sync(combined_prompt)

            if response:
                # Extraer SQL de la respuesta
                sql_query = self._extract_sql_from_response(response)

                if sql_query:
                    return sql_query
                else:
                    return None
            else:
                return None

        except Exception as e:
            self.logger.error(f"Error generando SQL centralizado: {e}")
            return None

    def _validate_and_generate_response(self, user_query: str, sql_query: str, data: List[Dict], row_count: int) -> Optional[Dict]:
        """
        PROMPT 3 CON AUTO-REFLEXIÃ“N + FILTRO INTELIGENTE: Filtra datos + Valida SQL + Genera respuesta + Auto-reflexiona sobre continuaciÃ³n
        ðŸ†• AHORA USA PromptManager centralizado
        """
        try:
            # ðŸ§  PASO 1: APLICAR FILTRO INTELIGENTE FINAL
            filtered_data, filter_decision = self._intelligent_final_filter(user_query, data, sql_query)

            self.logger.debug(f"Filtro inteligente aplicado:")
            self.logger.debug(f"   - Datos originales: {len(data)} registros")
            self.logger.debug(f"   - Datos filtrados: {len(filtered_data)} registros")
            self.logger.debug(f"   - Resuelve consulta: {filter_decision.get('resuelve_consulta', 'N/A')}")

            # Si el filtro detectÃ³ que no resuelve la consulta, fallar temprano
            if not filter_decision.get('resuelve_consulta', True):
                self.logger.warning("Filtro inteligente detectÃ³ que los datos no resuelven la consulta")
                return None

            # Usar los datos filtrados para el resto del proceso
            final_data = filtered_data
            final_row_count = len(filtered_data)

            # Formatear los datos filtrados para el prompt de manera mÃ¡s clara
            data_summary = self._format_data_for_validation_prompt(final_data, final_row_count, sql_query)

            # ðŸ†• USAR PROMPT MANAGER en lugar de prompt hardcodeado
            validation_response_prompt = self.prompt_manager.get_validation_and_response_prompt(
                user_query, sql_query, data_summary, filter_decision, final_row_count, len(data)
            )

            # Enviar al LLM
            response = self.gemini_client.send_prompt_sync(validation_response_prompt)

            if response:
                # Verificar si la validaciÃ³n fallÃ³
                if "VALIDACION_FALLIDA" in response.upper():
                    return None

                # Parsear respuesta JSON con auto-reflexiÃ³n
                parsed_response = self._parse_json_response(response)
                if parsed_response and "respuesta_usuario" in parsed_response:
                    print(f"ðŸ§  DEBUG - Auto-reflexiÃ³n LLM: {parsed_response.get('reflexion_conversacional', {}).get('razonamiento', 'N/A')}")
                    return parsed_response
                else:
                    # Fallback: El LLM devolviÃ³ JSON crudo, extraer solo la respuesta del usuario
                    print(f"âš ï¸ DEBUG - Respuesta no es JSON vÃ¡lido, intentando extraer texto")

                    # Intentar extraer respuesta_usuario del JSON crudo
                    try:
                        # Buscar "respuesta_usuario" en el texto
                        match = re.search(r'"respuesta_usuario":\s*"([^"]*(?:\\.[^"]*)*)"', response, re.DOTALL)
                        if match:
                            respuesta_extraida = match.group(1).replace('\\"', '"').replace('\\n', '\n')
                            self.logger.debug("Respuesta extraÃ­da exitosamente")
                            return {
                                "respuesta_usuario": respuesta_extraida,
                                "reflexion_conversacional": {
                                    "espera_continuacion": False,
                                    "tipo_esperado": "none",
                                    "datos_recordar": {},
                                    "razonamiento": "Respuesta extraÃ­da de JSON crudo"
                                }
                            }
                    except Exception as e:
                        self.logger.error(f"Error extrayendo respuesta: {e}")

                    # Si todo falla, usar respuesta completa
                    return {
                        "respuesta_usuario": "Error procesando respuesta. Por favor, reformula tu consulta.",
                        "reflexion_conversacional": {
                            "espera_continuacion": False,
                            "tipo_esperado": "none",
                            "datos_recordar": {},
                            "razonamiento": "Error en procesamiento (fallback final)"
                        }
                    }
            else:
                return None

        except Exception as e:
            self.logger.error(f"Error en validaciÃ³n con auto-reflexiÃ³n: {e}")
            return None

    def _format_data_for_validation_prompt(self, data: List[Dict], row_count: int, sql_query: str) -> str:
        """
        Formatea los datos especÃ­ficamente para el prompt de validaciÃ³n
        """
        try:
            # Detectar si es una consulta COUNT
            if 'count(' in sql_query.lower() or any(key.lower() in ['total', 'count(*)', 'count', 'cantidad'] for row in data for key in row.keys()):
                if data and len(data) > 0:
                    first_row = data[0]
                    for key, value in first_row.items():
                        if key.lower() in ['total', 'count(*)', 'count', 'cantidad']:
                            return f"""
TIPO DE CONSULTA: COUNT (conteo)
RESULTADO NUMÃ‰RICO: {value}
INTERPRETACIÃ“N: La consulta devolviÃ³ un conteo de {value} alumnos
DATOS BRUTOS: {data}
"""

            # Para consultas SELECT normales
            return f"""
TIPO DE CONSULTA: SELECT (listado)
NÃšMERO DE REGISTROS: {row_count}
DATOS OBTENIDOS: {data if row_count <= 15 else data[:10]}
{"... y " + str(row_count - 10) + " registros adicionales" if row_count > 10 else ""}
"""

        except Exception as e:
            self.logger.error(f"Error formateando datos para validaciÃ³n: {e}")
            return f"DATOS: {row_count} registros obtenidos"

    def _extract_sql_from_response(self, response: str) -> Optional[str]:
        """Extrae SQL de la respuesta del LLM"""
        try:
            # Limpiar la respuesta
            clean_response = response.strip()

            # Buscar SQL en markdown
            if "```sql" in clean_response:
                sql_query = clean_response.split("```sql")[1].split("```")[0].strip()
                return sql_query
            elif "```" in clean_response:
                sql_query = clean_response.split("```")[1].strip()
                return sql_query
            else:
                # Asumir que toda la respuesta es SQL
                return clean_response

        except Exception as e:
            self.logger.error(f"Error extrayendo SQL: {e}")
            return None

    # ðŸ—‘ï¸ MÃ‰TODO DUPLICADO ELIMINADO: _parse_intention_response()
    # RAZÃ“N: Ya existe una versiÃ³n idÃ©ntica arriba (lÃ­nea 976)
    # MANTENER: Solo la primera versiÃ³n



    def _intelligent_final_filter(self, user_query: str, data: List[Dict], sql_query: str) -> Tuple[List[Dict], Dict[str, Any]]:
        """
        FILTRO INTELIGENTE FINAL: Asegura que los datos resuelvan exactamente la consulta original
        ðŸ†• AHORA USA PromptManager centralizado
        """
        try:
            self.logger.debug("Aplicando filtro inteligente final")
            self.logger.debug(f"   - Consulta: {user_query}")
            self.logger.debug(f"   - Datos obtenidos: {len(data)} registros")

            # ðŸ†• USAR PROMPT MANAGER en lugar de prompt hardcodeado
            filter_prompt = self.prompt_manager.get_filter_prompt(user_query, data, sql_query)

            # Enviar al LLM
            response = self.gemini_client.send_prompt_sync(filter_prompt)

            if response:
                # Parsear respuesta del filtro
                filter_decision = self._parse_filter_response(response)

                if filter_decision:
                    # Aplicar el filtrado segÃºn la decisiÃ³n
                    filtered_data = self._apply_filter_decision(data, filter_decision)

                    self.logger.debug("Filtro aplicado:")
                    self.logger.debug(f"   - AcciÃ³n: {filter_decision.get('accion_requerida')}")
                    self.logger.debug(f"   - Cantidad final: {filter_decision.get('cantidad_final')}")
                    self.logger.debug(f"   - Resuelve consulta: {filter_decision.get('resuelve_consulta')}")

                    return filtered_data, filter_decision
                else:
                    self.logger.warning("Error parseando decisiÃ³n de filtro")
                    return data, {"accion_requerida": "mantener", "resuelve_consulta": True}
            else:
                self.logger.warning("Error en filtro inteligente, manteniendo datos originales")
                return data, {"accion_requerida": "mantener", "resuelve_consulta": True}

        except Exception as e:
            self.logger.error(f"Error en filtro inteligente: {e}")
            return data, {"accion_requerida": "mantener", "resuelve_consulta": True}

    def _parse_filter_response(self, filter_response: str) -> Optional[Dict[str, Any]]:
        """Parsea la respuesta del filtro inteligente"""
        try:
            # Limpiar la respuesta
            clean_response = filter_response.strip()

            # Buscar JSON en la respuesta
            json_patterns = [
                r'```json\s*(.*?)\s*```',
                r'```\s*(.*?)\s*```',
                r'(\{.*?\})'
            ]

            for pattern in json_patterns:
                matches = re.findall(pattern, clean_response, re.DOTALL)
                if matches:
                    try:
                        filter_decision = json.loads(matches[0])
                        return filter_decision
                    except json.JSONDecodeError:
                        continue

            # Si no encuentra JSON, intentar parsear directamente
            try:
                filter_decision = json.loads(clean_response)
                return filter_decision
            except json.JSONDecodeError:
                return None

        except Exception as e:
            self.logger.error(f"Error parseando filtro: {e}")
            return None

    def _apply_filter_decision(self, data: List[Dict], filter_decision: Dict[str, Any]) -> List[Dict]:
        """Aplica la decisiÃ³n del filtro a los datos"""
        try:
            # accion = filter_decision.get('accion_requerida', 'mantener')  # No se usa actualmente
            cantidad_final = filter_decision.get('cantidad_final', len(data))
            campos_incluir = filter_decision.get('campos_incluir', 'todos')
            registros_seleccionar = filter_decision.get('registros_seleccionar', 'todos')

            # Filtrar registros por cantidad
            if registros_seleccionar == 'todos':
                if isinstance(cantidad_final, int) and cantidad_final < len(data):
                    filtered_data = data[:cantidad_final]
                else:
                    filtered_data = data
            else:
                # Seleccionar registros especÃ­ficos por Ã­ndice
                filtered_data = []
                for idx in registros_seleccionar:
                    if 0 <= idx < len(data):
                        filtered_data.append(data[idx])

            # Filtrar campos si es necesario
            if campos_incluir != 'todos' and isinstance(campos_incluir, list):
                final_data = []
                for record in filtered_data:
                    filtered_record = {}
                    for campo in campos_incluir:
                        if campo in record:
                            filtered_record[campo] = record[campo]
                    final_data.append(filtered_record)
                return final_data
            else:
                return filtered_data

        except Exception as e:
            self.logger.error(f"Error aplicando filtro: {e}")
            return data








    def _process_constancia_request(self, user_query: str, context: Dict[str, Any] = None) -> Optional[InterpretationResult]:
        """
        ðŸ†• PROCESA SOLICITUDES DE CONSTANCIAS DIRECTAMENTE
        Extrae nombre del alumno y tipo de constancia, luego genera vista previa
        """
        try:
            self.logger.info(f"ðŸŽ¯ _process_constancia_request INICIADO")
            self.logger.info(f"   - Query: '{user_query}'")
            self.logger.info(f"   - Context: {context is not None}")
            if context:
                self.logger.info(f"   - PDF Panel: {getattr(context, 'pdf_panel', None) is not None}")
                self.logger.info(f"   - Entidades detectadas: {getattr(context, 'detected_entities', {})}")

            # ðŸš€ USAR ENTIDADES DETECTADAS SI ESTÃN DISPONIBLES
            # ðŸ†• DEBUGGING PROFUNDO - RASTREAR DÃ“NDE ESTÃN LAS ENTIDADES
            self.logger.debug(f"ðŸ” DEBUGGING CONTEXTO COMPLETO:")
            self.logger.debug(f"   - Tipo de contexto: {type(context)}")
            self.logger.debug(f"   - Atributos del contexto: {dir(context)}")
            self.logger.debug(f"   - hasattr intention_info: {hasattr(context, 'intention_info')}")

            if hasattr(context, 'intention_info'):
                self.logger.debug(f"   - intention_info: {context.intention_info}")
                self.logger.debug(f"   - tipo intention_info: {type(context.intention_info)}")

            # ðŸ†• ACCESO CORRECTO A ENTIDADES DEL MASTER
            detected_entities = {}
            if hasattr(context, 'intention_info') and context.intention_info:
                detected_entities = context.intention_info.get('detected_entities', {})
                self.logger.debug(f"   - Entidades desde intention_info: {detected_entities}")

            # Fallback: buscar en contexto directo (compatibilidad)
            if not detected_entities and context:
                detected_entities = getattr(context, 'detected_entities', {})
                self.logger.debug(f"   - Entidades desde contexto directo: {detected_entities}")

            self.logger.debug(f"ðŸ” ENTIDADES FINALES DETECTADAS: {detected_entities}")

            # ðŸ†• SOLO USAR ENTIDADES DEL MASTER (sin fallbacks)
            if not detected_entities:
                self.logger.error("âŒ No hay entidades detectadas del Master para constancia")
                return InterpretationResult(
                    action="constancia_error",
                    parameters={
                        "message": "âŒ Error: No se detectaron entidades para la constancia",
                        "error": "no_entities_detected"
                    },
                    confidence=0.1
                )

            self.logger.info("ðŸŽ¯ USANDO ENTIDADES PRE-DETECTADAS DEL MASTER")
            constancia_info = {
                'es_solicitud_constancia': True,
                'nombre_alumno': detected_entities.get('nombres', [None])[0] if detected_entities.get('nombres') else None,
                'tipo_constancia': detected_entities.get('tipo_constancia'),
                'fuente_datos': detected_entities.get('fuente_datos', 'base_datos'),
                'contexto_especifico': detected_entities.get('contexto_especifico'),
                'incluir_foto': detected_entities.get('incluir_foto', False),  # ðŸ†• FOTO DEL MASTER
                'confianza': 0.95
            }
            self.logger.info(f"   - Info construida: {constancia_info}")

            if not constancia_info:
                return InterpretationResult(
                    action="constancia_error",
                    parameters={
                        "message": "No pude entender quÃ© constancia necesitas. Por favor especifica el nombre del alumno y tipo de constancia.",
                        "error": "info_extraction_failed"
                    },
                    confidence=0.3
                )

            # ðŸ†• VERIFICAR SI HAY PDF CARGADO PARA TRANSFORMACIÃ“N
            pdf_panel = getattr(context, 'pdf_panel', None)
            if pdf_panel and hasattr(pdf_panel, 'original_pdf') and pdf_panel.original_pdf:
                # ðŸŽ¯ DISTINGUIR ENTRE PDF EXTERNO VS VISTA PREVIA GENERADA
                is_external_pdf = self._is_external_pdf_loaded(pdf_panel)
                is_transformation_request = self._is_transformation_request(constancia_info, detected_entities)

                self.logger.info(f"ðŸ“„ ANÃLISIS DE PDF:")
                self.logger.info(f"   - PDF cargado: {pdf_panel.original_pdf}")
                self.logger.info(f"   - Es PDF externo: {is_external_pdf}")
                self.logger.info(f"   - Es solicitud de transformaciÃ³n: {is_transformation_request}")

                # SOLO procesar como transformaciÃ³n si es PDF externo Y solicitud de transformaciÃ³n
                if is_external_pdf and is_transformation_request:
                    self.logger.info(f"ðŸ”„ TRANSFORMACIÃ“N: PDF externo + solicitud de transformaciÃ³n")
                    return self._process_constancia_from_pdf(constancia_info, pdf_panel, context)
                else:
                    self.logger.info(f"ðŸ“Š NUEVA CONSTANCIA: Ignorando PDF de vista previa, generando nueva constancia desde BD")

            # FLUJO NORMAL: Buscar alumno por nombre en BD
            alumno = self._find_student_by_name(constancia_info.get("nombre_alumno", ""))

            if not alumno:
                return InterpretationResult(
                    action="constancia_error",
                    parameters={
                        "message": f"No encontrÃ© al alumno '{constancia_info.get('nombre_alumno', '')}'. Verifica el nombre e intenta nuevamente.",
                        "error": "student_not_found",
                        "searched_name": constancia_info.get('nombre_alumno', ''),
                        "suggestion": "Intenta con el nombre completo o usa 'buscar alumnos' para ver opciones disponibles."
                    },
                    confidence=0.3
                )

            # ðŸ†• VERIFICAR SI ES CASO DE DEMASIADAS COINCIDENCIAS
            if alumno.get('_too_many_matches'):
                return InterpretationResult(
                    action="constancia_error",
                    parameters={
                        "message": alumno.get('message', 'Demasiadas coincidencias encontradas.'),
                        "error": "too_many_matches",
                        "total_found": alumno.get('total_found', 0),
                        "search_term": alumno.get('search_term', ''),
                        "suggestion": f"EncontrÃ© {alumno.get('total_found', 0)} estudiantes. Intenta con nombre y apellido completos."
                    },
                    confidence=0.7
                )

            # ðŸ†• INFORMAR SOBRE MÃšLTIPLES COINCIDENCIAS SI LAS HAY
            if alumno.get('_multiple_matches'):
                multiple_info = alumno['_multiple_matches']
                self.logger.info(f"ðŸ“‹ Se encontraron {multiple_info['total_found']} coincidencias para '{multiple_info['search_term']}'")
                self.logger.info(f"âœ… Seleccionado: {multiple_info['selected']}")
                if len(multiple_info['options']) > 1:
                    self.logger.info(f"ðŸ“ Otras opciones: {', '.join(multiple_info['options'][1:])}")

                # Limpiar la informaciÃ³n temporal antes de continuar
                del alumno['_multiple_matches']

            # Generar constancia en modo vista previa
            from app.core.service_provider import ServiceProvider
            service_provider = ServiceProvider.get_instance()
            constancia_service = service_provider.constancia_service

            # ðŸ”§ NORMALIZAR TIPO DE CONSTANCIA
            tipo_raw = constancia_info.get("tipo_constancia", "estudio")
            tipo_constancia = self._normalize_constancia_type(tipo_raw)
            incluir_foto = constancia_info.get("incluir_foto", False)

            self.logger.info(f"ðŸŽ¯ GENERANDO CONSTANCIA:")
            self.logger.info(f"   - Tipo: {tipo_constancia}")
            self.logger.info(f"   - Alumno: {alumno.get('nombre')} (ID: {alumno.get('id')})")
            self.logger.info(f"   - Incluir foto: {incluir_foto}")
            self.logger.info(f"   - Preview mode: True")

            # Generar vista previa
            self.logger.info("ðŸ”„ Llamando a constancia_service.generar_constancia_para_alumno()...")
            success, message, data = constancia_service.generar_constancia_para_alumno(
                alumno.get("id"), tipo_constancia, incluir_foto, preview_mode=True
            )

            self.logger.info(f"ðŸ“Š RESULTADO DEL SERVICIO:")
            self.logger.info(f"   - Success: {success}")
            self.logger.info(f"   - Message: {message}")
            self.logger.info(f"   - Data: {data is not None} ({'keys: ' + str(list(data.keys())) if data else 'None'})")

            if success and data:
                # Generar respuesta con AUTO-REFLEXIÃ“N sobre constancia
                self.logger.debug(f"ðŸ§  Generando auto-reflexiÃ³n para constancia de {tipo_constancia}")
                response_with_reflection = self._generate_constancia_response_with_reflection(
                    alumno, tipo_constancia, data
                )

                if response_with_reflection:
                    self.logger.debug(f"âœ… Auto-reflexiÃ³n generada: {response_with_reflection.get('reflexion_conversacional', {})}")
                    return InterpretationResult(
                        action="constancia_preview",  # AcciÃ³n especial para ChatEngine
                        parameters={
                            "message": response_with_reflection.get("respuesta_usuario", "Vista previa generada"),
                            "data": data,
                            "files": [data.get("ruta_archivo")] if data.get("ruta_archivo") else [],
                            "alumno": alumno,
                            "tipo_constancia": tipo_constancia,
                            "auto_reflexion": response_with_reflection.get("reflexion_conversacional", {})
                        },
                        confidence=0.95
                    )
                else:
                    self.logger.warning("âŒ No se pudo generar auto-reflexiÃ³n, usando respuesta simple")
                    return InterpretationResult(
                        action="constancia_preview",
                        parameters={
                            "message": f"Vista previa de constancia de {tipo_constancia} generada para {alumno.get('nombre')}",
                            "data": data,
                            "files": [data.get("ruta_archivo")] if data.get("ruta_archivo") else [],
                            "alumno": alumno,
                            "tipo_constancia": tipo_constancia
                        },
                        confidence=0.9
                    )
            else:
                return InterpretationResult(
                    action="constancia_error",
                    parameters={
                        "message": f"Error generando constancia: {message}",
                        "error": "generation_failed",
                        "service_message": message
                    },
                    confidence=0.3
                )

        except Exception as e:
            self.logger.error(f"Error procesando constancia: {e}")
            return InterpretationResult(
                action="constancia_error",
                parameters={
                    "message": "Error interno procesando la constancia. Intenta nuevamente.",
                    "error": "internal_error",
                    "exception": str(e)
                },
                confidence=0.1
            )

    # ðŸ—‘ï¸ MÃ‰TODO ELIMINADO: _extract_constancia_info
    # RAZÃ“N: Ahora solo usamos entidades del Master, sin fallbacks LLM

    # ðŸ—‘ï¸ MÃ‰TODO ELIMINADO: _build_intelligent_context
    # RAZÃ“N: Ya no se usa, era parte del sistema LLM fallback eliminado

    def _normalize_constancia_type(self, tipo_raw: str) -> str:
        """Normaliza el tipo de constancia a los valores esperados por el servicio"""
        if not tipo_raw:
            return "estudio"

        tipo_lower = tipo_raw.lower().strip()

        # Mapeo de variaciones a tipos vÃ¡lidos
        tipo_mapping = {
            # Estudios/Estudio
            "estudios": "estudio",
            "estudio": "estudio",
            "de estudios": "estudio",
            "de estudio": "estudio",

            # Calificaciones
            "calificaciones": "calificaciones",
            "calificacion": "calificaciones",
            "notas": "calificaciones",
            "boleta": "calificaciones",

            # Traslado
            "traslado": "traslado",
            "transferencia": "traslado",
            "cambio": "traslado"
        }

        # Buscar coincidencia exacta
        if tipo_lower in tipo_mapping:
            normalized = tipo_mapping[tipo_lower]
            self.logger.debug(f"ðŸ”§ Tipo normalizado: '{tipo_raw}' â†’ '{normalized}'")
            return normalized

        # Buscar coincidencia parcial
        for key, value in tipo_mapping.items():
            if key in tipo_lower:
                self.logger.debug(f"ðŸ”§ Tipo normalizado (parcial): '{tipo_raw}' â†’ '{value}'")
                return value

        # Por defecto, estudio
        self.logger.warning(f"âš ï¸ Tipo de constancia no reconocido: '{tipo_raw}', usando 'estudio' por defecto")
        return "estudio"

    def _find_student_by_name(self, nombre: str) -> Optional[Dict[str, Any]]:
        """Busca alumno por nombre con manejo inteligente de mÃºltiples coincidencias y tolerancia a errores"""
        try:
            from app.core.service_provider import ServiceProvider
            service_provider = ServiceProvider.get_instance()
            alumno_service = service_provider.alumno_service

            # ðŸŽ¯ ESTRATEGIA 1: BÃºsqueda exacta
            alumnos = alumno_service.buscar_alumnos(nombre)

            if not alumnos:
                # ðŸŽ¯ ESTRATEGIA 2: BÃºsqueda con tolerancia a errores tipogrÃ¡ficos
                self.logger.info(f"ðŸ” No se encontrÃ³ '{nombre}', intentando bÃºsqueda con tolerancia a errores...")
                alumnos = self._fuzzy_search_students(nombre, alumno_service)

            if not alumnos:
                self.logger.debug(f"âŒ No se encontraron alumnos con '{nombre}' (incluso con tolerancia a errores)")
                return None
            elif len(alumnos) == 1:
                # âœ… UNA SOLA COINCIDENCIA - PERFECTO
                self.logger.debug(f"âœ… Alumno Ãºnico encontrado: {alumnos[0].get('nombre', 'N/A')}")
                primer_alumno = alumnos[0]
                if hasattr(primer_alumno, 'to_dict'):
                    return primer_alumno.to_dict()
                else:
                    return primer_alumno
            else:
                # ðŸ” MÃšLTIPLES COINCIDENCIAS - MANEJAR INTELIGENTEMENTE
                return self._handle_multiple_students(alumnos, nombre)

        except Exception as e:
            self.logger.error(f"Error buscando alumno: {e}")
            return None

    def _fuzzy_search_students(self, nombre_buscado: str, alumno_service) -> List:
        """BÃºsqueda con tolerancia a errores tipogrÃ¡ficos"""
        try:
            # ðŸŽ¯ ESTRATEGIAS DE BÃšSQUEDA TOLERANTE:

            # 1. Buscar por cada palabra del nombre
            palabras = nombre_buscado.split()
            candidatos = []

            for palabra in palabras:
                if len(palabra) >= 3:  # Solo palabras de 3+ caracteres
                    resultados = alumno_service.buscar_alumnos(palabra)
                    candidatos.extend(resultados)

            # 2. Buscar variaciones comunes de nombres
            variaciones = self._generate_name_variations(nombre_buscado)
            for variacion in variaciones:
                resultados = alumno_service.buscar_alumnos(variacion)
                candidatos.extend(resultados)

            # 3. Eliminar duplicados
            alumnos_unicos = []
            ids_vistos = set()

            for alumno in candidatos:
                alumno_dict = alumno.to_dict() if hasattr(alumno, 'to_dict') else alumno
                alumno_id = alumno_dict.get('id')
                if alumno_id not in ids_vistos:
                    ids_vistos.add(alumno_id)
                    alumnos_unicos.append(alumno_dict)

            # 4. Filtrar por similitud de nombre
            if alumnos_unicos:
                alumnos_filtrados = self._filter_by_name_similarity(nombre_buscado, alumnos_unicos)
                if alumnos_filtrados:
                    self.logger.info(f"âœ… Encontrados {len(alumnos_filtrados)} candidatos con bÃºsqueda tolerante")
                    return alumnos_filtrados

            return []

        except Exception as e:
            self.logger.error(f"Error en bÃºsqueda tolerante: {e}")
            return []

    def _generate_name_variations(self, nombre: str) -> List[str]:
        """Genera variaciones comunes de nombres para bÃºsqueda tolerante"""
        variaciones = []
        nombre_lower = nombre.lower()

        # Correcciones comunes de nombres
        correcciones = {
            'habriela': 'gabriela',
            'habriel': 'gabriel',
            'nataly': 'natalia',
            'nathalia': 'natalia',
            'maria': 'marÃ­a',
            'jose': 'josÃ©',
            'jesus': 'jesÃºs',
            'andres': 'andrÃ©s',
            'adrian': 'adriÃ¡n',
            'sebastian': 'sebastiÃ¡n'
        }

        # Aplicar correcciones
        for error, correccion in correcciones.items():
            if error in nombre_lower:
                variacion = nombre_lower.replace(error, correccion)
                variaciones.append(variacion.title())

        # Variaciones sin acentos
        sin_acentos = nombre.replace('Ã¡', 'a').replace('Ã©', 'e').replace('Ã­', 'i').replace('Ã³', 'o').replace('Ãº', 'u')
        if sin_acentos != nombre:
            variaciones.append(sin_acentos)

        # Variaciones con acentos comunes
        con_acentos = nombre.replace('a', 'Ã¡').replace('e', 'Ã©').replace('i', 'Ã­').replace('o', 'Ã³').replace('u', 'Ãº')
        if con_acentos != nombre:
            variaciones.append(con_acentos)

        return list(set(variaciones))  # Eliminar duplicados

    def _filter_by_name_similarity(self, nombre_buscado: str, alumnos: List[Dict]) -> List[Dict]:
        """Filtra alumnos por similitud de nombre usando algoritmo simple"""
        try:
            candidatos_con_score = []
            nombre_buscado_lower = nombre_buscado.lower()

            for alumno in alumnos:
                nombre_alumno = alumno.get('nombre', '').lower()

                # Calcular similitud simple
                score = self._calculate_name_similarity(nombre_buscado_lower, nombre_alumno)

                # Solo incluir si tiene similitud razonable (>= 0.6)
                if score >= 0.6:
                    candidatos_con_score.append((alumno, score))

            # Ordenar por score descendente
            candidatos_con_score.sort(key=lambda x: x[1], reverse=True)

            # Devolver solo los alumnos (sin scores)
            return [alumno for alumno, score in candidatos_con_score]

        except Exception as e:
            self.logger.error(f"Error filtrando por similitud: {e}")
            return alumnos

    def _calculate_name_similarity(self, nombre1: str, nombre2: str) -> float:
        """Calcula similitud entre dos nombres usando algoritmo simple"""
        try:
            # Algoritmo simple de similitud basado en caracteres comunes
            if not nombre1 or not nombre2:
                return 0.0

            # Convertir a conjuntos de caracteres
            chars1 = set(nombre1.replace(' ', ''))
            chars2 = set(nombre2.replace(' ', ''))

            # Calcular intersecciÃ³n y uniÃ³n
            interseccion = len(chars1.intersection(chars2))
            union = len(chars1.union(chars2))

            # Similitud de Jaccard
            if union == 0:
                return 0.0

            jaccard = interseccion / union

            # Bonus por palabras comunes
            palabras1 = set(nombre1.split())
            palabras2 = set(nombre2.split())
            palabras_comunes = len(palabras1.intersection(palabras2))

            # Score final combinado
            score = jaccard + (palabras_comunes * 0.2)
            return min(score, 1.0)  # MÃ¡ximo 1.0

        except Exception as e:
            self.logger.error(f"Error calculando similitud: {e}")
            return 0.0

    def _handle_multiple_students(self, alumnos: List, nombre_buscado: str) -> Optional[Dict[str, Any]]:
        """Maneja mÃºltiples coincidencias de estudiantes de forma inteligente"""
        try:
            self.logger.info(f"ðŸ” Encontradas {len(alumnos)} coincidencias para '{nombre_buscado}'")

            # ðŸŽ¯ ESTRATEGIA 1: Buscar coincidencia exacta (nombre completo)
            for alumno in alumnos:
                alumno_dict = alumno.to_dict() if hasattr(alumno, 'to_dict') else alumno
                nombre_completo = alumno_dict.get('nombre', '').upper()
                if nombre_completo == nombre_buscado.upper():
                    self.logger.info(f"âœ… Coincidencia exacta encontrada: {nombre_completo}")
                    return alumno_dict

            # ðŸŽ¯ ESTRATEGIA 2: Si hay pocas coincidencias (2-5), tomar la primera y avisar
            if len(alumnos) <= 5:
                primer_alumno = alumnos[0]
                alumno_dict = primer_alumno.to_dict() if hasattr(primer_alumno, 'to_dict') else primer_alumno

                # Crear lista de nombres para mostrar al usuario
                nombres_encontrados = []
                for alumno in alumnos[:5]:
                    alumno_temp = alumno.to_dict() if hasattr(alumno, 'to_dict') else alumno
                    nombres_encontrados.append(alumno_temp.get('nombre', 'N/A'))

                self.logger.info(f"âš ï¸ MÃºltiples coincidencias, usando primera: {alumno_dict.get('nombre', 'N/A')}")
                self.logger.info(f"ðŸ“‹ Opciones encontradas: {', '.join(nombres_encontrados)}")

                # Agregar informaciÃ³n de mÃºltiples coincidencias al alumno
                alumno_dict['_multiple_matches'] = {
                    'total_found': len(alumnos),
                    'options': nombres_encontrados,
                    'selected': alumno_dict.get('nombre', 'N/A'),
                    'search_term': nombre_buscado
                }

                return alumno_dict

            # ðŸŽ¯ ESTRATEGIA 3: Si hay muchas coincidencias (6+), devolver informaciÃ³n especial
            else:
                self.logger.warning(f"âŒ Demasiadas coincidencias ({len(alumnos)}) para '{nombre_buscado}'. Se requiere nombre mÃ¡s especÃ­fico.")

                # Devolver diccionario especial para indicar mÃºltiples coincidencias
                return {
                    '_too_many_matches': True,
                    'total_found': len(alumnos),
                    'search_term': nombre_buscado,
                    'message': f"EncontrÃ© {len(alumnos)} estudiantes con '{nombre_buscado}'. Por favor, sÃ© mÃ¡s especÃ­fico con el nombre."
                }

        except Exception as e:
            self.logger.error(f"Error manejando mÃºltiples estudiantes: {e}")
            # En caso de error, devolver el primero como fallback
            primer_alumno = alumnos[0] if alumnos else None
            if primer_alumno:
                return primer_alumno.to_dict() if hasattr(primer_alumno, 'to_dict') else primer_alumno
            return None

    def _is_external_pdf_loaded(self, pdf_panel) -> bool:
        """Determina si el PDF cargado es externo (no generado por vista previa)"""
        try:
            if not pdf_panel or not hasattr(pdf_panel, 'original_pdf'):
                return False

            pdf_path = pdf_panel.original_pdf

            # ðŸŽ¯ CRITERIOS PARA DETECTAR PDF EXTERNO:
            # 1. NO estÃ¡ en directorio temporal del sistema
            # 2. NO contiene palabras de vista previa en el nombre
            # 3. Fue cargado por el usuario (no generado automÃ¡ticamente)

            import tempfile

            temp_dir = tempfile.gettempdir()

            # Verificar si estÃ¡ en directorio temporal
            is_in_temp = pdf_path.startswith(temp_dir)

            # Verificar si contiene palabras de vista previa
            preview_keywords = ['preview', 'constancia_preview', 'vista_previa', 'temp_constancia']
            has_preview_keywords = any(keyword in pdf_path.lower() for keyword in preview_keywords)

            # Verificar si tiene atributo de origen (si el panel lo soporta)
            is_user_loaded = getattr(pdf_panel, 'is_user_loaded', True)  # Por defecto True si no existe el atributo

            self.logger.debug(f"ðŸ” ANÃLISIS PDF EXTERNO:")
            self.logger.debug(f"   - Ruta: {pdf_path}")
            self.logger.debug(f"   - En directorio temporal: {is_in_temp}")
            self.logger.debug(f"   - Tiene palabras de vista previa: {has_preview_keywords}")
            self.logger.debug(f"   - Cargado por usuario: {is_user_loaded}")

            # Es externo si NO estÃ¡ en temp Y NO tiene palabras de preview Y fue cargado por usuario
            is_external = not is_in_temp and not has_preview_keywords and is_user_loaded

            self.logger.debug(f"   - RESULTADO: Es PDF externo = {is_external}")
            return is_external

        except Exception as e:
            self.logger.error(f"Error verificando PDF externo: {e}")
            # En caso de error, asumir que es externo para ser conservadores
            return True

    def _is_transformation_request(self, constancia_info: Dict, detected_entities: Dict) -> bool:
        """Determina si la solicitud es para transformar un PDF existente"""
        try:
            # ðŸŽ¯ CRITERIOS PARA DETECTAR TRANSFORMACIÃ“N:
            # 1. Sub-intenciÃ³n es "transformar_pdf"
            # 2. AcciÃ³n principal contiene "transformar"
            # 3. Fuente de datos es "pdf_cargado"
            # 4. Contexto especÃ­fico es "conversion_formato"

            sub_intention = detected_entities.get('sub_intention', '')
            accion_principal = detected_entities.get('accion_principal', '')
            fuente_datos = detected_entities.get('fuente_datos', '')
            contexto_especifico = detected_entities.get('contexto_especifico', '')

            # TambiÃ©n verificar en constancia_info por compatibilidad
            fuente_info = constancia_info.get('fuente_datos', '') if constancia_info else ''

            is_transform_sub = sub_intention == 'transformar_pdf'
            is_transform_action = 'transformar' in accion_principal.lower()
            is_pdf_source = fuente_datos == 'pdf_cargado' or fuente_info == 'pdf_cargado'
            is_conversion_context = 'conversion' in contexto_especifico.lower()

            self.logger.debug(f"ðŸ” ANÃLISIS TRANSFORMACIÃ“N:")
            self.logger.debug(f"   - Sub-intenciÃ³n transformar: {is_transform_sub}")
            self.logger.debug(f"   - AcciÃ³n transformar: {is_transform_action}")
            self.logger.debug(f"   - Fuente PDF: {is_pdf_source}")
            self.logger.debug(f"   - Contexto conversiÃ³n: {is_conversion_context}")

            # Es transformaciÃ³n si cumple al menos 2 criterios
            transformation_score = sum([is_transform_sub, is_transform_action, is_pdf_source, is_conversion_context])
            is_transformation = transformation_score >= 2

            self.logger.debug(f"   - RESULTADO: Es transformaciÃ³n = {is_transformation} (score: {transformation_score}/4)")
            return is_transformation

        except Exception as e:
            self.logger.error(f"Error verificando transformaciÃ³n: {e}")
            # En caso de error, asumir que NO es transformaciÃ³n
            return False

    def _generate_constancia_response_with_reflection(self, alumno: Dict, tipo_constancia: str, data: Dict) -> Optional[Dict]:
        """Genera respuesta con auto-reflexiÃ³n especÃ­fica para constancias"""
        try:
            response_prompt = f"""
Eres un comunicador experto para sistema escolar con CAPACIDAD DE AUTO-REFLEXIÃ“N especializada en CONSTANCIAS.

CONTEXTO DE CONSTANCIA GENERADA:
- Alumno: {alumno.get('nombre', 'N/A')}
- Tipo: {tipo_constancia}
- Estado: Vista previa generada exitosamente
- Archivo: {data.get('ruta_archivo', 'N/A')}

INSTRUCCIONES PRINCIPALES:
1. Genera respuesta profesional informando sobre la vista previa
2. ðŸ†• AUTO-REFLEXIONA especÃ­ficamente sobre constancias

ðŸ§  AUTO-REFLEXIÃ“N ESPECIALIZADA EN CONSTANCIAS:
DespuÃ©s de generar tu respuesta, reflexiona como un secretario escolar experto en documentos:

ANÃLISIS REFLEXIVO ESPECÃFICO:
- Â¿Acabo de mostrar una vista previa de constancia que requiere confirmaciÃ³n del usuario?
- Â¿El usuario necesitarÃ¡ decidir quÃ© hacer con esta constancia (confirmar, abrir, cancelar)?
- Â¿DeberÃ­a recordar los datos de esta constancia para futuras acciones?
- Â¿QuÃ© tipo de respuesta esperarÃ­a tÃ­picamente despuÃ©s de mostrar una vista previa?

DECISIÃ“N CONVERSACIONAL PARA CONSTANCIAS:
Si tu respuesta espera continuaciÃ³n, especifica:
- Tipo esperado: "confirmation" (confirmar/abrir/cancelar)
- Datos a recordar: informaciÃ³n de la constancia y alumno
- Razonamiento: por quÃ© esperas confirmaciÃ³n

FORMATO DE RESPUESTA:
{{
  "respuesta_usuario": "Tu respuesta profesional sobre la vista previa aquÃ­",
  "reflexion_conversacional": {{
    "espera_continuacion": true|false,
    "tipo_esperado": "confirmation|none",
    "datos_recordar": {{
      "query": "constancia generada",
      "alumno": {{"id": {alumno.get('id')}, "nombre": "{alumno.get('nombre')}"}},
      "tipo_constancia": "{tipo_constancia}",
      "archivo_temporal": "{data.get('ruta_archivo', '')}",
      "estado": "vista_previa_generada",
      "acciones_disponibles": ["confirmar", "abrir", "cancelar"]
    }},
    "razonamiento": "ExplicaciÃ³n de por quÃ© esperas confirmaciÃ³n o no"
  }}
}}
"""

            response = self.gemini_client.send_prompt_sync(response_prompt)
            return self._parse_json_response(response)

        except Exception as e:
            self.logger.error(f"Error generando respuesta de constancia: {e}")
            return None

    def _is_simple_confirmation(self, user_query: str) -> bool:
        """Detecta si es una confirmaciÃ³n simple (sÃ­, ok, confirmar) vs solicitud especÃ­fica"""
        try:
            user_lower = user_query.lower().strip()

            # Confirmaciones simples
            simple_confirmations = [
                "sÃ­", "si", "ok", "vale", "confirmar", "confirmo", "adelante",
                "procede", "continÃºa", "continua", "hazlo", "genÃ©rala", "generala"
            ]

            # Si es solo una palabra de confirmaciÃ³n simple
            if user_lower in simple_confirmations:
                return True

            # Si contiene palabras especÃ­ficas de constancia, NO es confirmaciÃ³n simple
            constancia_keywords = [
                "constancia", "certificado", "documento", "estudios",
                "calificaciones", "traslado", "generar", "crear"
            ]

            for keyword in constancia_keywords:
                if keyword in user_lower:
                    return False  # Es solicitud especÃ­fica, no confirmaciÃ³n simple

            return False

        except Exception as e:
            self.logger.error(f"Error detectando confirmaciÃ³n simple: {e}")
            return False

    def _detect_confirmation_type(self, user_query: str, context_item: Dict) -> str:
        """Detecta el tipo especÃ­fico de confirmaciÃ³n para constancias"""
        try:
            user_lower = user_query.lower().strip()

            # Confirmaciones para guardar/finalizar
            if any(word in user_lower for word in ["confirmar", "sÃ­", "si", "estÃ¡ bien", "correcto", "guardar", "finalizar"]):
                return "confirm"

            # Confirmaciones para abrir en navegador
            elif any(word in user_lower for word in ["abrir", "navegador", "browser", "ver", "mostrar"]):
                return "open"

            # Confirmaciones para cancelar
            elif any(word in user_lower for word in ["cancelar", "no", "cerrar", "salir", "descartar"]):
                return "cancel"

            # Por defecto, asumir confirmaciÃ³n
            else:
                return "confirm"

        except Exception as e:
            self.logger.error(f"Error detectando tipo de confirmaciÃ³n: {e}")
            return "confirm"

    def _process_constancia_confirmation(self, user_query: str, confirmation_type: str, context_item: Dict) -> Optional[InterpretationResult]:
        """Procesa confirmaciÃ³n especÃ­fica para constancias con vista previa"""
        try:
            alumno = context_item.get("alumno", {})
            tipo_constancia = context_item.get("tipo_constancia", "estudio")
            archivo_temporal = context_item.get("archivo_temporal", "")

            self.logger.debug(f"Procesando confirmaciÃ³n de constancia: {confirmation_type}")

            if confirmation_type == "confirm":
                # Confirmar y generar constancia definitiva
                return self._confirm_constancia_final(alumno, tipo_constancia, archivo_temporal)

            elif confirmation_type == "open":
                # Abrir en navegador
                return self._open_constancia_in_browser(alumno, tipo_constancia, archivo_temporal)

            elif confirmation_type == "cancel":
                # Cancelar y limpiar archivos temporales
                return self._cancel_constancia(archivo_temporal)

            else:
                # ConfirmaciÃ³n genÃ©rica
                return self._confirm_constancia_final(alumno, tipo_constancia, archivo_temporal)

        except Exception as e:
            self.logger.error(f"Error procesando confirmaciÃ³n de constancia: {e}")
            return None

    def _confirm_constancia_final(self, alumno: Dict, tipo_constancia: str, archivo_temporal: str) -> InterpretationResult:
        """Confirma y genera constancia definitiva"""
        try:
            from app.core.service_provider import ServiceProvider
            service_provider = ServiceProvider.get_instance()
            constancia_service = service_provider.constancia_service

            # Generar constancia definitiva (no preview)
            success, message, data = constancia_service.generar_constancia_para_alumno(
                alumno.get("id"), tipo_constancia, False, preview_mode=False
            )

            if success:
                response_text = f"âœ… Constancia de {tipo_constancia} para {alumno.get('nombre')} generada y guardada exitosamente."

                return InterpretationResult(
                    action="constancia_confirmada",
                    parameters={
                        "message": response_text,
                        "alumno": alumno,
                        "tipo_constancia": tipo_constancia,
                        "archivo_final": data.get("ruta_archivo") if data else None,
                        "estado": "confirmada"
                    },
                    confidence=0.95
                )
            else:
                return InterpretationResult(
                    action="constancia_error",
                    parameters={
                        "message": f"âŒ Error generando constancia definitiva: {message}",
                        "error": "generation_failed"
                    },
                    confidence=0.3
                )

        except Exception as e:
            self.logger.error(f"Error confirmando constancia: {e}")
            return InterpretationResult(
                action="constancia_error",
                parameters={
                    "message": "âŒ Error interno confirmando constancia",
                    "error": "internal_error"
                },
                confidence=0.1
            )

    def _open_constancia_in_browser(self, alumno: Dict, tipo_constancia: str, archivo_temporal: str) -> InterpretationResult:
        """Abre constancia en navegador"""
        try:
            import webbrowser
            import os

            if archivo_temporal and os.path.exists(archivo_temporal):
                webbrowser.open(f"file://{os.path.abspath(archivo_temporal)}")
                response_text = f"ðŸŒ Constancia de {tipo_constancia} para {alumno.get('nombre')} abierta en el navegador."
            else:
                response_text = f"âŒ No se pudo abrir la constancia. Archivo no encontrado."

            return InterpretationResult(
                action="constancia_abierta",
                parameters={
                    "message": response_text,
                    "alumno": alumno,
                    "tipo_constancia": tipo_constancia,
                    "archivo": archivo_temporal,
                    "estado": "abierta"
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error abriendo constancia: {e}")
            return InterpretationResult(
                action="constancia_error",
                parameters={
                    "message": "âŒ Error abriendo constancia en navegador",
                    "error": "open_failed"
                },
                confidence=0.3
            )

    def _cancel_constancia(self, archivo_temporal: str) -> InterpretationResult:
        """Cancela constancia y limpia archivos temporales"""
        try:
            import os

            # Limpiar archivo temporal si existe
            if archivo_temporal and os.path.exists(archivo_temporal):
                try:
                    os.remove(archivo_temporal)
                    self.logger.debug(f"Archivo temporal eliminado: {archivo_temporal}")
                except Exception as e:
                    self.logger.warning(f"No se pudo eliminar archivo temporal: {e}")

            response_text = "âŒ GeneraciÃ³n de constancia cancelada. Vista previa eliminada."

            return InterpretationResult(
                action="constancia_cancelada",
                parameters={
                    "message": response_text,
                    "estado": "cancelada"
                },
                confidence=0.9
            )

        except Exception as e:
            self.logger.error(f"Error cancelando constancia: {e}")
            return InterpretationResult(
                action="constancia_error",
                parameters={
                    "message": "âŒ Error cancelando constancia",
                    "error": "cancel_failed"
                },
                confidence=0.3
            )

    def _process_constancia_from_pdf(self, constancia_info: Dict[str, Any], pdf_panel, context: Dict[str, Any]) -> InterpretationResult:
        """Procesa constancia desde PDF cargado (transformaciÃ³n)"""
        try:
            self.logger.info(f"ðŸ”„ Procesando transformaciÃ³n de PDF: {pdf_panel.original_pdf}")

            # Obtener informaciÃ³n de la transformaciÃ³n
            tipo_constancia = constancia_info.get("tipo_constancia", "estudio")
            incluir_foto = constancia_info.get("incluir_foto", False)

            # Usar el servicio de constancias para transformar
            from app.core.service_provider import ServiceProvider
            service_provider = ServiceProvider.get_instance()
            constancia_service = service_provider.constancia_service

            # Crear directorio temporal para preview
            import tempfile
            temp_dir = tempfile.mkdtemp(prefix="transformation_preview_")

            # Ejecutar transformaciÃ³n en modo preview
            success, message, data = constancia_service.generar_constancia_desde_pdf(
                pdf_path=pdf_panel.original_pdf,
                tipo_constancia=tipo_constancia,
                incluir_foto=incluir_foto,
                guardar_alumno=False,  # No guardar en BD en preview
                preview_mode=True,
                output_dir=temp_dir
            )

            if success and data:
                # ðŸ› ï¸ OBTENER INFORMACIÃ“N DEL ALUMNO - MANEJO SEGURO
                alumno_info = data.get("alumno", {})

                # ðŸ†• VERIFICAR QUE ALUMNO_INFO SEA UN DICCIONARIO
                if not isinstance(alumno_info, dict):
                    self.logger.warning(f"âš ï¸ alumno_info no es diccionario: {type(alumno_info)}, convirtiendo...")
                    # Si es una lista, tomar el primer elemento si existe
                    if isinstance(alumno_info, list) and len(alumno_info) > 0:
                        alumno_info = alumno_info[0] if isinstance(alumno_info[0], dict) else {}
                    else:
                        # Si no es lista o estÃ¡ vacÃ­a, usar diccionario vacÃ­o
                        alumno_info = {}

                # Actualizar contexto del panel para transformaciÃ³n
                if hasattr(pdf_panel, 'set_transformation_completed_context'):
                    pdf_panel.set_transformation_completed_context(
                        original_data=pdf_panel.pdf_data,
                        transformed_data=data,
                        alumno_data=alumno_info
                    )

                # Generar respuesta con auto-reflexiÃ³n
                response_with_reflection = self._generate_constancia_response_with_reflection(
                    alumno_info, tipo_constancia, data
                )

                if response_with_reflection:
                    return InterpretationResult(
                        action="transformation_preview",
                        parameters={
                            "message": response_with_reflection.get("respuesta_usuario", "Vista previa de transformaciÃ³n generada"),
                            "data": data,
                            "files": [data.get("ruta_archivo")] if data.get("ruta_archivo") else [],
                            "alumno": alumno_info,
                            "tipo_constancia": tipo_constancia,
                            "auto_reflexion": response_with_reflection.get("reflexion_conversacional", {}),
                            "transformation_info": {
                                "original_pdf": pdf_panel.original_pdf,
                                "transformed_pdf": data.get("ruta_archivo"),
                                "tipo_transformacion": tipo_constancia
                            }
                        },
                        confidence=0.95
                    )
                else:
                    return InterpretationResult(
                        action="transformation_preview",
                        parameters={
                            "message": f"Vista previa de constancia de {tipo_constancia} generada desde PDF",
                            "data": data,
                            "files": [data.get("ruta_archivo")] if data.get("ruta_archivo") else [],
                            "alumno": alumno_info,
                            "tipo_constancia": tipo_constancia,
                            "transformation_info": {
                                "original_pdf": pdf_panel.original_pdf,
                                "transformed_pdf": data.get("ruta_archivo"),
                                "tipo_transformacion": tipo_constancia
                            }
                        },
                        confidence=0.9
                    )
            else:
                return InterpretationResult(
                    action="transformation_error",
                    parameters={
                        "message": f"Error transformando PDF: {message}",
                        "error": "transformation_failed",
                        "service_message": message
                    },
                    confidence=0.3
                )

        except Exception as e:
            self.logger.error(f"Error en transformaciÃ³n de PDF: {e}")
            return InterpretationResult(
                action="transformation_error",
                parameters={
                    "message": "Error interno procesando la transformaciÃ³n. Intenta nuevamente.",
                    "error": "internal_error",
                    "exception": str(e)
                },
                confidence=0.1
            )
